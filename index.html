<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"rooki3ray.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Rooki3Ray | Cyber Security">
<meta property="og:url" content="http://rooki3ray.github.io/index.html">
<meta property="og:site_name" content="Rooki3Ray | Cyber Security">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Rooki3Ray">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://rooki3ray.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Rooki3Ray | Cyber Security</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c5dcd17d515f19e2d355facf9d8e773a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Rooki3Ray | Cyber Security</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/" class="post-title-link" itemprop="url">(AAAI2022) Deformable Graph Convolutional Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-05 16:58:16" itemprop="dateCreated datePublished" datetime="2022-02-05T16:58:16+08:00">2022-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 23:39:12" itemprop="dateModified" datetime="2022-02-14T23:39:12+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>提出在多个潜在空间上进行卷积并根据relation动态调整卷积的方法，并提出了在学习node representation基础上同时学习node positional embeddings的框架。</p>
<p>论文地址：<a href="https://arxiv.org/abs/2112.14438" target="_blank" rel="noopener">https://arxiv.org/abs/2112.14438</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/26/CS224n/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/26/CS224n/" class="post-title-link" itemprop="url">Stanford-CS224n</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-26 18:53:16" itemprop="dateCreated datePublished" datetime="2022-01-26T18:53:16+08:00">2022-01-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 20:08:19" itemprop="dateModified" datetime="2022-02-14T20:08:19+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>28k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>25 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CS224n"><a href="#CS224n" class="headerlink" title="CS224n"></a>CS224n</h1><p>学习Stanford的CS224n Natural Language Processing with Deep Learning。(<a href="https://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">课程链接</a>)</p>
<p><code>&lt;!-- more --&gt;</code></p>
<p>[TOC]</p>
<p><strong><em>Lecture 1. Introduction and word vector</em></strong></p>
<hr>
<h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><ul>
<li>basic和key mothods：RNN attention transformers</li>
<li>理解对人类自然语言理解中问题</li>
<li>用PyTorch构建模型，解决word meaning，dependency parsing，machine translation，QA等问题</li>
</ul>
<h3 id="NLP-levels"><a href="#NLP-levels" class="headerlink" title="NLP levels"></a>NLP levels</h3><p><img src="https://s2.loli.net/2022/01/26/sblwBhYyJ8DS2g9.png" alt=""></p>
<p>输入speech text-&gt;单词分析-&gt;<strong>句法分析-&gt;语义理解</strong>-&gt;上下文处理</p>
<h3 id="human-language"><a href="#human-language" class="headerlink" title="human language"></a>human language</h3><p>language is <strong>discrete/symbolic/categorical signaling system</strong></p>
<ul>
<li>rocket=🚀 violin=🎻</li>
<li>love loooove</li>
</ul>
<p>人类语言是一套符号系统，深度学习领域将大脑视为有连续的激活模式。</p>
<h3 id="deep-learning"><a href="#deep-learning" class="headerlink" title="deep learning"></a>deep learning</h3><ul>
<li>machine learning：由人类审视一个特定的问题，找出问题中的关键要素，设计useful features/signals，使用这些features解决问题，机器做数值优化。大部分工作是人类的数据分析。<ul>
<li>手动设计的特征过于具体，长时间设计和调整</li>
</ul>
</li>
<li>deep learning：表征学习的一个分支，输入原始的真实世界信号，机器自己学习特征，可以得到多层的learned representations。<ul>
<li>deep learning的特征适应性强，训练快，可以灵活通用地学习</li>
<li>海量数据+算力提升</li>
</ul>
</li>
</ul>
<h2 id="Word-Vector-represent-the-meaning-of-a-word"><a href="#Word-Vector-represent-the-meaning-of-a-word" class="headerlink" title="Word Vector (represent the meaning of a word)"></a>Word Vector (represent the meaning of a word)</h2><h3 id="meaning"><a href="#meaning" class="headerlink" title="meaning"></a>meaning</h3><ul>
<li>用一个词、词组表示的概念</li>
<li>一个人想用语言、符号表示的想法</li>
<li>表达在文学、艺术作品等方面的思想</li>
</ul>
<p>理解meaning的linguistic way：<br><img src="https://s2.loli.net/2022/01/26/u2oiGExBDya5eWH.png" alt=""><br>（指称语义）</p>
<h3 id="how-do-we-have-usable-meaning-in-a-computer"><a href="#how-do-we-have-usable-meaning-in-a-computer" class="headerlink" title="how do we have usable meaning in a computer"></a>how do we have usable meaning in a computer</h3><p>common NLP solution：<strong>WordNet</strong>，包含同义词集合和上位词(例如”is a”关系)的列表的词典。</p>
<ul>
<li>同义词和上位词<br><img src="https://s2.loli.net/2022/01/26/kW9Uax1jSOG3Bnz.png" alt=""></li>
</ul>
<h4 id="problems-with-resources-like-WordNet"><a href="#problems-with-resources-like-WordNet" class="headerlink" title="problems with resources like WordNet"></a>problems with resources like WordNet</h4><ul>
<li>作为资源使用很好，但忽略了细微差别（missing nuance）<ul>
<li>e.g. proficient 和 good是同义词，但需要结合上下文</li>
</ul>
</li>
<li>难以持续更新，缺乏单词的新含义</li>
<li>比较主观subjective</li>
<li>需要人力物力创建和调整词库</li>
<li>没有<strong>单词相似度</strong>的概念</li>
</ul>
<h4 id="representing-words-as-discrete-symbols"><a href="#representing-words-as-discrete-symbols" class="headerlink" title="representing words as discrete symbols"></a>representing words as discrete symbols</h4><p>在传统NLP中，word被视为离散符号，被表示为one-hot vec。</p>
<ul>
<li>problems：<ul>
<li>所有的vec都正交</li>
<li>one-hot vec没有相似度的概念</li>
<li>向量维度太大</li>
</ul>
</li>
<li>solution：<ul>
<li>使用类似wordnet的list来表示相似度，但有不完整性</li>
<li><strong>学习使用real-value vec本身来编码计算相似度</strong></li>
</ul>
</li>
</ul>
<h4 id="Representing-words-by-their-context"><a href="#Representing-words-by-their-context" class="headerlink" title="Representing words by their context"></a>Representing words by their context</h4><ul>
<li><strong>分布式语义</strong>（distributional semantics）：word的语义由经常出现在其附近的单词给出。</li>
<li>当一个单词w出现在一段文本中时，在一个固定大小的窗口内co-occur的单词就是w的上下文词，使用w的contexts来构建w的表示</li>
</ul>
<h3 id="word-vector-word-embedding-word-representation"><a href="#word-vector-word-embedding-word-representation" class="headerlink" title="word vector/word embedding/word representation"></a>word vector/word embedding/word representation</h3><p>为每个单词构建dense vec，使其与context中出现的单词的dense vec相似。</p>
<ul>
<li>word2vec</li>
<li>glove</li>
<li>sanjiv aurora’s paper</li>
</ul>
<h4 id="word2vec（Mikolov-et-al-2013）"><a href="#word2vec（Mikolov-et-al-2013）" class="headerlink" title="word2vec（Mikolov et al. 2013）"></a>word2vec（Mikolov et al. 2013）</h4><p>一个学习word vec的框架：</p>
<ul>
<li>有一个大的语料库corpus</li>
<li>词汇表中的每个单词用一个vec表示</li>
<li>对text中的每个位置t，有一个中心词$c$和一些上下文（context/outside）词$o$</li>
<li>用c和o词向量的相似性来计算给定$c$时$o$的概率$P(o|c)$</li>
<li>不断调整词向量，最大化这个概率$P(o|c)$</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/tgIXk1LHnpDT2we.png" alt=""><br><img src="https://s2.loli.net/2022/01/27/HKYXtyRQnosamg3.png" alt=""></p>
<h4 id="word2vec-objective-func"><a href="#word2vec-objective-func" class="headerlink" title="word2vec objective func"></a>word2vec objective func</h4><p>对每个位置$t=1,\cdots,T$，在一个大小为$m$的固定窗口内预测context words，给定中心词$w_j$，有<br>$Likelihood=L(\theta)=\Pi^T_{t=1}\Pi_{-m\le j\le m,j\neq 0}P(w_{t+j}|w_t;\theta)$</p>
<p>目标函数为：<br>$J(\theta)=-\frac{1}{T}\Sigma_{t=1}^T\Sigma_{-m\le j\le m,j\neq 0}\ log\ P(w_{t+j}|w_t;\theta)$</p>
<p>经典的$\Pi$通过log变成$\Sigma$，最小化目标函数-&gt;最大化似然函数</p>
<ul>
<li>如何计算 $P(w_{t+j}|w_t;\theta)$<ul>
<li>$v_w$：w为中心词的vec </li>
<li>$u_W$：w是上下文词的vec</li>
<li>则$P(o|c)=\frac{exp(u_o^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}$，向量点积越大即相似度越高</li>
</ul>
</li>
</ul>
<p>此处使用<strong>Softmax</strong>:$\mathbb{R}^n-&gt;(0,1)^n$将点积得到的任意值$x_i$映射到概率分布$p_i$</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>在负梯度上take small step</p>
<ul>
<li><p>Gradient</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial }{\partial v_c}log\ P(o|c) &= \frac{\partial }{\partial v_c}(log\ exp(u^T_ov_c)-\Sigma_{w\in V}exp(u_w^Tv_c)) \\
&= \frac{\partial }{\partial v_c}(u_o^Tv_c-log\ \Sigma_{w\in V}exp(u_w^Tv_c)) \\
&= u_o - \frac{\Sigma_{w\in V}exp(u_w^Tv_c)u_w}{\Sigma_{w\in V}exp(u_w^Tv_c)} \\
&= u_o - \Sigma_{w\in V}\frac{exp(u_w^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}u_w \\ 
&= u_o - \Sigma_{w\in V}P(w|c)u_w \\
&= observed - expected
\end{aligned}</script></li>
<li><p>Update equation<br>$\theta^{new} = \theta^{old} - \alpha \bigtriangledown_\theta J(\theta)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    theta_grad = evaluate_gradient(J, corpus, theta)</span><br><span class="line">    theta = theta - alpha * theta_grade</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p><strong><em>Lecture 2. Word Vectors 2 and Word Window Classification</em></strong></p>
<h4 id="stochastic-gradient-descent（SGD）"><a href="#stochastic-gradient-descent（SGD）" class="headerlink" title="stochastic gradient descent（SGD）"></a>stochastic gradient descent（SGD）</h4><ul>
<li>问题：$J(\theta)$是语料库中所有window的函数，梯度计算开销很大</li>
<li>方案：SGD，随机采样windows，在每个window或batch后update</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    window = sample_window(corpus)</span><br><span class="line">    theta_grad = evaluate_gradient(J, window, theta)</span><br><span class="line">    theta = theta - alpha * theta_grade</span><br></pre></td></tr></table></figure>
<h4 id="more-details"><a href="#more-details" class="headerlink" title="more details"></a>more details</h4><p>两个向量：更好优化，center vec和context vec最终会是相似的</p>
<p>两类模型：</p>
<ul>
<li>Skip-Grams: 给定中心词预测上下文</li>
<li>Continuous Bag of Words: 给定（bag of）上下文词，预测中心词</li>
</ul>
<p>其他训练策略：</p>
<ul>
<li>negative sampling</li>
<li>negative sampling for naive softmax</li>
</ul>
<p>word2vec 比较crude，很大程度上忽略了词的具体位置</p>
<h4 id="skip-gram-model-with-negative-sampling（HW2）"><a href="#skip-gram-model-with-negative-sampling（HW2）" class="headerlink" title="skip-gram model with negative sampling（HW2）"></a>skip-gram model with negative sampling（HW2）</h4><p>$P(o|c)=\frac{exp(u_o^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}$中分母的计算开销过大</p>
<ul>
<li><p>Main idea：用true pair（中心词与其上下文窗口中的词）与几个noise pair（中心词与随机词）训练<strong>二元逻辑回归</strong></p>
</li>
<li><p>maximize目标函数：$J(\theta)=\frac{1}{T}\Sigma^T_{t=1}J_t(\theta)$<br>$J_t(\theta)=log\ \sigma(u_o^Tv_c)+\Sigma^k_{i=1}\mathbb{E}_{j\sim P(w)}[log\ \sigma(-u^T_jv_c)]$<br>$\sigma(x)=\frac{1}{1+e^{-x}}$</p>
</li>
</ul>
<p>maximize前半部分真实上下文词的概率，minimize后半部分负采样随机词的概率</p>
<p>负采样方案：$P(w)=U(w)^{3/4}/Z$，$U(w)$是unigram分布，用幂提升罕见词的采样概率，$Z$为归一化。</p>
<h3 id="why-not-capture-co-ocurrence-counts-directly"><a href="#why-not-capture-co-ocurrence-counts-directly" class="headerlink" title="why not capture co-ocurrence counts directly?"></a>why not capture co-ocurrence counts directly?</h3><p>构建co-occurence matrix $X$的两种方法：</p>
<ul>
<li>窗口windows：与w2v类似，使用中心词附近的window，捕获句法和语义信息</li>
<li>全文full document：窗口大小设为文章长度，常用于信息检索方法中，如Latent Semantic Analysis</li>
</ul>
<p>问题：</p>
<ul>
<li>词表变大则向量维度也要增大，但向量很稀疏</li>
<li>分类模型也会遭遇稀疏的问题，鲁棒性差</li>
</ul>
<p>需要使用low-dim的dense matrix。</p>
<h4 id="Classic-Method-Dimensionality-Reduction-on-X（HW1）"><a href="#Classic-Method-Dimensionality-Reduction-on-X（HW1）" class="headerlink" title="Classic Method Dimensionality Reduction on X（HW1）"></a>Classic Method Dimensionality Reduction on X（HW1）</h4><p>用SVD分解$X$为$U\Sigma V^T$，$U$和$V$对应行和列的正交基。</p>
<p>为了减少尺度，可以保留$\Sigma$对角阵中的topK最大值，并保留$U,V$对应的行列。</p>
<ul>
<li>直接对原始的count做SVD效果很差，需要对数字进行调整：<ul>
<li>取log</li>
<li>取$min(X,\delta)$</li>
<li>忽略虚词的count</li>
</ul>
</li>
<li>使用斜坡ramped window，对更近的词计更大的count</li>
<li>用Pearson相关系数（Pearson Correlation Coefficient）代替count，然后将负值置零</li>
</ul>
<h4 id="Towards-GloVe：Count-based-vs-direct-prediction"><a href="#Towards-GloVe：Count-based-vs-direct-prediction" class="headerlink" title="Towards GloVe：Count based vs direct prediction"></a>Towards GloVe：Count based vs direct prediction</h4><ul>
<li>count based：基于co-occurence<ul>
<li>LSA、HAL、COALS、Hellingger-PCA</li>
<li>训练快；有效使用统计数据</li>
<li>主要用于捕获词相似性；过分重视count大的数据</li>
</ul>
</li>
<li>direct prediction：定义概率分布，根据概率预测词<ul>
<li>Skip-gram/CBOW、NNLM、HLBL、RNN</li>
<li>规模跟corpus大小有关；没有有效利用统计数据（采样和窗口）</li>
<li>提高了在其他任务上的表现；可以捕获相似度之外更复杂的信息</li>
</ul>
</li>
</ul>
<h4 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h4><p>结合两种方法，在神经网络中引入count矩阵</p>
<p>核心思想：co-orrurrence概率之比可以编码meaning components </p>
<p><img src="https://s2.loli.net/2022/01/27/Sfgh7JzbwAOtvBx.png" alt=""></p>
<p>单一的概率并非重点，概率的比值更能反映(linear) meanning component。</p>
<p>关键：$\frac{P(x|a)}{P(x|b)}$如何在词向量空间中捕获？</p>
<ul>
<li>用向量的点积表示co-occurence 概率的对数</li>
<li>$w_i\cdot w_j = log P(i|j)$</li>
<li>$w_x\cdot (w_a - w_b)\frac{P(x|a)}{P(x|b)}$</li>
<li>$J=\Sigma^V_{i,j=1}f(X_{ij})(w_i^T\tilde w_j+b_i+\tilde b_j-log X_{ij})^2$<br>$f$限制了常见词的count </li>
</ul>
<h3 id="how-to-evaluate-word-vectors"><a href="#how-to-evaluate-word-vectors" class="headerlink" title="how to evaluate word vectors"></a>how to evaluate word vectors</h3><ul>
<li>Intrinsic<ul>
<li>在特定和中间子任务上评估</li>
<li>评估运算很快</li>
<li>有助于理解整个系统</li>
<li>在用于实际任务前不清楚是否有用</li>
</ul>
</li>
<li>Extrinsic<ul>
<li>在实际任务上评估</li>
<li>通常需要更长时间（任务更大）</li>
<li>不清楚系统整个系统的问题所在</li>
</ul>
</li>
</ul>
<h3 id="word-sense-and-word-sense-ambiguity"><a href="#word-sense-and-word-sense-ambiguity" class="headerlink" title="word sense and word sense ambiguity"></a>word sense and word sense ambiguity</h3><ul>
<li>多义的word 和 一一对应的word vec相悖</li>
</ul>
<h4 id="imporving-word-representations-via-global-context-and-multiple-word-prototypes（Huang-et-al-2012）"><a href="#imporving-word-representations-via-global-context-and-multiple-word-prototypes（Huang-et-al-2012）" class="headerlink" title="imporving word representations via global context and multiple word prototypes（Huang et al. 2012）"></a>imporving word representations via global context and multiple word prototypes（Huang et al. 2012）</h4><p>对常用词的所有context聚类，从而得到一些表示不同含义的簇。</p>
<p>划分并不明确且存在重叠。</p>
<h4 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</h4><ul>
<li>单词的不同含义以加权的线性形式叠加：<br>$v_w=\Sigma\alpha_iv_{w_i}$<br>$\alpha_i=\frac{f_i}{\Sigma f_j}$</li>
<li>实际上，用稀疏编码Sparse coding的思想，不同维度表示了不同的含义，所以weighted-avg对不同含义的维度损失很小。</li>
</ul>
<p><strong><em>Lecture 3. Backprop and Neural Networks</em></strong></p>
<h2 id="matrix-calculus"><a href="#matrix-calculus" class="headerlink" title="matrix calculus"></a>matrix calculus</h2><h3 id="gradient"><a href="#gradient" class="headerlink" title="gradient"></a>gradient</h3><ul>
<li>单输出多输入的函数$f(\mathbf{x})=f(x_1,\dots,x_n)$</li>
<li>梯度gradient是关于每个输入的偏导数partial derivatives的向量$\frac{\partial f}{\partial x}=[\frac{\partial f}{\partial x_1},\dots,\frac{\partial f}{\partial x_n}]$</li>
</ul>
<h3 id="Jacobian-Matirx-Generalization-of-the-Gradient"><a href="#Jacobian-Matirx-Generalization-of-the-Gradient" class="headerlink" title="Jacobian Matirx: Generalization of the Gradient"></a>Jacobian Matirx: Generalization of the Gradient</h3><ul>
<li>多输出多输入的函数$\mathbf{f(x)}=[f_1(x_1,\dots,x_n),\dots,f_m(x_1,\dots,x_n)]$</li>
<li>Jacobian mat是$m\times n$的矩阵：<br>$\mathbf{\frac{\partial f}{\partial x}}=<br>\begin{bmatrix}<br>\frac{\partial f_1}{\partial x_1} &amp; \dots &amp; \frac{\partial f_1}{\partial x_n} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>\frac{\partial f_m}{\partial x_1} &amp; \dots &amp; \frac{\partial f_m}{\partial x_n} \\<br>\end{bmatrix}$</li>
</ul>
<h3 id="chain-rule"><a href="#chain-rule" class="headerlink" title="chain rule"></a>chain rule</h3><p>多变量的链式法则：乘Jacobian mat</p>
<h3 id="example：NER"><a href="#example：NER" class="headerlink" title="example：NER"></a>example：NER</h3><ul>
<li>找出文本中的实体并分类<br><img src="https://s2.loli.net/2022/01/27/gZzbEHY2RVdBPGO.png" alt=""></li>
<li>Idea：通过上下文窗口来对每个词分类，对每个类训练logistic分类器<br><img src="https://s2.loli.net/2022/01/27/g5IB4W3qwnMasOf.png" alt=""></li>
</ul>
<h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><ul>
<li>计算$\frac{\partial s}{\partial \mathbf{b}}$<br><img src="https://s2.loli.net/2022/01/27/vrBADQU2n8Vz7dT.png" alt=""></li>
<li>计算$\frac{\partial s}{\partial \mathbf{W}}$<br>与上述过程相似，其中前两项是一致的，无需重复计算<br>$\frac{\partial s}{\partial \mathbf{W}}=\delta\frac{\partial z}{\partial \mathbf{W}}$<br>$\delta=\frac{\partial s}{\partial \mathbf{h}}\frac{\partial \mathbf{h}}{\partial z}=\mathbf{u}^T\circ f’(z)$是local error signal</li>
</ul>
<h4 id="derivative-with-respect-to-Matrix-output-shape"><a href="#derivative-with-respect-to-Matrix-output-shape" class="headerlink" title="derivative with respect to Matrix: output shape"></a>derivative with respect to Matrix: output shape</h4><ul>
<li>输出大小为1，输入大小为$n\times m$.</li>
<li>从数学角度，输出应该为$1\times nm$</li>
<li>但为了方便更新参数，其shape应为$n\times m$</li>
</ul>
<p>$\frac{\partial s}{\partial \mathbf{W}}=\delta^Tx^T$，transpose可以解决shape的问题</p>
<p><strong><em>Lecture 4. Dependency Parsing</em></strong></p>
<hr>
<h2 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h2><h3 id="constituency-Parsing"><a href="#constituency-Parsing" class="headerlink" title="constituency Parsing"></a>constituency Parsing</h3><p>constituency = phrase structure grammer = <strong>context-free</strong> grammers（CFGs）</p>
<ul>
<li>starting unit：word</li>
<li>words 组合成短语</li>
<li><p>短语可以递归地组合成更长的短语</p>
</li>
<li><p>Det Determiner限定词 </p>
</li>
<li>NP Noun Phrase名词短语 </li>
<li>VP Verb Phrase动词短语 </li>
<li>P Preposition 介词<ul>
<li>PP Prepositional Phrase介词短语</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/XybS6hjsQ3KWJc9.png" alt=""></p>
<h3 id="dependency-structure"><a href="#dependency-structure" class="headerlink" title="dependency structure"></a>dependency structure</h3><p>用单词间的依赖/修饰关系表示句子的结构</p>
<h4 id="why-do-we-need-sentense-structure"><a href="#why-do-we-need-sentense-structure" class="headerlink" title="why do we need sentense structure"></a>why do we need sentense structure</h4><ul>
<li>自然语言中将单词组合成更大的短语、句子来表达复杂的含义和思想</li>
<li>人类在理解自然语言是需要弄清楚单词间的相关性</li>
<li>自然语言模型需要同样的、理解句子结构的能力</li>
</ul>
<h4 id="some-ambiguity"><a href="#some-ambiguity" class="headerlink" title="some ambiguity"></a>some ambiguity</h4><h5 id="PP-attachment-ambiguity-介词短语歧义"><a href="#PP-attachment-ambiguity-介词短语歧义" class="headerlink" title="PP attachment ambiguity 介词短语歧义"></a>PP attachment ambiguity 介词短语歧义</h5><ul>
<li><p><em>San Jose cops kill man with knife</em></p>
<ul>
<li>San Jose cops kill <strong>man with knife</strong><ul>
<li>knife是man的modifier</li>
</ul>
</li>
<li>San Jose cops <strong>kill</strong> man <strong>with knife</strong><ul>
<li>knife是kill的modifier</li>
</ul>
</li>
</ul>
</li>
<li><p>关键是如何分析依存关系</p>
<ul>
<li>对复杂的句子结构，有指数级的依存关系parsing tree </li>
<li>Catalan number $C_n=(2n)!/[(n+1)!n!]$<br>出现在树形结构的环境中，如n+2多边形的三角剖分可能数</li>
</ul>
</li>
</ul>
<h5 id="coordination-scope-ambiguity-协调范围模糊"><a href="#coordination-scope-ambiguity-协调范围模糊" class="headerlink" title="coordination scope ambiguity 协调范围模糊"></a>coordination scope ambiguity 协调范围模糊</h5><ul>
<li><em>Shuttle veteran and longtime NASA executive Fred Gregory appointed to board</em><ul>
<li><strong>Shuttle veteran and longtime NASA executive</strong> Fred Gregory appointed to board<ul>
<li>shuttle veteran 和 longtime NASA executive 都是指Fred</li>
</ul>
</li>
<li>Shuttle veteran and <strong>longtime NASA executive Fred Gregory</strong> appointed to board<ul>
<li>仅longtime NASA executive 是指Fred</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="adj-adv-modifier-ambiguity"><a href="#adj-adv-modifier-ambiguity" class="headerlink" title="adj adv modifier ambiguity"></a>adj adv modifier ambiguity</h5><ul>
<li><em>Students get first hand job experience</em><ul>
<li>Students get <strong>first hand job</strong> experience<ul>
<li>first hand 修饰 job</li>
</ul>
</li>
<li>Students get <strong>first hand</strong> job <strong>experience</strong><ul>
<li>first hand 修饰 experience</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="VP-attachment-ambiguity"><a href="#VP-attachment-ambiguity" class="headerlink" title="VP attachment ambiguity"></a>VP attachment ambiguity</h5><ul>
<li><em>Multilated body washed up on Rio beach to be used for Olympics beach volleyball</em><ul>
<li>Multilated <strong>body</strong> washed up on Rio beach <strong>to be used for Olympics beach volleyball</strong><ul>
<li>to be used 修饰 body</li>
</ul>
</li>
<li>Multilated body washed up on Rio <strong>beach to be used for Olympics beach volleyball</strong><ul>
<li>to be used 修饰 beach</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="dependency-grammar-and-dependency-structure"><a href="#dependency-grammar-and-dependency-structure" class="headerlink" title="dependency grammar and dependency structure"></a>dependency grammar and dependency structure</h4><p>dependency syntax中单词间的关系通常是二元不对称的依赖关系（箭头 ）</p>
<ul>
<li>箭头连接head和dependency<ul>
<li>head-&gt;dependency</li>
</ul>
</li>
<li>依赖关系形成单头无环的图 </li>
<li>通常额外引入一个ROOT节点，使所有单词有head</li>
</ul>
<h4 id="universal-dependency-treebanks"><a href="#universal-dependency-treebanks" class="headerlink" title="universal dependency treebanks"></a>universal dependency treebanks</h4><p>统一、并行、通用的依赖关系描述</p>
<ul>
<li>人工编写语法规则，训练得到parser</li>
<li>规则愈发复杂，不能重用之前的工作</li>
</ul>
<p><strong>treebank</strong></p>
<ul>
<li>起初构建treebank比语法规则要慢且用处有限</li>
<li>可重用<ul>
<li>在treebank基础上建立parser、tagger等</li>
<li>语言学的</li>
</ul>
</li>
<li>覆盖面广泛</li>
<li>利用了统计信息</li>
<li>可以评估NLP系统</li>
</ul>
<h4 id="dependency-conditioning-preference"><a href="#dependency-conditioning-preference" class="headerlink" title="dependency conditioning preference"></a>dependency conditioning preference</h4><p>依赖解析的信息：</p>
<ul>
<li>Bilexical affinities两个单词间的密切关系</li>
<li>Dependency distance：大多数依赖都是和相邻词的</li>
<li>intervening material：依赖关系很少跨越中间动词or标点符号</li>
<li>valency of head：不同类型head的依赖关系位置相对固定</li>
</ul>
<h4 id="Dependency-Parsing-1"><a href="#Dependency-Parsing-1" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h4><ul>
<li>为每个单词选择其dependency来解析句子</li>
<li>只有一个单词是依赖于root的</li>
<li>不能成环（只能成tree）</li>
<li>箭头可以交叉，不交叉的称为non-projective</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/jDhBrbPnF7osI3z.png" alt=""></p>
<h5 id="projectivity"><a href="#projectivity" class="headerlink" title="projectivity"></a>projectivity</h5><ul>
<li>单词按线性顺序排列时，没有交叉的dependency。</li>
<li>CFG树的依赖关系必须是projective的</li>
<li>non-projective结构解释移位的word</li>
</ul>
<p>介词搁浅<br><img src="https://s2.loli.net/2022/01/27/xSE4cLkqvpFNbHP.png" alt=""></p>
<h5 id="methods"><a href="#methods" class="headerlink" title="methods"></a>methods</h5><ul>
<li>Dynamic programming<ul>
<li>Eisner提出的复杂度为$O(n^3)$的clever algo，通过在末尾而非中间生成head</li>
</ul>
</li>
<li>Graph algo<ul>
<li>对句子建立最小生成树（Minimun Spanning Tree）</li>
</ul>
</li>
<li>constraint satisfaction<ul>
<li>去掉不满足硬约束的edge</li>
</ul>
</li>
<li>transition-based parsing or deterministic dependency parsing<ul>
<li>基于ml classifier的贪心选择</li>
<li>比较复杂，具体详见<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/index.html#schedule" target="_blank" rel="noopener">本课程Lecture4</a></li>
</ul>
</li>
</ul>
<p><em>Lecture 5. Recurrent Neural Networks and Language Models</em></p>
<hr>
<h5 id="how-do-we-gain-from-a-neural-dependency-parser"><a href="#how-do-we-gain-from-a-neural-dependency-parser" class="headerlink" title="how do we gain from a neural dependency parser"></a>how do we gain from a neural dependency parser</h5><ul>
<li><p>indicator feature 的问题：</p>
<ul>
<li>这样的特征向量很稀疏</li>
<li>特征不完整</li>
<li>特征的计算开销很大</li>
</ul>
</li>
<li><p>distributed representation</p>
<ul>
<li>将每个单词表示为dense vector</li>
<li>part-of-sppeech tags和dependency labels也被表示为同样维度的dense vector</li>
</ul>
</li>
<li><p>deep learning and non-linear classifier</p>
<ul>
<li>传统 ML classifer只有linear boundary</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/WdrXvBsxtLJQKTi.png" alt=""></p>
<h5 id="graph-based-dependency-parsers"><a href="#graph-based-dependency-parsers" class="headerlink" title="graph-based dependency parsers"></a>graph-based dependency parsers</h5><ul>
<li>对每个可能的dependency计算score<ul>
<li>需要理解单词的contextual representation</li>
</ul>
</li>
</ul>
<h2 id="more-things-about-NN"><a href="#more-things-about-NN" class="headerlink" title="more things about NN"></a>more things about NN</h2><h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><ul>
<li>完整的loss func包含对所有参数$\theta$的正则化<ul>
<li>L2 regularization $\lambda\Sigma_k \theta_k^2$</li>
</ul>
</li>
<li>正则化可以防止大量特征/深层模型出现过拟合</li>
<li>为模型提供了更强的泛化能力（过拟合实际上不是问题，需要泛化能力）</li>
</ul>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><ul>
<li>在训练时随机将神经元输出特征的一部分置零（从而不更新该部分权重）</li>
<li>避免特征co-adaptation：不会出现某特征仅在其他特征出现时才有用的情况</li>
<li>在单层中，可以看做naive bayes（所有权重被独立设置）和logistic regression（权重在所有上下文中设置）的折中</li>
<li>可以看做一种model bagging（ensemble model），或是一种特征独立的强正则化</li>
</ul>
<h3 id="non-linearities"><a href="#non-linearities" class="headerlink" title="non-linearities"></a>non-linearities</h3><p><img src="https://s2.loli.net/2022/01/27/GBCEb8AKiyukLvJ.png" alt=""></p>
<ul>
<li>old：指数级计算<ul>
<li>logistic（sigmoid）：0～1</li>
<li>tanh：1～-1，实际上是重新缩放和移动的sigmoid<ul>
<li>$tanh(z)=2sigmoid(2z)-1$</li>
</ul>
</li>
</ul>
</li>
<li>new<ul>
<li>hard tanh</li>
<li>ReLU<ul>
<li>神经元要么dead要么就在传递信息</li>
<li>有很好的梯度backflow，训练很快</li>
</ul>
</li>
</ul>
</li>
<li>varient<br><img src="https://s2.loli.net/2022/01/27/Jt426ZfwesoF1yv.png" alt=""></li>
</ul>
<h3 id="prameter-initialization"><a href="#prameter-initialization" class="headerlink" title="prameter initialization"></a>prameter initialization</h3><ul>
<li>将权重初始化为小随机值，避免对称性影响学习</li>
<li>Uniform(-r,r)</li>
<li>Xavier初始化中，方差和前后层的尺寸有关<ul>
<li>$Var(W_i)=\frac{2}{n_{in}+n_{out}}$</li>
</ul>
</li>
</ul>
<h3 id="optimizers"><a href="#optimizers" class="headerlink" title="optimizers"></a>optimizers</h3><ul>
<li>简单的SGD很有效</li>
<li>其他优化器<ul>
<li>Adagrad</li>
<li>RMSprop</li>
<li>Adam（安全有效）</li>
<li>SparseAdam</li>
</ul>
</li>
</ul>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><ul>
<li>lr太大模型会不收敛，太小模型效果会变差</li>
<li>在训练时通常需要动态调整lr<ul>
<li>by hand：k个epoch后减半</li>
<li>by formula：$lr=lr_0e_{-kt}$, for epoch t</li>
<li>循环lr</li>
</ul>
</li>
</ul>
<h2 id="language-model-and-RNN"><a href="#language-model-and-RNN" class="headerlink" title="language model and RNN"></a>language model and RNN</h2><h3 id="language-modeling"><a href="#language-modeling" class="headerlink" title="language modeling"></a>language modeling</h3><ul>
<li>语言模型的任务是预测下一位置的单词<ul>
<li>给定单词序列$x^{(1)},\dots,x^{(t)}$，计算下一单词$x^{(t+1)}$的概率分布$P(x^{(t+1)}|x^{(t)},\dots,x^{(1 )})$</li>
</ul>
</li>
<li>语言模型也可以看做将概率分配给一段text<ul>
<li>$P(x^{(1)},\dots,x^{(t)})=\Pi^T_{t=1}P(x^{(t)}|x^{(t-1 )},\dots,x^{(1 )})$</li>
</ul>
</li>
</ul>
<h3 id="n-gram-language-model"><a href="#n-gram-language-model" class="headerlink" title="n-gram language model"></a>n-gram language model</h3><ul>
<li>n-gram：由n个单词组成的块</li>
<li>统计不同n-gram出现的频率来预测下一个单词</li>
<li><p>Markov假设：</p>
<ul>
<li>$x^{(t+1)}$只取决于之前的n-1个单词<br>$\begin{aligned} P(x^{(t+1)}|x^{(t)},\dots,x^{(1 )})&amp;=P(x^{(t+1)}|x^{(t)},\dots,x^{(t-n+2)})\\<br>&amp;= \frac{P(x^{(t+1)},\dots,x^{(t-n+2)})}{P(x^{(t)},\dots,x^{(t-n+2)})} \\<br>&amp;= \frac{\text{prob of n-gram}}{\text{prob of (n-1)-gram}}\end{aligned}$</li>
</ul>
</li>
<li><p>使用count的统计近似计算以上概率</p>
</li>
</ul>
<h4 id="sparsity-problem-with-n-gram"><a href="#sparsity-problem-with-n-gram" class="headerlink" title="sparsity problem with n-gram"></a>sparsity problem with n-gram</h4><ul>
<li>问题：(n-1)-gram + w 的形式不存在，概率为0<ul>
<li>解决方案：为每个w设置一个$\delta$的小概率（smoothing）</li>
</ul>
</li>
<li>问题：(n-1)-gram的形式不存在，概率为0<ul>
<li>解决方案：回退到(n-2)-gram</li>
</ul>
</li>
<li>n越大，稀疏问题越严重</li>
</ul>
<h4 id="generating-text-with-n-gram"><a href="#generating-text-with-n-gram" class="headerlink" title="generating text with n-gram"></a>generating text with n-gram</h4><ul>
<li>可以生成语法上正确的text，但语义上不连贯。</li>
<li>模拟语言的能力需要更大的n，但与sparsity相悖</li>
</ul>
<h3 id="neural-language-model"><a href="#neural-language-model" class="headerlink" title="neural language model"></a>neural language model</h3><h4 id="fixed-window-neural-language-model"><a href="#fixed-window-neural-language-model" class="headerlink" title="fixed-window neural language model"></a>fixed-window neural language model</h4><p><img src="/uploads/upload_f3fafeddf213bc74ed958d82ad952ba1.png" alt=""></p>
<p>优点：</p>
<ul>
<li>不存在sparsity问题</li>
<li>不需要使用所有n-gram<br>问题：</li>
<li>窗口太小</li>
<li>增大窗口需要增大W（增大模型）</li>
<li>输入乘以W中不同的权重，对单词的处理不对称</li>
</ul>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p><img src="https://s2.loli.net/2022/01/27/H6MUQuW51aT2ROq.png" alt=""></p>
<ul>
<li>核心思想：重复使用同样的Weight<br>优点：</li>
<li>可以处理任意长度的输入</li>
<li>理论上可以利用序列之前的信息</li>
<li>模型大小不会随输入增大而变化</li>
<li>在每个timestep上使用对称的处理方式</li>
</ul>
<p>问题：</p>
<ul>
<li>时序的递归计算慢</li>
<li>长程依赖</li>
</ul>
<h5 id="training-RNN-LM"><a href="#training-RNN-LM" class="headerlink" title="training RNN LM"></a>training RNN LM</h5><p>计算每一步的输出概率分布</p>
<p>每一步的损失函数：<br>$J^{(t)}(\theta)=CE(y^{(t)}, \hat{y}^{(t)})=-\Sigma log\ y_w^{(t)}log\ \hat y_w^{(t)}$</p>
<p>总体损失函数<br>$J(\theta)=\frac{1}{T}\Sigma\ J^{(t)}(\theta)$</p>
<ul>
<li>在整个corpus$x^{(1)},\dots,x^{(T)}$上计算loss和梯度开销很大，通常对一个sentence和document计算并更新weight</li>
</ul>
<h5 id="backpropagation-for-RNN"><a href="#backpropagation-for-RNN" class="headerlink" title="backpropagation for RNN"></a>backpropagation for RNN</h5><p>$W_h$是共享参数，梯度计算：<br>$\frac{\partial J^{(t)}}{\partial W_h}=\Sigma^t_i\frac{\partial J^{(t)}}{\partial W_h}|_{(i)}$<br>将各时刻的梯度计算后求和。</p>
<p>以上结论可以通过多变量导数的计算来证明。</p>
<p>backpropagation through time</p>
<h5 id="generating-text-with-a-RNN-LM"><a href="#generating-text-with-a-RNN-LM" class="headerlink" title="generating text with a RNN LM"></a>generating text with a RNN LM</h5><p>不断采样，采样输出是下一步的输入。</p>
<pre><code>- 比n-gram生成的文本更加流畅，但总体上仍不连贯
</code></pre><ul>
<li>考虑RNN和手工规则的结合<ul>
<li>Beam Search</li>
</ul>
</li>
</ul>
<h4 id="evaluating-LM"><a href="#evaluating-LM" class="headerlink" title="evaluating LM"></a>evaluating LM</h4><ul>
<li>困惑度perplexity<br>$\Pi^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},\dots,x^{(1)})})^{(1/T)}$</li>
<li>等于CE-loss的exp</li>
</ul>
<h4 id="problems-with-vanishing-and-exploding-gradients"><a href="#problems-with-vanishing-and-exploding-gradients" class="headerlink" title="problems with vanishing and exploding gradients"></a>problems with vanishing and exploding gradients</h4><h5 id="vanishing-gradient"><a href="#vanishing-gradient" class="headerlink" title="vanishing gradient"></a>vanishing gradient</h5><ul>
<li>chain rule使反向传播深入后梯度信号越来越小</li>
<li>proof<ul>
<li>$h^{(t)} = \sigma(W_hh^{(t-1)}+W_xx^{(t)}+b_!)$</li>
<li>假设$\sigma(x)=x$</li>
<li>$\frac{\partial h^{(t)}}{\partial h^{(t-1)}}=diag(\sigma’(W_hh^{(t-1)}+w_x^{(t)}+b_1))W_h=W_h$</li>
<li>那么$\frac{\partial J^{(i)}(\theta)}{\partial h^{(j)}}=\frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}}W_h^{(i-j)}$</li>
<li>如果$W_h$的特征值均小于1(因为使用了线性的$\sigma$所以为1)，则梯度会呈指数衰减；</li>
</ul>
</li>
</ul>
<p>RNN-LM更善于从<strong>顺序上相近</strong>的而非<strong>语法上相近</strong>的单词学习</p>
<h5 id="exploding-gradient"><a href="#exploding-gradient" class="headerlink" title="exploding gradient"></a>exploding gradient</h5><p>$\theta^{new} = \theta^{old} - \alpha \bigtriangledown_\theta J(\theta)$</p>
<ul>
<li>梯度爆炸会让SGD的“步子”过大</li>
<li>极端情况下会导致网络中出现inf或NaN</li>
</ul>
<h5 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h5><p><img src="https://s2.loli.net/2022/01/27/sIQich9d8Smvqf4.png" alt=""></p>
<ul>
<li>在同样的方向上take a smaller step</li>
</ul>
<h5 id="how-to-fix-the-vanishing-gradient-problem"><a href="#how-to-fix-the-vanishing-gradient-problem" class="headerlink" title="how to fix the vanishing gradient problem"></a>how to fix the vanishing gradient problem</h5><ul>
<li>RNN中很难学习若干时间步之前的信息</li>
<li>使用具有单独记忆的RNN</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul>
<li>在每一步中，拥有两个hidden vector<ul>
<li>hidden state $h^{(t)}$和cell state $c^{(t)}$</li>
<li>二者都是长度为n的向量</li>
<li>cell state保存长期信息</li>
<li>LSTM何以从cell读写删信息</li>
</ul>
</li>
<li>信息被读/写/删由三个对应的gate控制<ul>
<li>gates也是长度为n的向量</li>
<li>每一步上，gate的每个元素可以是open 1/close 0/两者之间的一个概率</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/SgTA4K5b3jLlha9.png" alt=""><br><img src="https://s2.loli.net/2022/01/27/QWURPti4zaoZBET.png" alt=""></p>
<ul>
<li>LSTM相比经典RNN更容易保存时间步上的信息</li>
<li>如果forget gate 保存为1，则可以保存所有历史信息</li>
<li>LSTM的参数也更难学习</li>
<li>不保证没有梯度消失和爆炸</li>
</ul>
<h4 id="is-vanishing-exploding-gradient-just-a-RNN-problem？"><a href="#is-vanishing-exploding-gradient-just-a-RNN-problem？" class="headerlink" title="is vanishing/exploding gradient just a RNN problem？"></a>is vanishing/exploding gradient just a RNN problem？</h4><ul>
<li>所有的神经网络结构（尤其是深度解构）都会有<ul>
<li>chain rule和非线性函数会导致反向传播时梯度很小</li>
<li>低层次的学习很慢</li>
<li>RNN中重复乘以统一梯度，更加明显</li>
</ul>
</li>
<li>solution：添加direct coneection<ul>
<li>residual connection</li>
</ul>
</li>
</ul>
<h4 id="bidirectional-RNN"><a href="#bidirectional-RNN" class="headerlink" title="bidirectional RNN"></a>bidirectional RNN</h4><p><img src="https://s2.loli.net/2022/01/27/bLHlzje3ZtnuNDG.png" alt=""></p>
<ul>
<li>包含forward RNN和backward RNN</li>
<li>需要有整个输入序列时才能使用（LM中预测下一个word时，相当于只有左边的context，故不能使用双向RNN）</li>
</ul>
<h4 id="multi-layer-RNN"><a href="#multi-layer-RNN" class="headerlink" title="multi-layer RNN"></a>multi-layer RNN</h4><ul>
<li>堆叠多层RNN，在空间维度上深入（RNN本身是在时序上深入）</li>
<li>捕获不同层次的特征</li>
<li>i层的输入是i-1层的hidden state</li>
<li>经验法则：2到4层比较合适</li>
<li></li>
</ul>
<p><em>Lecture 7. Machine Translation, Attention, Subword Models</em></p>
<hr>
<h2 id="machine-translation"><a href="#machine-translation" class="headerlink" title="machine translation"></a>machine translation</h2><h3 id="statistical-MT"><a href="#statistical-MT" class="headerlink" title="statistical MT"></a>statistical MT</h3><ul>
<li>给定法语sentence x，想要找到最好的英语sentence y</li>
<li>$argmax_xP(y|x)$</li>
<li>用bayes rule可以分解为两个组件：$argmax_xP(x|y)P(y)$</li>
<li>$P(x|y)$<ul>
<li>翻译模型，分析单词和短语应该如何翻译</li>
<li>从人工翻译的并行数据中学习</li>
</ul>
</li>
<li>$P(y)$<ul>
<li>语言模型，模型如何生成合适的语言</li>
<li>从单词数据中学习</li>
</ul>
</li>
</ul>
<h4 id="learning-alignment-for-SMT"><a href="#learning-alignment-for-SMT" class="headerlink" title="learning alignment for SMT"></a>learning alignment for SMT</h4><ul>
<li>需要学习$P(x,a|y)$,其中$a$是单词间的对应</li>
<li>$a$是隐变量，需要用EM算法等来学习</li>
</ul>
<h4 id="decoding-for-SMT"><a href="#decoding-for-SMT" class="headerlink" title="decoding for SMT"></a>decoding for SMT</h4><ul>
<li>如何计算argmax，列举所有y并计算概率显然不可行</li>
<li>用启发式算法丢弃概率过低的假设</li>
</ul>
<h3 id="neural-MT"><a href="#neural-MT" class="headerlink" title="neural MT"></a>neural MT</h3><h4 id="sequence-to-sequence-model"><a href="#sequence-to-sequence-model" class="headerlink" title="sequence-to-sequence model"></a>sequence-to-sequence model</h4><ul>
<li>encoder+decoder</li>
<li>用于很多NLP任务<ul>
<li>文本摘要</li>
<li>对话生成</li>
<li>文本解析</li>
<li>代码生成</li>
</ul>
</li>
<li>是一种<strong>conditional LM</strong><ul>
<li>$P(y|x)=P(y_1|x)P(y_2|y_1,x)\dots P(y_T|y_1,\dots,y_{T-1},x)$</li>
</ul>
</li>
<li>sequence-to-sequence是一个端到端的系统，反向传播在整个系统中。</li>
</ul>
<h4 id="greedy-decoding"><a href="#greedy-decoding" class="headerlink" title="greedy decoding"></a>greedy decoding</h4><ul>
<li>在解码器的每一步都使用argmax（但总体的概率并非最大）</li>
<li>无法撤销步骤</li>
</ul>
<h4 id="exhaustive-search-decoding"><a href="#exhaustive-search-decoding" class="headerlink" title="exhaustive search decoding"></a>exhaustive search decoding</h4><ul>
<li>计算所有可能的序列，开销很大，有V^T种可能</li>
</ul>
<h4 id="beam-search-decoding"><a href="#beam-search-decoding" class="headerlink" title="beam search decoding"></a>beam search decoding</h4><ul>
<li>核心思想：在decoder的每一步，跟踪k个最可能的输出（hypotheses）<ul>
<li>k是beam size，通常为5-10</li>
<li>按得分选择$score=log P_{LM}(y_1,\dots,y_t|x)=\Sigma^t_{i=1}log P_{LM}(y_i|y_1,\dots,y_{i-1},x)$</li>
</ul>
</li>
<li>不一定能找到最优解，但效率比穷举高的多</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/BfdEmkpctlAq4bC.png" alt=""></p>
<h5 id="stopping-criterion"><a href="#stopping-criterion" class="headerlink" title="stopping criterion"></a>stopping criterion</h5><ul>
<li>在greedy decoding中会持续解码到$<END>$ token</li>
<li>在beem search decoding 中，可能会提前生成$<END>$ token<ul>
<li>一个hypothesis到达$<END>$，则其已经完成</li>
<li>继续处理其他hypothesis</li>
</ul>
</li>
<li>beem search decoding的停止条件：<ul>
<li>到达预设timestep T</li>
<li>有预设的n个hypothesis已经完成</li>
</ul>
</li>
</ul>
<h5 id="finishing-up"><a href="#finishing-up" class="headerlink" title="finishing up"></a>finishing up</h5><ul>
<li>结束时获得了若干个hypothesis，需要从中选择最合适的</li>
<li>如果直接使用score，则长度越长score越小</li>
<li>需要对score按长度标准化（乘$\frac{1}{T}$）</li>
</ul>
<h4 id="advantages-and-disadvantages-of-NMT"><a href="#advantages-and-disadvantages-of-NMT" class="headerlink" title="advantages and disadvantages of NMT"></a>advantages and disadvantages of NMT</h4><ul>
<li>advantage<ul>
<li>性能更好，翻译更通顺，从上下文使用了更多信息</li>
<li>端到端系统</li>
<li>手动工作更少，不需要特征工程和对每种语言单独建模</li>
</ul>
</li>
<li>disadvantage<ul>
<li>可解释性</li>
<li>可控性（无法制定规则）</li>
<li>安全性</li>
</ul>
</li>
</ul>
<h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h3><ul>
<li>源序列encoding过程需要捕获源语句的所有信息，存在信息瓶颈</li>
<li>attention的核心思想：在decoder的每一步，用encoder的连接来使其能关注到源序列的特定部分</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/wRmljrWgoh15OTi.png" alt=""></p>
<p><em>Lecture 8. self-attention and Transformer</em></p>
<hr>
<h4 id="attention-in-equations"><a href="#attention-in-equations" class="headerlink" title="attention in equations"></a>attention in equations</h4><ul>
<li>encoder hidden states: $h_i\in \mathbb{R}^h$</li>
<li>decoder hidden state on timestep t: $s_t\in \mathbb{R}^h$</li>
<li>attention scores: $e^t=[s^T_th_1,\dots,s_t^Th_N]\in\mathbb{R}^N$</li>
<li>attention distribution: $\alpha^t={\rm softmax}(e^t)\in\mathbb{R}^N$</li>
<li>attention output $\mathbf{a}_t=\Sigma_{i=1}^N\alpha_i^t\mathbf{h}_i\in\mathbb{R}^h$</li>
<li>$[\mathbf{a}_t,s_t]\in\mathbb{R}^{2h}$</li>
</ul>
<h4 id="advantages-of-attention"><a href="#advantages-of-attention" class="headerlink" title="advantages of attention"></a>advantages of attention</h4><ul>
<li>绕过了信息瓶颈的问题，直接允许decoder连接来自source的信息</li>
<li>注意力机制更human-like</li>
<li>可以解决梯度消失问题——direct connection</li>
<li>可解释性<ul>
<li>提供了soft alignment（网络自己学习了alignment）</li>
<li>通过注意力分布可以看到decoder的关注点</li>
</ul>
</li>
</ul>
<h4 id="attention-variants"><a href="#attention-variants" class="headerlink" title="attention variants"></a>attention variants</h4><p><strong>query attends to the values</strong></p>
<p>给定values $h_!,\dots,h_N\in\mathbb{R}^{d_1}$和query $s\in\mathbb{R}^{d_2}$，attention机制包括：</p>
<ul>
<li>计算attention scores $e\in\mathbb{R}^N$</li>
<li>计算softmax获得attention distribution $\alpha={\rm softmax}(e)\in\mathbb{R}^N$</li>
<li>对attention distribution加权求和  $\mathbf{a}=\Sigma_{i=1}^N\alpha_i\mathbf{h}_i\in\mathbb{R}^{d_1}$</li>
</ul>
<p>几种attention方法</p>
<ul>
<li>dot-product attention: $e_i=s^Th_i$<ul>
<li>要求$s$和$h$维度相同</li>
</ul>
</li>
<li>multiplicative attention: $e_i=s^TWH_i$<ul>
<li>$W\in\mathbb{R}^{d_1\times d_2}$是一个额外的权重矩阵</li>
<li>权重似乎过多，需要学习对所有部分的attention</li>
</ul>
</li>
<li>reduced rank multiplicative attention: $e_i=s^T(U^TV)H_i=(Us)^T(Vh_i)$<ul>
<li>$I\in\mathbb{R}^{k\times d_2}, V\in\mathbb{R}^{k\times d_1},k\ll d_1,d_2$</li>
</ul>
</li>
<li>additive attention: $e_i=v^T{\rm tanh}(W_1h_i+W_2s)$<ul>
<li>$W_1\in\mathbb{R}^{d_3\times d_1},W_2\in\mathbb{R}^{d_3\times d_2}$是权重矩阵，$v\in\mathbb{R}^{d_3}$是权重向量</li>
<li>实际上用了一个网络来计算attention</li>
</ul>
</li>
</ul>
<p><em>Lecture 9. Transformers</em></p>
<hr>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Issues-with-RNN"><a href="#Issues-with-RNN" class="headerlink" title="Issues with RNN"></a>Issues with RNN</h3><ul>
<li>Linear interaction distance：捕获两个词的交互需要$O(sequence length)$ steps<ul>
<li>长程依赖问题（梯度难以传播）</li>
<li>线性顺序的输入处理sentence不合理</li>
</ul>
</li>
<li>Lack of parallelizability<ul>
<li>forward 和 backward都需要$O(sequence length)$</li>
</ul>
</li>
</ul>
<h3 id="if-not-recurrence-the-what"><a href="#if-not-recurrence-the-what" class="headerlink" title="if not recurrence, the what?"></a>if not recurrence, the what?</h3><ul>
<li>word window(CNN)<ul>
<li>1D-Conv</li>
<li>maximum interaction distance = sequence length/window size——通过多层卷积</li>
</ul>
</li>
<li>attention</li>
</ul>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h3><h4 id="self-attention-equation"><a href="#self-attention-equation" class="headerlink" title="self-attention equation"></a>self-attention equation</h4><ul>
<li>query $q_i\in\mathbb{R}^d$</li>
<li>key $k_i\in\mathbb{R}^d$</li>
<li>value $v_i\in\mathbb{R}^d$</li>
<li>对self-attention来说，q,k,v来自同一个东西<ul>
<li>最简单的方式是$q_i=k_i=v_i=x_i$</li>
<li>此时$e_{ij}=q_i^Tk_j$</li>
<li>$\alpha_{ij}=\frac{exp(e_{ij})}{\Sigma_{j’}exp(e_{ij’})}$</li>
<li>$output=\Sigma_j{\alpha_{ij}v_j}$</li>
</ul>
</li>
</ul>
<h4 id="Problem1-sequence-order-position-embedding"><a href="#Problem1-sequence-order-position-embedding" class="headerlink" title="Problem1. sequence order(position embedding)"></a>Problem1. sequence order(position embedding)</h4><ul>
<li>self-attention对语序完全不敏感，所以需要编码order</li>
<li>考虑将sequence index编码为vector $p_i$，加到qkv上</li>
</ul>
<h5 id="sinusoidal-position-representations"><a href="#sinusoidal-position-representations" class="headerlink" title="sinusoidal position representations"></a>sinusoidal position representations</h5><p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$<br>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$</p>
<ul>
<li>绝对位置并不那么重要</li>
<li>可以外推到更长的长度（周期性）</li>
<li>没有可学习的参数</li>
</ul>
<h5 id="learned-absolute-position-representation"><a href="#learned-absolute-position-representation" class="headerlink" title="learned absolute position representation"></a>learned absolute position representation</h5><p>学习position embedding matrix $p \in \mathbb{R}^{d\times T}$</p>
<ul>
<li>比较灵活，每个位置都可以根据data去学习</li>
<li>长度固定，无法外推</li>
</ul>
<h4 id="Problem2-lack-of-nonlinearities"><a href="#Problem2-lack-of-nonlinearities" class="headerlink" title="Problem2. lack of nonlinearities"></a>Problem2. lack of nonlinearities</h4><ul>
<li>self-attention只是weighted average，不包含非线性</li>
<li>解决方案<ul>
<li>添加一个feed-forward network对输出做后处理</li>
<li>$m_i=MLP(o_i)=W_2ReLu(W_1o_i+b_1)+b_2$</li>
</ul>
</li>
</ul>
<h4 id="Problem3-“don’t-look-at-the-feature”"><a href="#Problem3-“don’t-look-at-the-feature”" class="headerlink" title="Problem3. “don’t look at the feature”"></a>Problem3. “don’t look at the feature”</h4><ul>
<li>在decoder中需要mask</li>
<li>mask out attention to future words——将attention scores设置为$-\infty$</li>
</ul>
<h3 id="Transformer-1"><a href="#Transformer-1" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="key-query-value-attention"><a href="#key-query-value-attention" class="headerlink" title="key-query-value attention"></a>key-query-value attention</h4><ul>
<li>$k_i=Kx_i, K \in\mathbb{R}^{d\times d}\text{ is the key matrix}$</li>
<li>$q_i=Qx_i, Q \in\mathbb{R}^{d\times d}\text{ is the query matrix}$</li>
<li>$v_i=Vx_i, V \in\mathbb{R}^{d\times d}\text{ is the value matrix}$</li>
<li>$output=softmax(XQK^TX^T)XV$</li>
</ul>
<h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><ul>
<li>$Q_l,K_l,V_l\in \mathbb{R}^{d\times\frac{d}{h}}$</li>
<li>计算attention然后concat</li>
</ul>
<h4 id="Residual-connection"><a href="#Residual-connection" class="headerlink" title="Residual connection"></a>Residual connection</h4><ul>
<li>instead of $X^{(i)}=Layer(X^{(i-1)})$</li>
<li>let $X^{(i)}=X^{(i-1)+Layer(X^{(i-1)})$</li>
</ul>
<h4 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h4><ul>
<li>layernorm的成功可能因为其对梯度的norm</li>
<li>$x\in\mathbb{R}^d$是一个单独的word vector</li>
<li>$\mu=\Sigma_{j=1}^dx_j\in \mathbb{R}$是均值</li>
<li>$\sigma=\sqrt{\frac{1}{d}\Sigma_{j=1}^d(x_j-\mu)^2}\in\mathbb{R}$是标准差</li>
<li>$\gamma\in\mathbb{R}^d,\beta\in\mathbb{R}^d$是可学习的gain和bias参数(工业上的习惯)</li>
<li>$output=\frac{x-\mu}{\sigma+\epsilon}*\gamma+\beta$</li>
</ul>
<h4 id="scaled-dot-product"><a href="#scaled-dot-product" class="headerlink" title="scaled dot-product"></a>scaled dot-product</h4><ul>
<li>向量维度很大时，dot-product也会很大，导致softmax很陡峭，梯度很小</li>
<li>${\rm Attention}(Q,K,V)={\rm softmax}(\frac{QK_T}{\sqrt{d_k}})V$</li>
</ul>
<h4 id="why-fix-about-Transformer"><a href="#why-fix-about-Transformer" class="headerlink" title="why fix about Transformer"></a>why fix about Transformer</h4><ul>
<li>quadratic compute in self attention<ul>
<li>linear in RNN</li>
</ul>
</li>
<li>position representation<ul>
<li>relative linear position attention</li>
<li>dependency syntax-based position</li>
</ul>
</li>
</ul>
<h4 id="varient-of-Transformer"><a href="#varient-of-Transformer" class="headerlink" title="varient of Transformer"></a>varient of Transformer</h4><ul>
<li>for quadratic compute<ul>
<li>投影到更低维度计算attention （linformer）</li>
<li>不计算所有的pairs，只计算一部分（BigBird）</li>
</ul>
</li>
</ul>
<h4 id="for-pretrain"><a href="#for-pretrain" class="headerlink" title="for pretrain"></a>for pretrain</h4><ul>
<li>如果只用decoder，则不需要cross attention和layernorm</li>
</ul>
<p><em>Lecture 10. Transformers and pretrain</em></p>
<hr>
<h2 id="word-structure-and-subword-models"><a href="#word-structure-and-subword-models" class="headerlink" title="word structure and subword models"></a>word structure and subword models</h2><ul>
<li>测试时遇到的OOV word将会被映射为UNK</li>
<li>有限的词汇显然不能满足很多语境</li>
</ul>
<h3 id="the-byte-pair-encoding-algorithm"><a href="#the-byte-pair-encoding-algorithm" class="headerlink" title="the byte-pair encoding algorithm"></a>the byte-pair encoding algorithm</h3><ul>
<li>从只包含单个字母的vocabulary开始</li>
<li>寻找最常相邻出现的字母，将其添加为subword</li>
<li>不断添加直到特定的大小</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/HGZQu7p3WcgEf9L.png" alt=""></p>
<h2 id="Motivating-model-pretraining-from-word-embedding"><a href="#Motivating-model-pretraining-from-word-embedding" class="headerlink" title="Motivating model pretraining from word embedding"></a>Motivating model pretraining from word embedding</h2><h3 id="two-types-of-pretrain"><a href="#two-types-of-pretrain" class="headerlink" title="two types of pretrain"></a>two types of pretrain</h3><h4 id="pretrained-word-embedding"><a href="#pretrained-word-embedding" class="headerlink" title="pretrained word embedding"></a>pretrained word embedding</h4><ul>
<li>仅预训练词向量</li>
<li>预训练的训练数据必须能够充分学习上下文<ul>
<li>一个word的预训练词向量是固定的，与上下文无关</li>
</ul>
</li>
<li>整个网络的大部分仍是随机初始化的</li>
</ul>
<h4 id="pretraining-whole-models"><a href="#pretraining-whole-models" class="headerlink" title="pretraining whole models"></a>pretraining whole models</h4><ul>
<li>所有参数都被pretrain</li>
<li>可以学习语言的概率分布</li>
</ul>
<h3 id="three-ways-of-model-pretrain"><a href="#three-ways-of-model-pretrain" class="headerlink" title="three ways of model pretrain"></a>three ways of model pretrain</h3><ul>
<li>decoder<ul>
<li>LM</li>
<li>生成任务，看不到future word</li>
</ul>
</li>
<li>encoder<ul>
<li>有双向的context</li>
<li>如何训练？？</li>
</ul>
</li>
<li>encoder-decoder </li>
</ul>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul>
<li>finetune可以忽略pretrained model原本的任务<br><img src="https://s2.loli.net/2022/01/27/xKmtgsUA3S4Y61V.png" alt=""></li>
<li><p>可以继续用generator的方法(Prompt)<br><img src="https://s2.loli.net/2022/01/27/PrljySvV9sBF7RW.png" alt=""></p>
</li>
<li><p>GPT</p>
<ul>
<li>12层 Transformer decoder </li>
</ul>
</li>
<li>GPT-2<ul>
<li>更大的GPT</li>
</ul>
</li>
</ul>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul>
<li>encoder方法中可以看到双向的上下文，不能直接用LM方法训练</li>
<li><p>Idea：用[MASK] token盖住部分word，让模型预测被盖住的word</p>
<ul>
<li>只添加masked word 的loss，称作Masked LM </li>
</ul>
</li>
<li><p>BERT</p>
<ul>
<li>预测15%的(sub)word<ul>
<li>80% 用[mask]替换</li>
<li>10% 用随机的token替换</li>
<li>10% 不变（但仍然predict）</li>
</ul>
</li>
<li>bert也预测下一个chunk是真实follow的还是随机采样的，即next sentence prediction（NSP）</li>
</ul>
</li>
</ul>
<h5 id="limitation-of-pretrained-encoders"><a href="#limitation-of-pretrained-encoders" class="headerlink" title="limitation of pretrained encoders"></a>limitation of pretrained encoders</h5><ul>
<li>在生成任务上最好使用decoder</li>
<li>bert在自回归任务上并没有那么好</li>
</ul>
<h5 id="extension-of-bert"><a href="#extension-of-bert" class="headerlink" title="extension of bert"></a>extension of bert</h5><ul>
<li>RoBERTa：train BERT for longer and remove next sentence prediction</li>
<li>SpanBERT：连续mask——更难且有用的任务</li>
</ul>
<h4 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h4><p><img src="https://s2.loli.net/2022/01/27/ezkWJgcwqCl6hjA.png" alt=""></p>
<ul>
<li>Encoder编码prefix且不进行预测，Decoder预测后面的部分</li>
<li><p>encoder从context受益，decoder则像LM一样预测</p>
</li>
<li><p>T5<br><img src="https://s2.loli.net/2022/01/27/HTqr6XfiCj8IEZS.png" alt=""></p>
<ul>
<li>span corruption<ul>
<li>从输入中用特定的占位符去替代不同长度的文本（和BERT不同的是，T5只知道此处缺失，不知道缺失的长度）</li>
<li>decoder解码出缺失部分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="very-large-model-and-in-context-learning"><a href="#very-large-model-and-in-context-learning" class="headerlink" title="very large model and in-context learning"></a>very large model and in-context learning</h4><ul>
<li>两种使用pretrained model的方式：<ul>
<li>prompt</li>
<li>finetune</li>
</ul>
</li>
<li>GPT-3<ul>
<li>seems to perform some kind of learning <strong>withou gradient steps</strong> simply from examples you provide within their contexts</li>
<li>in-context learning：甚至没有finetune</li>
</ul>
</li>
</ul>
<p><em>Lecture 11. QA (Chen Danqi)</em></p>
<hr>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><h3 id="Read-Comprehension"><a href="#Read-Comprehension" class="headerlink" title="Read Comprehension"></a>Read Comprehension</h3><ul>
<li><p>理解一段text，然后回答一段问题 $(Passage, Question)-&gt;Answer$</p>
</li>
<li><p>Problem formulation for SQuAD(Stanford的QA数据集，answer在passage中出现)</p>
<ul>
<li>Input: $C=(c_1,c_2,\dots,c_N),Q=(q_1,q_2,\dots,q_M),c_i,q_i\in V$</li>
<li>Output: $1\le start \le end le N$</li>
</ul>
</li>
<li><p>neural models for reading comprehension</p>
<ul>
<li>LSTM-based model</li>
<li>BERT finetune model </li>
</ul>
</li>
</ul>
<h4 id="LSTM-based-model"><a href="#LSTM-based-model" class="headerlink" title="LSTM-based model"></a>LSTM-based model</h4><ul>
<li>MT任务中的source和target变成passge和question</li>
<li>需要通过attention建模哪些word和question最相关，以及和question中哪部分word相关</li>
<li>不需要auto regressive decoder，而是训练预测start和end位置的分类器</li>
</ul>
<h5 id="BiDAF-Bidirectional-Attention-Flow"><a href="#BiDAF-Bidirectional-Attention-Flow" class="headerlink" title="BiDAF(Bidirectional Attention Flow)"></a>BiDAF(Bidirectional Attention Flow)</h5><p><img src="https://s2.loli.net/2022/01/27/wax2cASZhqPltGo.png" alt=""></p>
<ul>
<li>Encoding：character embed layer+word embed layer+phrase embed layer<ul>
<li>$e(c_i)=f([Glove(c_i);charEmb(c_i)])$<br>$e(q_i)=f([Glove(q_i);charEmb(q_i)])$<br>将word embedding（GloVe）和character embedding（CNN+maxpooling over character embedding）拼接在一起，并通过high-way networks</li>
<li>用Bi-LSTM生成context和query的上下文embedding</li>
</ul>
</li>
<li>attention：attention flow layer<ul>
<li>context-to-query attention：对context中的word，寻找query中与之最相关的word</li>
<li>query-to-context attention</li>
<li>计算每个词对的similarity score<br>$S_{i,j}=w_{sim}^T[c_i;q_j;c_i\odot q_j]\in\mathbb{R}, w_{sim}\in\mathbb{R}^{6H}$<br>c2q: 找出和c_i相关的question words $\alpha_{i,j}=softmax_j(S_{i,j})\in\mathbb{R}, a_i=\Sigma_j\alpha_{i,j}q_j\in\mathbb{R}^{2H}$<br>q2c: 找出context中重要的词 $\beta_i=softmax_i(max_j(S_{i,j}))\in\mathbb{R}^{N}, b_i=\Sigma_i\beta_ic_i\in\mathbb{R}^{2H}$<br>output: $g_i=[c_i;a_i;c_i\odot a_i;ci\odot b_i]\in\mathbb{R}^{8H}$</li>
</ul>
</li>
<li>modeling and output layer<ul>
<li>modeling layer： 将$g_i$送进两层Bi-LSTM<br>$m_i=BiLSTM(G_i)\in\mathbb{R}^{2H}$<ul>
<li>attention layer用于建模query和context的关系</li>
<li>modeling layer用于建模 context内部的关系</li>
</ul>
</li>
<li>output layer： 两个用于预测start和end位置的classifer<br>$p_{start}=softmax(w_s^T[g_i;m_i])$<br>$p_{end}=softmax(w_e^T[g_i;m_i’]), m_i’=BiLSTM(m_i)\in\mathbb{R}^{2H}$</li>
</ul>
</li>
</ul>
<h4 id="Bert-for-reading-comprehension"><a href="#Bert-for-reading-comprehension" class="headerlink" title="Bert for reading comprehension"></a>Bert for reading comprehension</h4><p><img src="https://s2.loli.net/2022/01/27/VqIZBAheSKRJbUg.png" alt=""></p>
<h4 id="comparisons"><a href="#comparisons" class="headerlink" title="comparisons"></a>comparisons</h4><ul>
<li>BiDAF等模型捕获question和passage的关系</li>
<li>BERT在question和passage的concat上做self-attention（attention(p,q),attention(q,p),attention(p,p),attention(q,q)），包含了q和p的关系</li>
<li>Clark and Gardner, 2018证明在BiDAF中添加attention(p,p)可以获得提升</li>
</ul>
<p><em>Lecture 12.natural language generation（Antoine Bosselut）</em></p>
<hr>
<h2 id="natural-language-generation（Antoine-Bosselut）"><a href="#natural-language-generation（Antoine-Bosselut）" class="headerlink" title="natural language generation（Antoine Bosselut）"></a>natural language generation（Antoine Bosselut）</h2><h3 id="what-is-NLG"><a href="#what-is-NLG" class="headerlink" title="what is NLG"></a>what is NLG</h3><ul>
<li>NLG task<ul>
<li>Machine Translation</li>
<li>dialogue systems</li>
<li>Summarization</li>
<li>Data-to-Text Generation</li>
<li>visual description</li>
<li>creative generation</li>
</ul>
</li>
</ul>
<h3 id="formulating-for-NLG"><a href="#formulating-for-NLG" class="headerlink" title="formulating for NLG"></a>formulating for NLG</h3><ul>
<li>auto-regressive model</li>
<li>Step： $S=f({y_{&lt;t},\theta})$<br>  $P(y_t|{y_{&lt;t}})=\frac{exp(S_w)}{\Sigma_{w’\in V}exp(S_{w’})}$</li>
<li>Loss: $L=-\Sigma_t logP(y_t|{y_{&lt;t}})$ </li>
</ul>
<h3 id="decoding-from-NLG-models"><a href="#decoding-from-NLG-models" class="headerlink" title="decoding from NLG models"></a>decoding from NLG models</h3><h4 id="Greedy-methods"><a href="#Greedy-methods" class="headerlink" title="Greedy methods"></a>Greedy methods</h4><ul>
<li>argmax decoding</li>
<li>beam search</li>
</ul>
<h4 id="get-random-sampling"><a href="#get-random-sampling" class="headerlink" title="get random: sampling"></a>get random: sampling</h4><h5 id="top-k"><a href="#top-k" class="headerlink" title="top-k"></a>top-k</h5><ul>
<li>每次只选top k个token</li>
<li>增大k会让模型更多样也容易出错</li>
<li>减小k会让模型更greedy</li>
</ul>
<h5 id="top-p"><a href="#top-p" class="headerlink" title="top-p"></a>top-p</h5><ul>
<li>top-k的问题<ul>
<li>对平滑的分布，top-k会削减许多可能的选项</li>
<li>对陡峭的分布，top-k会添加许多不太可能的选项</li>
</ul>
</li>
<li>solution<ul>
<li>根据分布的特点，动态选取k</li>
<li>调整（rebalance）概率分布：除以一个超参数 $\tau$后再计算softmax</li>
</ul>
</li>
</ul>
<h4 id="re-balancing-distribution"><a href="#re-balancing-distribution" class="headerlink" title="re-balancing distribution"></a>re-balancing distribution</h4><p>仅依赖model输出的概率分布可能并不可靠</p>
<ul>
<li>根据n-gram的统计数据来调整概率分布</li>
<li>根据梯度反向传播调整概率分布<br><img src="https://s2.loli.net/2022/01/27/YcwPTaDo8xAeZSn.png" alt=""></li>
</ul>
<h3 id="traning-NLG-models"><a href="#traning-NLG-models" class="headerlink" title="traning NLG models"></a>traning NLG models</h3><h4 id="Maximum-Likelihood-Training"><a href="#Maximum-Likelihood-Training" class="headerlink" title="Maximum Likelihood Training"></a>Maximum Likelihood Training</h4><p>$L=-\Sigma_t logP(y_t|{y_{&lt;t}})$ </p>
<p>不利于多样化的text generation</p>
<h4 id="Unlikelihood-Training"><a href="#Unlikelihood-Training" class="headerlink" title="Unlikelihood Training"></a>Unlikelihood Training</h4><p>给定一个不期望得到的token 集合$\mathcal{C}$，降低其在上下文中的likelihood</p>
<p>$\mathcal{L}_{UL}^t=-\Sigma_{y_{neg}\in\mathcal{C}}log(1-P(y_{neg}|\{y\}_{&lt;t}))$</p>
<p>将unlikelihood和likelihood结合<br>$\mathcal{L}_{ULE}=\mathcal{L}_{MLE}+\alpha\mathcal{L}_{UL}$</p>
<h4 id="exposure-bias问题"><a href="#exposure-bias问题" class="headerlink" title="exposure bias问题"></a>exposure bias问题</h4><ul>
<li>训练时，模型输入的是gold context；生成时，模型输入是decoder之前的输出</li>
<li>Solution<ul>
<li>scheduled sampling<ul>
<li>以概率p解码出一个token并送入模型作为下一个input（而非gold token）</li>
<li>在训练过程中逐渐增大p</li>
<li>但训练中同时也会生成一些奇怪的输出</li>
</ul>
</li>
</ul>
</li>
<li>dataset aggregation<ul>
<li>在训练过程中，生成一些新序列加入训练集</li>
</ul>
</li>
<li>sequence re-writing<ul>
<li>在corpus中搜索一个human-written的prototype text，</li>
</ul>
</li>
<li>reinforcement learning</li>
</ul>
<h3 id="evaluating-NLG"><a href="#evaluating-NLG" class="headerlink" title="evaluating NLG"></a>evaluating NLG</h3><ul>
<li>N-gram overlap metrics——BLEU、ROUGE、METEOR（效果不好）</li>
<li>semantic overlap metrics——PYRAMID、SPICE、SPIDEr</li>
<li>model-based metrics<ul>
<li>word distance functions<ul>
<li>vector similarity</li>
<li>word mover’s distance</li>
<li>bert score</li>
</ul>
</li>
<li>beyond word matching<ul>
<li>sentnce movers similarity</li>
<li>BLEURT</li>
</ul>
</li>
</ul>
</li>
<li>human evalutions</li>
</ul>
<p><em>Lecture 13. Coreference resolution</em></p>
<hr>
<h2 id="Co-reference-resolution"><a href="#Co-reference-resolution" class="headerlink" title="Co-reference resolution"></a>Co-reference resolution</h2><h3 id="what-is-co-reference-resolution"><a href="#what-is-co-reference-resolution" class="headerlink" title="what is co-reference resolution"></a>what is co-reference resolution</h3><ul>
<li><p>识别出文本中共指同一个实体的单词</p>
</li>
<li><p>application</p>
<ul>
<li>文本理解<ul>
<li>信息抽取、QA、summarization</li>
</ul>
</li>
<li>机器翻译</li>
<li>对话系统</li>
</ul>
</li>
</ul>
<h3 id="co-reference-resolution-in-two-steps"><a href="#co-reference-resolution-in-two-steps" class="headerlink" title="co-reference resolution in two steps"></a>co-reference resolution in two steps</h3><ul>
<li>detect the mentions——easy</li>
<li>cluster the mentions——hard</li>
</ul>
<h4 id="mention-detection"><a href="#mention-detection" class="headerlink" title="mention detection"></a>mention detection</h4><ul>
<li>detect a span of text referring to some entity<ul>
<li>pronouns: I, your, it, she, him, etc.</li>
<li>named entitied: people, place, Biden, etc.</li>
<li>noun phrase: a dog, etc.</li>
</ul>
</li>
<li>简单但没那么简单<ul>
<li>很多结果并非mentions</li>
<li><strong>it</strong> is sunny、<strong>every student</strong> </li>
</ul>
</li>
</ul>
<h5 id="how-to-deal-with-bad-mentions"><a href="#how-to-deal-with-bad-mentions" class="headerlink" title="how to deal with bad mentions"></a>how to deal with bad mentions</h5><ul>
<li>可以训练一个分类器来过滤掉spurious mentions<ul>
<li>POS tagger、NER、parser</li>
</ul>
</li>
<li>更常用的方法：让所有的mentions都作为”candidate mentions”<ul>
<li>在co-reference 系统运行完后，丢弃掉这些mention</li>
</ul>
</li>
</ul>
<h3 id="some-linguistics"><a href="#some-linguistics" class="headerlink" title="some linguistics"></a>some linguistics</h3><ul>
<li>co-reference：两个mention指代同一个实体</li>
<li><p>anaphora：一个照应词anaphor指代另一个先行词antecedent</p>
</li>
<li><p>并非所有的anaphoric关系都是co-referential</p>
<ul>
<li>并非所有的词都有reference<ul>
<li><em>every dancer twisted her knee</em><br>二者都有</li>
<li><em>no dancer twisted her knee</em><br>存在anaphoric但没有co-referential</li>
</ul>
</li>
<li>bridging anaphora<ul>
<li><em>We went to see a concert last night. The tickets were really expensive.</em></li>
<li>tickets对concert没有co-reference，是bridging anaphora的关系</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/aeiMFBNr3C41jO5.png" alt=""></p>
<h3 id="4-kinds-of-co-reference-models"><a href="#4-kinds-of-co-reference-models" class="headerlink" title="4 kinds of co-reference models"></a>4 kinds of co-reference models</h3><ul>
<li>Rule-based（pronominal anaphora resolution）</li>
<li>mention pair</li>
<li>mention ranking</li>
<li>clustering</li>
</ul>
<h4 id="Hobbs’-naive-algo"><a href="#Hobbs’-naive-algo" class="headerlink" title="Hobbs’ naive algo"></a>Hobbs’ naive algo</h4><ul>
<li>Rule-based</li>
<li><p>并未真正理解语言</p>
</li>
<li><p>Knowledge-based Pronominal Coreference</p>
<ul>
<li>She poured water from the pitcher into the cup until it was full.</li>
<li>She poured water from the pitcher into the cup until it was empty.</li>
<li>The city council refused the women a permit because they feared violence.</li>
<li>The city council refused the women a permit because they advocated violence</li>
</ul>
</li>
</ul>
<h4 id="mention-pair"><a href="#mention-pair" class="headerlink" title="mention pair"></a>mention pair</h4><ul>
<li>训练一个二分类器，对每个mention pair分类</li>
<li>Loss：regular CE loss<ul>
<li>$J=-\Sigma^N_{i=2}\Sigma^i_{j=1}y_{ij}log\ p(m_j,m_i)$</li>
</ul>
</li>
<li><p>选择一定的阈值，置信度大于该阈值即添加co-reference 连接</p>
</li>
<li><p>disadvantage</p>
<ul>
<li>许多mention只有一个co-reference 连接</li>
</ul>
</li>
<li>solution<ul>
<li>对每个mention只计算一个antecedent</li>
</ul>
</li>
</ul>
<h4 id="mention-ranking"><a href="#mention-ranking" class="headerlink" title="mention ranking"></a>mention ranking</h4><ul>
<li>为得分/概率最高的candidate添加连接</li>
<li>Loss：<ul>
<li>$J=\Sigma^{i-1}_{j=1}\mathbb{1}(y_{ij}=1) p(m_j,m_i)$$</li>
</ul>
</li>
</ul>
<h4 id="how-to-calculate-score-probability"><a href="#how-to-calculate-score-probability" class="headerlink" title="how to calculate score/probability"></a>how to calculate score/probability</h4><ul>
<li>Non-neural statistical classifier</li>
<li>Simple neural network</li>
<li>More advanced model using LSTMs, attention, transformers</li>
</ul>
<h4 id="Non-Neural-model-Features"><a href="#Non-Neural-model-Features" class="headerlink" title="Non-Neural model: Features"></a>Non-Neural model: Features</h4><p><img src="https://s2.loli.net/2022/01/27/Y9pQXbrMAOJL723.png" alt=""></p>
<h4 id="neural-model：标准前馈神经网络"><a href="#neural-model：标准前馈神经网络" class="headerlink" title="neural model：标准前馈神经网络"></a>neural model：标准前馈神经网络</h4><p><img src="https://s2.loli.net/2022/01/27/NYhIscpa1WJRMeT.png" alt=""></p>
<ul>
<li>输入word embedding和distance、document genre体裁、speaker infomation等其他特征</li>
</ul>
<h4 id="neural-model：CNN"><a href="#neural-model：CNN" class="headerlink" title="neural model：CNN"></a>neural model：CNN</h4><ul>
<li>忽略了语法信息，并不那么linguistically</li>
</ul>
<h4 id="neural-model：end-to-end-neural-coref-model"><a href="#neural-model：end-to-end-neural-coref-model" class="headerlink" title="neural model：end-to-end neural coref model"></a>neural model：end-to-end neural coref model</h4><ul>
<li>mention ranking model</li>
<li>在FFN基础上使用LSTM、attention</li>
<li>没有mention detection的过程</li>
<li>不再将每段文本都作为candidate，利用attention去修剪</li>
</ul>
<p>模型架构：</p>
<ul>
<li>用<strong>character-level CNN</strong> embed the words</li>
<li>将document送入Bi-LSTM</li>
<li>将每个span表示为向量</li>
<li>计算每对span的得分</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/X53FQpeHSOVnqB8.png" alt=""><br>$\hat x_i$是span中word embedding的attention加权求和。<br><img src="https://s2.loli.net/2022/01/27/mLipzkHxgRZdISs.png" alt=""></p>
<ul>
<li>$score(i,j)=s_m(i)+s_m(j)+s_a(i,j)$</li>
<li>$s_m(i)=w_m\cdot FFNN_m(g_i)$ 表征i是否为mention</li>
<li>$s_a(i,j)=w_a\cdot FFNN_a([g_i,g_j,g_i\circ g_i,\phi(i,j)])$表征i和j是否coreferent</li>
</ul>
<h4 id="neural-model：BERT-based-model"><a href="#neural-model：BERT-based-model" class="headerlink" title="neural model：BERT-based model"></a>neural model：BERT-based model</h4><ul>
<li>idea<ul>
<li>预训练在span-based预测任务上（coref和QA）表现更好的BERT模型</li>
<li>BERT-QA：将coref看作QA task<ul>
<li>Q：mention</li>
<li>A：mention的antecedent</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="co-reference-evaluation"><a href="#co-reference-evaluation" class="headerlink" title="co-reference evaluation"></a>co-reference evaluation</h3><ul>
<li>B-cubed<ul>
<li>对每个mention计算p和r</li>
<li>对P和R取平均</li>
</ul>
</li>
</ul>
<p><em>Lecture14. T5 and large LM:the good, the bad and the ugly(Colin Raffel)</em></p>
<hr>
<h2 id="T5-and-large-LM"><a href="#T5-and-large-LM" class="headerlink" title="T5 and large LM"></a>T5 and large LM</h2><h3 id="larger-LM"><a href="#larger-LM" class="headerlink" title="larger LM"></a>larger LM</h3><ul>
<li>larger corpus</li>
<li>more parameters</li>
<li>more tokens</li>
<li>different optimizer</li>
</ul>
<h3 id="T5-Text-to-text-Transfer-Transformer"><a href="#T5-Text-to-text-Transfer-Transformer" class="headerlink" title="T5(Text-to-text Transfer Transformer)"></a>T5(Text-to-text Transfer Transformer)</h3><ul>
<li>text-to-text task<ul>
<li>MT</li>
<li>CoLA(Corpus of Linguistic Acceptability)</li>
<li>STS-B(Semantic Textual Similarity Benchmark)</li>
<li>summarize</li>
</ul>
</li>
<li>pre-train dataset<ul>
<li>C4 dataset</li>
</ul>
</li>
</ul>
<h4 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h4><ul>
<li>original text<ul>
<li>thank you for inviting me to your party last week</li>
</ul>
</li>
<li>inputs<ul>
<li>thank you <X> me to your party <Y> week</li>
</ul>
</li>
<li>targets<ul>
<li><X> for inviting <Y> last <Z></li>
</ul>
</li>
</ul>
<h4 id="pretrain"><a href="#pretrain" class="headerlink" title="pretrain"></a>pretrain</h4><p><img src="https://s2.loli.net/2022/01/26/7UCwHlkAMSqvYDe.png" alt=""></p>
<p><em>Lecture 14. Integrating knowledge in language models</em></p>
<h2 id="Integrating-knowledge-in-language-models"><a href="#Integrating-knowledge-in-language-models" class="headerlink" title="Integrating knowledge in language models"></a>Integrating knowledge in language models</h2><p>can a LM be used as a KB？</p>
<h3 id="what-does-a-LM-know"><a href="#what-does-a-LM-know" class="headerlink" title="what does a LM know?"></a>what does a LM know?</h3><ul>
<li>预测结果通常是有意义的（语法/类型匹配），但并不正确</li>
<li>原因：<ul>
<li>unseen fact：在训练集中一些fact还没有发生</li>
<li>rare fact：样本不够，无法记住相关fact</li>
<li>model sensitivity：对上下文非常敏感<ul>
<li>x was made in y 和 x was created in y</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="advantages-of-LM-over-traditional-KB"><a href="#advantages-of-LM-over-traditional-KB" class="headerlink" title="advantages of LM over traditional KB"></a>advantages of LM over traditional KB</h3><ul>
<li>LM从大规模非结构化和无标注文本上预训练<ul>
<li>KB需要人工标注和复杂的NLP规则</li>
</ul>
</li>
<li><p>LM支持更灵活的query</p>
</li>
<li><p>challenges</p>
<ul>
<li>难以解释某些结果</li>
<li>有时难以相信</li>
<li>难以修改模型</li>
</ul>
</li>
</ul>
<h3 id="techniques-to-add-knowledge-to-LMs"><a href="#techniques-to-add-knowledge-to-LMs" class="headerlink" title="techniques to add knowledge to LMs"></a>techniques to add knowledge to LMs</h3><ul>
<li>add pretrained entity embeddings<ul>
<li>ERNIE</li>
<li>KnowBERT</li>
</ul>
</li>
<li>use an external memory<ul>
<li>KGLM</li>
<li>KNN-LM</li>
</ul>
</li>
<li>modify the training data<ul>
<li>WKLM</li>
<li>ERNIE, salient span masking</li>
</ul>
</li>
</ul>
<h4 id="add-pretrained-entity-embedding"><a href="#add-pretrained-entity-embedding" class="headerlink" title="add pretrained entity embedding"></a>add pretrained entity embedding</h4><ul>
<li>真实世界的事物通常是实体</li>
<li>pretrained word embedding没有实体的概念<ul>
<li>比如USA和America是指向同一实体的</li>
</ul>
</li>
<li>解决方案<ul>
<li>对指向同一实体的words连接同样的entity embedding<br>entity linking：link mentions in text to entities in a KB</li>
</ul>
</li>
</ul>
<h5 id="techniques-for-training-entity-embeddings"><a href="#techniques-for-training-entity-embeddings" class="headerlink" title="techniques for training entity embeddings"></a>techniques for training entity embeddings</h5><ul>
<li>KG embedding(TransE)</li>
<li>word-entity co-occurrence(Wikipedia2Vec)</li>
<li>Transformer encodings of entity description(BLINK)</li>
</ul>
<h5 id="how-to-incorporate-entity-embeddings-from-a-different-embedding-space"><a href="#how-to-incorporate-entity-embeddings-from-a-different-embedding-space" class="headerlink" title="how to incorporate entity embeddings from a different embedding space"></a>how to incorporate entity embeddings from a different embedding space</h5><ul>
<li>学习一个fusion layer融合context和entity information<br>$h_j=F(W_tw_j+W_Ee_k+b)$</li>
</ul>
<h3 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h3><h4 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h4><p><img src="https://s2.loli.net/2022/01/27/PHzyKTZMmVJdFna.png" alt=""></p>
<ul>
<li>text-encoder<ul>
<li>multi-layer bidirectional Transformer</li>
</ul>
</li>
<li>knowledge encoder<ul>
<li>2 multi-head attention over entity embedding and token embedding</li>
<li>1 fusion layer to combine<br><img src="https://s2.loli.net/2022/01/26/MpydcXslbDKvqOI.png" alt=""></li>
</ul>
</li>
</ul>
<h4 id="pretrain-tasks"><a href="#pretrain-tasks" class="headerlink" title="pretrain tasks"></a>pretrain tasks</h4><ul>
<li>BERT tasks<ul>
<li>MLM</li>
<li>NSP</li>
</ul>
</li>
<li>knowledge pretraining task(dEA, denoising entity autoencoder)<ul>
<li>随机mask掉一个token-entity alignment，然后根据sequence中的实体预测出该token的相关entity<br>$p(e_j|w_i)=\frac{exp(Ww_i\cdot e_j)}{\Sigma_{k=1}^{m}exp(Ww_i\cdot e_k)}$</li>
</ul>
</li>
</ul>
<p>$L_{ERNIE}=L_{MLM}+L_{NSP}+L_{dEA}$</p>
<h4 id="strengths"><a href="#strengths" class="headerlink" title="strengths"></a>strengths</h4><ul>
<li>通过fusion layer和dEA融合了entity和context</li>
<li><p>对知识驱动的下游任务提升很大</p>
</li>
<li><p>remaining challenges</p>
<ul>
<li>需要带entity标注的数据作为输入，需要一个entity linker</li>
<li>需要further pretrain</li>
</ul>
</li>
</ul>
<h3 id="KnowBERT"><a href="#KnowBERT" class="headerlink" title="KnowBERT"></a>KnowBERT</h3><ul>
<li>核心思想：在BERT之外预训练一个entity linker<br>$L_{KnowBERT}=L_{MLM}+L_{NSP}+L_{EL}$</li>
<li>在下游任务中，不需要额外的entity标注</li>
</ul>
<h3 id="KGLM"><a href="#KGLM" class="headerlink" title="KGLM"></a>KGLM</h3><ul>
<li>之前的方法依赖于预训练的entity embedding来从KB编码知识</li>
<li>需要更直接的方法来为LM提供额外知识</li>
<li><p>solution：让模型可以直接访问三元组（key-value store）</p>
</li>
<li><p>advantages</p>
<ul>
<li>可以更好地更新知识</li>
<li>可解释性更强</li>
</ul>
</li>
<li><p>核心思想：在KG上调整LM</p>
</li>
<li>利用entity的信息来预测下一word $P(x^{(t+1)},\epsilon^{(t+1)}|x^{(t)},\dots,x^{(1)},\epsilon^{(t)},\dots,\epsilon^{(1)})$<br>$\epsilon^{(t)}$是t时刻提及的KG entities</li>
</ul>
<h4 id="architecture-1"><a href="#architecture-1" class="headerlink" title="architecture"></a>architecture</h4><ul>
<li>在迭代时构建一个”local” KG<ul>
<li>local KG——full KG的子集：只包含和sequence相关的entities</li>
</ul>
</li>
<li>next word有三类：<ul>
<li>已经在local KG中的related entity<ul>
<li>需要找到最高得分的head和relation</li>
<li>$P(e_h)=softmax(v_h\cdot h_t)$</li>
<li>找到tail entity</li>
<li>从entity扩展token</li>
</ul>
</li>
<li>不在local KG中的new entity<ul>
<li>在full KG中找到tail entity</li>
</ul>
</li>
<li>非entity<ul>
<li>退化</li>
</ul>
</li>
</ul>
</li>
<li>用LSTM的hidden state预测net word 的类型</li>
</ul>
<h3 id="KNN-LM"><a href="#KNN-LM" class="headerlink" title="KNN-LM"></a>KNN-LM</h3><ul>
<li>核心思想：学习句子间的similarity要比预测next word更容易，尤其对long-tail pattern更有效<ul>
<li>寻找k个相似句子</li>
<li>从中检索相应的value（i.e. next word）</li>
<li>将knn概率和LM概率结合</li>
</ul>
</li>
</ul>
<h3 id="WKLM"><a href="#WKLM" class="headerlink" title="WKLM"></a>WKLM</h3><ul>
<li>之前的方法是从外部引入知识或使用额外的memory</li>
<li><p>将knowledge引入非结构文本</p>
</li>
<li><p>核心思想：训练模型去辨别知识的真伪</p>
</li>
<li>用同类型的其他entity替换，生成negative knowledge statement</li>
<li>Loss：$L=L_{MLM}+L_{entRep}$，前者是token-level的，后者是entity-level的<br><img src="https://s2.loli.net/2022/01/27/x9rEy6kcVaiFlO8.png" alt=""></li>
</ul>
<h3 id="Learn-inductive-biases-through-masking"><a href="#Learn-inductive-biases-through-masking" class="headerlink" title="Learn inductive biases through masking"></a>Learn inductive biases through masking</h3><h4 id="ERNIE-1"><a href="#ERNIE-1" class="headerlink" title="ERNIE"></a>ERNIE</h4><p><img src="https://s2.loli.net/2022/01/27/VuRlc84eyKxYXfA.png" alt=""></p>
<ul>
<li>使用更有效的mask<ul>
<li>phrase-level</li>
<li>entity-level</li>
</ul>
</li>
</ul>
<h4 id="Salient-span-masking"><a href="#Salient-span-masking" class="headerlink" title="Salient span masking"></a>Salient span masking</h4><ul>
<li>mask掉salient spans<ul>
<li>named entities </li>
<li>dates</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/" class="post-title-link" itemprop="url">(AAAI2022) When Shift Operation Meets Vision Transformer:An Extremely Simple Alternative to Attention Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-24 14:58:16" itemprop="dateCreated datePublished" datetime="2022-01-24T14:58:16+08:00">2022-01-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 23:35:51" itemprop="dateModified" datetime="2022-02-14T23:35:51+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>MicroSoft 在AAAI2022的一篇文章，将ViT中的attention机制替换为无参数的shift操作，在几个主流任务中也取得了相当好的结果——attention机制或许并非ViT成功的关键因素。<br>论文地址：<a href="https://arxiv.org/pdf/2201.10801.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2201.10801.pdf</a><br>开源代码：<a href="https://github.com/microsoft/SPACH" target="_blank" rel="noopener">https://github.com/microsoft/SPACH</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/20/attention%20is%20all%20you%20need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/attention%20is%20all%20you%20need/" class="post-title-link" itemprop="url">Reread Attention Is All You Need</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-20 18:53:16" itemprop="dateCreated datePublished" datetime="2022-01-20T18:53:16+08:00">2022-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-26 00:09:44" itemprop="dateModified" datetime="2022-01-26T00:09:44+08:00">2022-01-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Transformer/Attention/" itemprop="url" rel="index"><span itemprop="name">Attention</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>跟李沐再读<em>Attention Is All You Need</em>。<br>视频链接：<a href="https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/20/attention%20is%20all%20you%20need/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/" class="post-title-link" itemprop="url">(EMNLP2021) A Semantic Filter Based on Relations for Knowledge Graph Completion</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-20 14:20:16" itemprop="dateCreated datePublished" datetime="2022-01-20T14:20:16+08:00">2022-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 20:17:03" itemprop="dateModified" datetime="2022-02-14T20:17:03+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/KGC/" itemprop="url" rel="index"><span itemprop="name">KGC</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>来自国防科技大学，针对关系和头尾实体的联系，提出应该为每个关系r定义一个对头尾实体的滤波器。<br>论文地址：<a href="https://aclanthology.org/2021.emnlp-main.625.pdf" target="_blank" rel="noopener">https://aclanthology.org/2021.emnlp-main.625.pdf</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/" class="post-title-link" itemprop="url">(EMNLP2021) Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-10 14:20:16" itemprop="dateCreated datePublished" datetime="2021-12-10T14:20:16+08:00">2021-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 23:46:11" itemprop="dateModified" datetime="2022-02-14T23:46:11+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">短文本分类</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>来自百度和清华，提出为文本构建多个异质图的方法。<br>论文地址：<a href="https://arxiv.org/abs/2111.00180v1" target="_blank" rel="noopener">https://arxiv.org/abs/2111.00180v1</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/" class="post-title-link" itemprop="url">(EMNLP2019) Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-27 18:53:16" itemprop="dateCreated datePublished" datetime="2021-11-27T18:53:16+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 20:10:02" itemprop="dateModified" datetime="2022-02-14T20:10:02+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">短文本分类</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>来自北邮团队，提出了可以集成多种额外信息的HIN和dual-level的GAT，利用附加信息帮助半监督STC。在AGNews上由TextGCN的67.67涨到72.10。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/" class="post-title-link" itemprop="url">(EMNLP2021) SimCSE: Simple Contrastive Learning of Sentence Embeddings</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-29 14:20:16" itemprop="dateCreated datePublished" datetime="2021-10-29T14:20:16+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-14 23:50:21" itemprop="dateModified" datetime="2022-02-14T23:50:21+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sentence-embedding/" itemprop="url" rel="index"><span itemprop="name">Sentence embedding</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>From Princeton Chen Danqi，提出对比学习的sentence embedding学习框架 ，发表在EMNLP2021。</p>
<p>论文地址: <a href="https://arxiv.org/pdf/2104.08821.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2104.08821.pdf</a><br>开源代码: <a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener">https://github.com/princeton-nlp/SimCSE</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/" class="post-title-link" itemprop="url">统计学习方法（五）--决策树</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-01-27 14:06:28" itemprop="dateCreated datePublished" datetime="2021-01-27T14:06:28+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-17 14:53:00" itemprop="dateModified" datetime="2021-02-17T14:53:00+08:00">2021-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>决策树（design tree）是一种基本的分类与回归方法。 是if-then规则的集合或者定义在特征空间与类空间上的条件概率分布。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="post-title-link" itemprop="url">统计学习方法（四）--朴素贝叶斯</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-01-27 00:25:15 / 修改时间：14:05:26" itemprop="dateCreated datePublished" datetime="2021-01-27T00:25:15+08:00">2021-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">统计学习方法</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>420</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>朴素贝叶斯（naive Bayes）是基于Bayes定理与特征条件独立假设的分类方法。基于特征条件独立假设学习输入输出的联合概率分布，然后基于此模型对给定输入x利用Bayes定理求出后验概率最大的输出y。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rooki3Ray"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Rooki3Ray</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/rooki3ray" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;rooki3ray" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2922859032@qq.com" title="E-Mail → mailto:2922859032@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5355766445" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5355766445" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="/2922859032" title="QQ → 2922859032"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/rooki3ray" title="https:&#x2F;&#x2F;github.com&#x2F;rooki3ray" rel="noopener" target="_blank">GitHub</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rooki3Ray</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">154k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:20</span>
</div>



        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
