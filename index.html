<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"rooki3ray.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Rooki3Ray | Cyber Security">
<meta property="og:url" content="http://rooki3ray.github.io/index.html">
<meta property="og:site_name" content="Rooki3Ray | Cyber Security">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Rooki3Ray">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://rooki3ray.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Rooki3Ray | Cyber Security</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c5dcd17d515f19e2d355facf9d8e773a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Rooki3Ray | Cyber Security</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="æœç´¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/" class="post-title-link" itemprop="url">(AAAI2022) Deformable Graph Convolutional Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-02-05 16:58:16" itemprop="dateCreated datePublished" datetime="2022-02-05T16:58:16+08:00">2022-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 23:39:12" itemprop="dateModified" datetime="2022-02-14T23:39:12+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>3 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>æå‡ºåœ¨å¤šä¸ªæ½œåœ¨ç©ºé—´ä¸Šè¿›è¡Œå·ç§¯å¹¶æ ¹æ®relationåŠ¨æ€è°ƒæ•´å·ç§¯çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†åœ¨å­¦ä¹ node representationåŸºç¡€ä¸ŠåŒæ—¶å­¦ä¹ node positional embeddingsçš„æ¡†æ¶ã€‚</p>
<p>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/2112.14438" target="_blank" rel="noopener">https://arxiv.org/abs/2112.14438</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/02/05/(AAAI2022)%20Deformable%20Graph%20Convolutional%20Networks/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/26/CS224n/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/26/CS224n/" class="post-title-link" itemprop="url">Stanford-CS224n</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-26 18:53:16" itemprop="dateCreated datePublished" datetime="2022-01-26T18:53:16+08:00">2022-01-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 20:08:19" itemprop="dateModified" datetime="2022-02-14T20:08:19+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>28k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>25 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CS224n"><a href="#CS224n" class="headerlink" title="CS224n"></a>CS224n</h1><p>å­¦ä¹ Stanfordçš„CS224n Natural Language Processing with Deep Learningã€‚(<a href="https://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">è¯¾ç¨‹é“¾æ¥</a>)</p>
<p><code>&lt;!-- more --&gt;</code></p>
<p>[TOC]</p>
<p><strong><em>Lecture 1. Introduction and word vector</em></strong></p>
<hr>
<h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><ul>
<li>basicå’Œkey mothodsï¼šRNN attention transformers</li>
<li>ç†è§£å¯¹äººç±»è‡ªç„¶è¯­è¨€ç†è§£ä¸­é—®é¢˜</li>
<li>ç”¨PyTorchæ„å»ºæ¨¡å‹ï¼Œè§£å†³word meaningï¼Œdependency parsingï¼Œmachine translationï¼ŒQAç­‰é—®é¢˜</li>
</ul>
<h3 id="NLP-levels"><a href="#NLP-levels" class="headerlink" title="NLP levels"></a>NLP levels</h3><p><img src="https://s2.loli.net/2022/01/26/sblwBhYyJ8DS2g9.png" alt=""></p>
<p>è¾“å…¥speech text-&gt;å•è¯åˆ†æ-&gt;<strong>å¥æ³•åˆ†æ-&gt;è¯­ä¹‰ç†è§£</strong>-&gt;ä¸Šä¸‹æ–‡å¤„ç†</p>
<h3 id="human-language"><a href="#human-language" class="headerlink" title="human language"></a>human language</h3><p>language is <strong>discrete/symbolic/categorical signaling system</strong></p>
<ul>
<li>rocket=ğŸš€ violin=ğŸ»</li>
<li>love loooove</li>
</ul>
<p>äººç±»è¯­è¨€æ˜¯ä¸€å¥—ç¬¦å·ç³»ç»Ÿï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸå°†å¤§è„‘è§†ä¸ºæœ‰è¿ç»­çš„æ¿€æ´»æ¨¡å¼ã€‚</p>
<h3 id="deep-learning"><a href="#deep-learning" class="headerlink" title="deep learning"></a>deep learning</h3><ul>
<li>machine learningï¼šç”±äººç±»å®¡è§†ä¸€ä¸ªç‰¹å®šçš„é—®é¢˜ï¼Œæ‰¾å‡ºé—®é¢˜ä¸­çš„å…³é”®è¦ç´ ï¼Œè®¾è®¡useful features/signalsï¼Œä½¿ç”¨è¿™äº›featuresè§£å†³é—®é¢˜ï¼Œæœºå™¨åšæ•°å€¼ä¼˜åŒ–ã€‚å¤§éƒ¨åˆ†å·¥ä½œæ˜¯äººç±»çš„æ•°æ®åˆ†æã€‚<ul>
<li>æ‰‹åŠ¨è®¾è®¡çš„ç‰¹å¾è¿‡äºå…·ä½“ï¼Œé•¿æ—¶é—´è®¾è®¡å’Œè°ƒæ•´</li>
</ul>
</li>
<li>deep learningï¼šè¡¨å¾å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè¾“å…¥åŸå§‹çš„çœŸå®ä¸–ç•Œä¿¡å·ï¼Œæœºå™¨è‡ªå·±å­¦ä¹ ç‰¹å¾ï¼Œå¯ä»¥å¾—åˆ°å¤šå±‚çš„learned representationsã€‚<ul>
<li>deep learningçš„ç‰¹å¾é€‚åº”æ€§å¼ºï¼Œè®­ç»ƒå¿«ï¼Œå¯ä»¥çµæ´»é€šç”¨åœ°å­¦ä¹ </li>
<li>æµ·é‡æ•°æ®+ç®—åŠ›æå‡</li>
</ul>
</li>
</ul>
<h2 id="Word-Vector-represent-the-meaning-of-a-word"><a href="#Word-Vector-represent-the-meaning-of-a-word" class="headerlink" title="Word Vector (represent the meaning of a word)"></a>Word Vector (represent the meaning of a word)</h2><h3 id="meaning"><a href="#meaning" class="headerlink" title="meaning"></a>meaning</h3><ul>
<li>ç”¨ä¸€ä¸ªè¯ã€è¯ç»„è¡¨ç¤ºçš„æ¦‚å¿µ</li>
<li>ä¸€ä¸ªäººæƒ³ç”¨è¯­è¨€ã€ç¬¦å·è¡¨ç¤ºçš„æƒ³æ³•</li>
<li>è¡¨è¾¾åœ¨æ–‡å­¦ã€è‰ºæœ¯ä½œå“ç­‰æ–¹é¢çš„æ€æƒ³</li>
</ul>
<p>ç†è§£meaningçš„linguistic wayï¼š<br><img src="https://s2.loli.net/2022/01/26/u2oiGExBDya5eWH.png" alt=""><br>ï¼ˆæŒ‡ç§°è¯­ä¹‰ï¼‰</p>
<h3 id="how-do-we-have-usable-meaning-in-a-computer"><a href="#how-do-we-have-usable-meaning-in-a-computer" class="headerlink" title="how do we have usable meaning in a computer"></a>how do we have usable meaning in a computer</h3><p>common NLP solutionï¼š<strong>WordNet</strong>ï¼ŒåŒ…å«åŒä¹‰è¯é›†åˆå’Œä¸Šä½è¯(ä¾‹å¦‚â€is aâ€å…³ç³»)çš„åˆ—è¡¨çš„è¯å…¸ã€‚</p>
<ul>
<li>åŒä¹‰è¯å’Œä¸Šä½è¯<br><img src="https://s2.loli.net/2022/01/26/kW9Uax1jSOG3Bnz.png" alt=""></li>
</ul>
<h4 id="problems-with-resources-like-WordNet"><a href="#problems-with-resources-like-WordNet" class="headerlink" title="problems with resources like WordNet"></a>problems with resources like WordNet</h4><ul>
<li>ä½œä¸ºèµ„æºä½¿ç”¨å¾ˆå¥½ï¼Œä½†å¿½ç•¥äº†ç»†å¾®å·®åˆ«ï¼ˆmissing nuanceï¼‰<ul>
<li>e.g. proficient å’Œ goodæ˜¯åŒä¹‰è¯ï¼Œä½†éœ€è¦ç»“åˆä¸Šä¸‹æ–‡</li>
</ul>
</li>
<li>éš¾ä»¥æŒç»­æ›´æ–°ï¼Œç¼ºä¹å•è¯çš„æ–°å«ä¹‰</li>
<li>æ¯”è¾ƒä¸»è§‚subjective</li>
<li>éœ€è¦äººåŠ›ç‰©åŠ›åˆ›å»ºå’Œè°ƒæ•´è¯åº“</li>
<li>æ²¡æœ‰<strong>å•è¯ç›¸ä¼¼åº¦</strong>çš„æ¦‚å¿µ</li>
</ul>
<h4 id="representing-words-as-discrete-symbols"><a href="#representing-words-as-discrete-symbols" class="headerlink" title="representing words as discrete symbols"></a>representing words as discrete symbols</h4><p>åœ¨ä¼ ç»ŸNLPä¸­ï¼Œwordè¢«è§†ä¸ºç¦»æ•£ç¬¦å·ï¼Œè¢«è¡¨ç¤ºä¸ºone-hot vecã€‚</p>
<ul>
<li>problemsï¼š<ul>
<li>æ‰€æœ‰çš„vecéƒ½æ­£äº¤</li>
<li>one-hot vecæ²¡æœ‰ç›¸ä¼¼åº¦çš„æ¦‚å¿µ</li>
<li>å‘é‡ç»´åº¦å¤ªå¤§</li>
</ul>
</li>
<li>solutionï¼š<ul>
<li>ä½¿ç”¨ç±»ä¼¼wordnetçš„listæ¥è¡¨ç¤ºç›¸ä¼¼åº¦ï¼Œä½†æœ‰ä¸å®Œæ•´æ€§</li>
<li><strong>å­¦ä¹ ä½¿ç”¨real-value vecæœ¬èº«æ¥ç¼–ç è®¡ç®—ç›¸ä¼¼åº¦</strong></li>
</ul>
</li>
</ul>
<h4 id="Representing-words-by-their-context"><a href="#Representing-words-by-their-context" class="headerlink" title="Representing words by their context"></a>Representing words by their context</h4><ul>
<li><strong>åˆ†å¸ƒå¼è¯­ä¹‰</strong>ï¼ˆdistributional semanticsï¼‰ï¼šwordçš„è¯­ä¹‰ç”±ç»å¸¸å‡ºç°åœ¨å…¶é™„è¿‘çš„å•è¯ç»™å‡ºã€‚</li>
<li>å½“ä¸€ä¸ªå•è¯wå‡ºç°åœ¨ä¸€æ®µæ–‡æœ¬ä¸­æ—¶ï¼Œåœ¨ä¸€ä¸ªå›ºå®šå¤§å°çš„çª—å£å†…co-occurçš„å•è¯å°±æ˜¯wçš„ä¸Šä¸‹æ–‡è¯ï¼Œä½¿ç”¨wçš„contextsæ¥æ„å»ºwçš„è¡¨ç¤º</li>
</ul>
<h3 id="word-vector-word-embedding-word-representation"><a href="#word-vector-word-embedding-word-representation" class="headerlink" title="word vector/word embedding/word representation"></a>word vector/word embedding/word representation</h3><p>ä¸ºæ¯ä¸ªå•è¯æ„å»ºdense vecï¼Œä½¿å…¶ä¸contextä¸­å‡ºç°çš„å•è¯çš„dense vecç›¸ä¼¼ã€‚</p>
<ul>
<li>word2vec</li>
<li>glove</li>
<li>sanjiv auroraâ€™s paper</li>
</ul>
<h4 id="word2vecï¼ˆMikolov-et-al-2013ï¼‰"><a href="#word2vecï¼ˆMikolov-et-al-2013ï¼‰" class="headerlink" title="word2vecï¼ˆMikolov et al. 2013ï¼‰"></a>word2vecï¼ˆMikolov et al. 2013ï¼‰</h4><p>ä¸€ä¸ªå­¦ä¹ word vecçš„æ¡†æ¶ï¼š</p>
<ul>
<li>æœ‰ä¸€ä¸ªå¤§çš„è¯­æ–™åº“corpus</li>
<li>è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯ç”¨ä¸€ä¸ªvecè¡¨ç¤º</li>
<li>å¯¹textä¸­çš„æ¯ä¸ªä½ç½®tï¼Œæœ‰ä¸€ä¸ªä¸­å¿ƒè¯$c$å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼ˆcontext/outsideï¼‰è¯$o$</li>
<li>ç”¨cå’Œoè¯å‘é‡çš„ç›¸ä¼¼æ€§æ¥è®¡ç®—ç»™å®š$c$æ—¶$o$çš„æ¦‚ç‡$P(o|c)$</li>
<li>ä¸æ–­è°ƒæ•´è¯å‘é‡ï¼Œæœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡$P(o|c)$</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/tgIXk1LHnpDT2we.png" alt=""><br><img src="https://s2.loli.net/2022/01/27/HKYXtyRQnosamg3.png" alt=""></p>
<h4 id="word2vec-objective-func"><a href="#word2vec-objective-func" class="headerlink" title="word2vec objective func"></a>word2vec objective func</h4><p>å¯¹æ¯ä¸ªä½ç½®$t=1,\cdots,T$ï¼Œåœ¨ä¸€ä¸ªå¤§å°ä¸º$m$çš„å›ºå®šçª—å£å†…é¢„æµ‹context wordsï¼Œç»™å®šä¸­å¿ƒè¯$w_j$ï¼Œæœ‰<br>$Likelihood=L(\theta)=\Pi^T_{t=1}\Pi_{-m\le j\le m,j\neq 0}P(w_{t+j}|w_t;\theta)$</p>
<p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<br>$J(\theta)=-\frac{1}{T}\Sigma_{t=1}^T\Sigma_{-m\le j\le m,j\neq 0}\ log\ P(w_{t+j}|w_t;\theta)$</p>
<p>ç»å…¸çš„$\Pi$é€šè¿‡logå˜æˆ$\Sigma$ï¼Œæœ€å°åŒ–ç›®æ ‡å‡½æ•°-&gt;æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°</p>
<ul>
<li>å¦‚ä½•è®¡ç®— $P(w_{t+j}|w_t;\theta)$<ul>
<li>$v_w$ï¼šwä¸ºä¸­å¿ƒè¯çš„vec </li>
<li>$u_W$ï¼šwæ˜¯ä¸Šä¸‹æ–‡è¯çš„vec</li>
<li>åˆ™$P(o|c)=\frac{exp(u_o^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}$ï¼Œå‘é‡ç‚¹ç§¯è¶Šå¤§å³ç›¸ä¼¼åº¦è¶Šé«˜</li>
</ul>
</li>
</ul>
<p>æ­¤å¤„ä½¿ç”¨<strong>Softmax</strong>:$\mathbb{R}^n-&gt;(0,1)^n$å°†ç‚¹ç§¯å¾—åˆ°çš„ä»»æ„å€¼$x_i$æ˜ å°„åˆ°æ¦‚ç‡åˆ†å¸ƒ$p_i$</p>
<h4 id="æ¢¯åº¦ä¸‹é™"><a href="#æ¢¯åº¦ä¸‹é™" class="headerlink" title="æ¢¯åº¦ä¸‹é™"></a>æ¢¯åº¦ä¸‹é™</h4><p>åœ¨è´Ÿæ¢¯åº¦ä¸Štake small step</p>
<ul>
<li><p>Gradient</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial }{\partial v_c}log\ P(o|c) &= \frac{\partial }{\partial v_c}(log\ exp(u^T_ov_c)-\Sigma_{w\in V}exp(u_w^Tv_c)) \\
&= \frac{\partial }{\partial v_c}(u_o^Tv_c-log\ \Sigma_{w\in V}exp(u_w^Tv_c)) \\
&= u_o - \frac{\Sigma_{w\in V}exp(u_w^Tv_c)u_w}{\Sigma_{w\in V}exp(u_w^Tv_c)} \\
&= u_o - \Sigma_{w\in V}\frac{exp(u_w^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}u_w \\ 
&= u_o - \Sigma_{w\in V}P(w|c)u_w \\
&= observed - expected
\end{aligned}</script></li>
<li><p>Update equation<br>$\theta^{new} = \theta^{old} - \alpha \bigtriangledown_\theta J(\theta)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    theta_grad = evaluate_gradient(J, corpus, theta)</span><br><span class="line">    theta = theta - alpha * theta_grade</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p><strong><em>Lecture 2. Word Vectors 2 and Word Window Classification</em></strong></p>
<h4 id="stochastic-gradient-descentï¼ˆSGDï¼‰"><a href="#stochastic-gradient-descentï¼ˆSGDï¼‰" class="headerlink" title="stochastic gradient descentï¼ˆSGDï¼‰"></a>stochastic gradient descentï¼ˆSGDï¼‰</h4><ul>
<li>é—®é¢˜ï¼š$J(\theta)$æ˜¯è¯­æ–™åº“ä¸­æ‰€æœ‰windowçš„å‡½æ•°ï¼Œæ¢¯åº¦è®¡ç®—å¼€é”€å¾ˆå¤§</li>
<li>æ–¹æ¡ˆï¼šSGDï¼Œéšæœºé‡‡æ ·windowsï¼Œåœ¨æ¯ä¸ªwindowæˆ–batchåupdate</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    window = sample_window(corpus)</span><br><span class="line">    theta_grad = evaluate_gradient(J, window, theta)</span><br><span class="line">    theta = theta - alpha * theta_grade</span><br></pre></td></tr></table></figure>
<h4 id="more-details"><a href="#more-details" class="headerlink" title="more details"></a>more details</h4><p>ä¸¤ä¸ªå‘é‡ï¼šæ›´å¥½ä¼˜åŒ–ï¼Œcenter vecå’Œcontext vecæœ€ç»ˆä¼šæ˜¯ç›¸ä¼¼çš„</p>
<p>ä¸¤ç±»æ¨¡å‹ï¼š</p>
<ul>
<li>Skip-Grams: ç»™å®šä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡</li>
<li>Continuous Bag of Words: ç»™å®šï¼ˆbag ofï¼‰ä¸Šä¸‹æ–‡è¯ï¼Œé¢„æµ‹ä¸­å¿ƒè¯</li>
</ul>
<p>å…¶ä»–è®­ç»ƒç­–ç•¥ï¼š</p>
<ul>
<li>negative sampling</li>
<li>negative sampling for naive softmax</li>
</ul>
<p>word2vec æ¯”è¾ƒcrudeï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†è¯çš„å…·ä½“ä½ç½®</p>
<h4 id="skip-gram-model-with-negative-samplingï¼ˆHW2ï¼‰"><a href="#skip-gram-model-with-negative-samplingï¼ˆHW2ï¼‰" class="headerlink" title="skip-gram model with negative samplingï¼ˆHW2ï¼‰"></a>skip-gram model with negative samplingï¼ˆHW2ï¼‰</h4><p>$P(o|c)=\frac{exp(u_o^Tv_c)}{\Sigma_{w\in V}exp(u_w^Tv_c)}$ä¸­åˆ†æ¯çš„è®¡ç®—å¼€é”€è¿‡å¤§</p>
<ul>
<li><p>Main ideaï¼šç”¨true pairï¼ˆä¸­å¿ƒè¯ä¸å…¶ä¸Šä¸‹æ–‡çª—å£ä¸­çš„è¯ï¼‰ä¸å‡ ä¸ªnoise pairï¼ˆä¸­å¿ƒè¯ä¸éšæœºè¯ï¼‰è®­ç»ƒ<strong>äºŒå…ƒé€»è¾‘å›å½’</strong></p>
</li>
<li><p>maximizeç›®æ ‡å‡½æ•°ï¼š$J(\theta)=\frac{1}{T}\Sigma^T_{t=1}J_t(\theta)$<br>$J_t(\theta)=log\ \sigma(u_o^Tv_c)+\Sigma^k_{i=1}\mathbb{E}_{j\sim P(w)}[log\ \sigma(-u^T_jv_c)]$<br>$\sigma(x)=\frac{1}{1+e^{-x}}$</p>
</li>
</ul>
<p>maximizeå‰åŠéƒ¨åˆ†çœŸå®ä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ï¼ŒminimizeååŠéƒ¨åˆ†è´Ÿé‡‡æ ·éšæœºè¯çš„æ¦‚ç‡</p>
<p>è´Ÿé‡‡æ ·æ–¹æ¡ˆï¼š$P(w)=U(w)^{3/4}/Z$ï¼Œ$U(w)$æ˜¯unigramåˆ†å¸ƒï¼Œç”¨å¹‚æå‡ç½•è§è¯çš„é‡‡æ ·æ¦‚ç‡ï¼Œ$Z$ä¸ºå½’ä¸€åŒ–ã€‚</p>
<h3 id="why-not-capture-co-ocurrence-counts-directly"><a href="#why-not-capture-co-ocurrence-counts-directly" class="headerlink" title="why not capture co-ocurrence counts directly?"></a>why not capture co-ocurrence counts directly?</h3><p>æ„å»ºco-occurence matrix $X$çš„ä¸¤ç§æ–¹æ³•ï¼š</p>
<ul>
<li>çª—å£windowsï¼šä¸w2vç±»ä¼¼ï¼Œä½¿ç”¨ä¸­å¿ƒè¯é™„è¿‘çš„windowï¼Œæ•è·å¥æ³•å’Œè¯­ä¹‰ä¿¡æ¯</li>
<li>å…¨æ–‡full documentï¼šçª—å£å¤§å°è®¾ä¸ºæ–‡ç« é•¿åº¦ï¼Œå¸¸ç”¨äºä¿¡æ¯æ£€ç´¢æ–¹æ³•ä¸­ï¼Œå¦‚Latent Semantic Analysis</li>
</ul>
<p>é—®é¢˜ï¼š</p>
<ul>
<li>è¯è¡¨å˜å¤§åˆ™å‘é‡ç»´åº¦ä¹Ÿè¦å¢å¤§ï¼Œä½†å‘é‡å¾ˆç¨€ç–</li>
<li>åˆ†ç±»æ¨¡å‹ä¹Ÿä¼šé­é‡ç¨€ç–çš„é—®é¢˜ï¼Œé²æ£’æ€§å·®</li>
</ul>
<p>éœ€è¦ä½¿ç”¨low-dimçš„dense matrixã€‚</p>
<h4 id="Classic-Method-Dimensionality-Reduction-on-Xï¼ˆHW1ï¼‰"><a href="#Classic-Method-Dimensionality-Reduction-on-Xï¼ˆHW1ï¼‰" class="headerlink" title="Classic Method Dimensionality Reduction on Xï¼ˆHW1ï¼‰"></a>Classic Method Dimensionality Reduction on Xï¼ˆHW1ï¼‰</h4><p>ç”¨SVDåˆ†è§£$X$ä¸º$U\Sigma V^T$ï¼Œ$U$å’Œ$V$å¯¹åº”è¡Œå’Œåˆ—çš„æ­£äº¤åŸºã€‚</p>
<p>ä¸ºäº†å‡å°‘å°ºåº¦ï¼Œå¯ä»¥ä¿ç•™$\Sigma$å¯¹è§’é˜µä¸­çš„topKæœ€å¤§å€¼ï¼Œå¹¶ä¿ç•™$U,V$å¯¹åº”çš„è¡Œåˆ—ã€‚</p>
<ul>
<li>ç›´æ¥å¯¹åŸå§‹çš„countåšSVDæ•ˆæœå¾ˆå·®ï¼Œéœ€è¦å¯¹æ•°å­—è¿›è¡Œè°ƒæ•´ï¼š<ul>
<li>å–log</li>
<li>å–$min(X,\delta)$</li>
<li>å¿½ç•¥è™šè¯çš„count</li>
</ul>
</li>
<li>ä½¿ç”¨æ–œå¡ramped windowï¼Œå¯¹æ›´è¿‘çš„è¯è®¡æ›´å¤§çš„count</li>
<li>ç”¨Pearsonç›¸å…³ç³»æ•°ï¼ˆPearson Correlation Coefficientï¼‰ä»£æ›¿countï¼Œç„¶åå°†è´Ÿå€¼ç½®é›¶</li>
</ul>
<h4 id="Towards-GloVeï¼šCount-based-vs-direct-prediction"><a href="#Towards-GloVeï¼šCount-based-vs-direct-prediction" class="headerlink" title="Towards GloVeï¼šCount based vs direct prediction"></a>Towards GloVeï¼šCount based vs direct prediction</h4><ul>
<li>count basedï¼šåŸºäºco-occurence<ul>
<li>LSAã€HALã€COALSã€Hellingger-PCA</li>
<li>è®­ç»ƒå¿«ï¼›æœ‰æ•ˆä½¿ç”¨ç»Ÿè®¡æ•°æ®</li>
<li>ä¸»è¦ç”¨äºæ•è·è¯ç›¸ä¼¼æ€§ï¼›è¿‡åˆ†é‡è§†countå¤§çš„æ•°æ®</li>
</ul>
</li>
<li>direct predictionï¼šå®šä¹‰æ¦‚ç‡åˆ†å¸ƒï¼Œæ ¹æ®æ¦‚ç‡é¢„æµ‹è¯<ul>
<li>Skip-gram/CBOWã€NNLMã€HLBLã€RNN</li>
<li>è§„æ¨¡è·Ÿcorpuså¤§å°æœ‰å…³ï¼›æ²¡æœ‰æœ‰æ•ˆåˆ©ç”¨ç»Ÿè®¡æ•°æ®ï¼ˆé‡‡æ ·å’Œçª—å£ï¼‰</li>
<li>æé«˜äº†åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›å¯ä»¥æ•è·ç›¸ä¼¼åº¦ä¹‹å¤–æ›´å¤æ‚çš„ä¿¡æ¯</li>
</ul>
</li>
</ul>
<h4 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h4><p>ç»“åˆä¸¤ç§æ–¹æ³•ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­å¼•å…¥countçŸ©é˜µ</p>
<p>æ ¸å¿ƒæ€æƒ³ï¼šco-orrurrenceæ¦‚ç‡ä¹‹æ¯”å¯ä»¥ç¼–ç meaning components </p>
<p><img src="https://s2.loli.net/2022/01/27/Sfgh7JzbwAOtvBx.png" alt=""></p>
<p>å•ä¸€çš„æ¦‚ç‡å¹¶éé‡ç‚¹ï¼Œæ¦‚ç‡çš„æ¯”å€¼æ›´èƒ½åæ˜ (linear) meanning componentã€‚</p>
<p>å…³é”®ï¼š$\frac{P(x|a)}{P(x|b)}$å¦‚ä½•åœ¨è¯å‘é‡ç©ºé—´ä¸­æ•è·ï¼Ÿ</p>
<ul>
<li>ç”¨å‘é‡çš„ç‚¹ç§¯è¡¨ç¤ºco-occurence æ¦‚ç‡çš„å¯¹æ•°</li>
<li>$w_i\cdot w_j = log P(i|j)$</li>
<li>$w_x\cdot (w_a - w_b)\frac{P(x|a)}{P(x|b)}$</li>
<li>$J=\Sigma^V_{i,j=1}f(X_{ij})(w_i^T\tilde w_j+b_i+\tilde b_j-log X_{ij})^2$<br>$f$é™åˆ¶äº†å¸¸è§è¯çš„count </li>
</ul>
<h3 id="how-to-evaluate-word-vectors"><a href="#how-to-evaluate-word-vectors" class="headerlink" title="how to evaluate word vectors"></a>how to evaluate word vectors</h3><ul>
<li>Intrinsic<ul>
<li>åœ¨ç‰¹å®šå’Œä¸­é—´å­ä»»åŠ¡ä¸Šè¯„ä¼°</li>
<li>è¯„ä¼°è¿ç®—å¾ˆå¿«</li>
<li>æœ‰åŠ©äºç†è§£æ•´ä¸ªç³»ç»Ÿ</li>
<li>åœ¨ç”¨äºå®é™…ä»»åŠ¡å‰ä¸æ¸…æ¥šæ˜¯å¦æœ‰ç”¨</li>
</ul>
</li>
<li>Extrinsic<ul>
<li>åœ¨å®é™…ä»»åŠ¡ä¸Šè¯„ä¼°</li>
<li>é€šå¸¸éœ€è¦æ›´é•¿æ—¶é—´ï¼ˆä»»åŠ¡æ›´å¤§ï¼‰</li>
<li>ä¸æ¸…æ¥šç³»ç»Ÿæ•´ä¸ªç³»ç»Ÿçš„é—®é¢˜æ‰€åœ¨</li>
</ul>
</li>
</ul>
<h3 id="word-sense-and-word-sense-ambiguity"><a href="#word-sense-and-word-sense-ambiguity" class="headerlink" title="word sense and word sense ambiguity"></a>word sense and word sense ambiguity</h3><ul>
<li>å¤šä¹‰çš„word å’Œ ä¸€ä¸€å¯¹åº”çš„word vecç›¸æ‚–</li>
</ul>
<h4 id="imporving-word-representations-via-global-context-and-multiple-word-prototypesï¼ˆHuang-et-al-2012ï¼‰"><a href="#imporving-word-representations-via-global-context-and-multiple-word-prototypesï¼ˆHuang-et-al-2012ï¼‰" class="headerlink" title="imporving word representations via global context and multiple word prototypesï¼ˆHuang et al. 2012ï¼‰"></a>imporving word representations via global context and multiple word prototypesï¼ˆHuang et al. 2012ï¼‰</h4><p>å¯¹å¸¸ç”¨è¯çš„æ‰€æœ‰contextèšç±»ï¼Œä»è€Œå¾—åˆ°ä¸€äº›è¡¨ç¤ºä¸åŒå«ä¹‰çš„ç°‡ã€‚</p>
<p>åˆ’åˆ†å¹¶ä¸æ˜ç¡®ä¸”å­˜åœ¨é‡å ã€‚</p>
<h4 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy</h4><ul>
<li>å•è¯çš„ä¸åŒå«ä¹‰ä»¥åŠ æƒçš„çº¿æ€§å½¢å¼å åŠ ï¼š<br>$v_w=\Sigma\alpha_iv_{w_i}$<br>$\alpha_i=\frac{f_i}{\Sigma f_j}$</li>
<li>å®é™…ä¸Šï¼Œç”¨ç¨€ç–ç¼–ç Sparse codingçš„æ€æƒ³ï¼Œä¸åŒç»´åº¦è¡¨ç¤ºäº†ä¸åŒçš„å«ä¹‰ï¼Œæ‰€ä»¥weighted-avgå¯¹ä¸åŒå«ä¹‰çš„ç»´åº¦æŸå¤±å¾ˆå°ã€‚</li>
</ul>
<p><strong><em>Lecture 3. Backprop and Neural Networks</em></strong></p>
<h2 id="matrix-calculus"><a href="#matrix-calculus" class="headerlink" title="matrix calculus"></a>matrix calculus</h2><h3 id="gradient"><a href="#gradient" class="headerlink" title="gradient"></a>gradient</h3><ul>
<li>å•è¾“å‡ºå¤šè¾“å…¥çš„å‡½æ•°$f(\mathbf{x})=f(x_1,\dots,x_n)$</li>
<li>æ¢¯åº¦gradientæ˜¯å…³äºæ¯ä¸ªè¾“å…¥çš„åå¯¼æ•°partial derivativesçš„å‘é‡$\frac{\partial f}{\partial x}=[\frac{\partial f}{\partial x_1},\dots,\frac{\partial f}{\partial x_n}]$</li>
</ul>
<h3 id="Jacobian-Matirx-Generalization-of-the-Gradient"><a href="#Jacobian-Matirx-Generalization-of-the-Gradient" class="headerlink" title="Jacobian Matirx: Generalization of the Gradient"></a>Jacobian Matirx: Generalization of the Gradient</h3><ul>
<li>å¤šè¾“å‡ºå¤šè¾“å…¥çš„å‡½æ•°$\mathbf{f(x)}=[f_1(x_1,\dots,x_n),\dots,f_m(x_1,\dots,x_n)]$</li>
<li>Jacobian matæ˜¯$m\times n$çš„çŸ©é˜µï¼š<br>$\mathbf{\frac{\partial f}{\partial x}}=<br>\begin{bmatrix}<br>\frac{\partial f_1}{\partial x_1} &amp; \dots &amp; \frac{\partial f_1}{\partial x_n} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>\frac{\partial f_m}{\partial x_1} &amp; \dots &amp; \frac{\partial f_m}{\partial x_n} \\<br>\end{bmatrix}$</li>
</ul>
<h3 id="chain-rule"><a href="#chain-rule" class="headerlink" title="chain rule"></a>chain rule</h3><p>å¤šå˜é‡çš„é“¾å¼æ³•åˆ™ï¼šä¹˜Jacobian mat</p>
<h3 id="exampleï¼šNER"><a href="#exampleï¼šNER" class="headerlink" title="exampleï¼šNER"></a>exampleï¼šNER</h3><ul>
<li>æ‰¾å‡ºæ–‡æœ¬ä¸­çš„å®ä½“å¹¶åˆ†ç±»<br><img src="https://s2.loli.net/2022/01/27/gZzbEHY2RVdBPGO.png" alt=""></li>
<li>Ideaï¼šé€šè¿‡ä¸Šä¸‹æ–‡çª—å£æ¥å¯¹æ¯ä¸ªè¯åˆ†ç±»ï¼Œå¯¹æ¯ä¸ªç±»è®­ç»ƒlogisticåˆ†ç±»å™¨<br><img src="https://s2.loli.net/2022/01/27/g5IB4W3qwnMasOf.png" alt=""></li>
</ul>
<h4 id="è®¡ç®—æ¢¯åº¦"><a href="#è®¡ç®—æ¢¯åº¦" class="headerlink" title="è®¡ç®—æ¢¯åº¦"></a>è®¡ç®—æ¢¯åº¦</h4><ul>
<li>è®¡ç®—$\frac{\partial s}{\partial \mathbf{b}}$<br><img src="https://s2.loli.net/2022/01/27/vrBADQU2n8Vz7dT.png" alt=""></li>
<li>è®¡ç®—$\frac{\partial s}{\partial \mathbf{W}}$<br>ä¸ä¸Šè¿°è¿‡ç¨‹ç›¸ä¼¼ï¼Œå…¶ä¸­å‰ä¸¤é¡¹æ˜¯ä¸€è‡´çš„ï¼Œæ— éœ€é‡å¤è®¡ç®—<br>$\frac{\partial s}{\partial \mathbf{W}}=\delta\frac{\partial z}{\partial \mathbf{W}}$<br>$\delta=\frac{\partial s}{\partial \mathbf{h}}\frac{\partial \mathbf{h}}{\partial z}=\mathbf{u}^T\circ fâ€™(z)$æ˜¯local error signal</li>
</ul>
<h4 id="derivative-with-respect-to-Matrix-output-shape"><a href="#derivative-with-respect-to-Matrix-output-shape" class="headerlink" title="derivative with respect to Matrix: output shape"></a>derivative with respect to Matrix: output shape</h4><ul>
<li>è¾“å‡ºå¤§å°ä¸º1ï¼Œè¾“å…¥å¤§å°ä¸º$n\times m$.</li>
<li>ä»æ•°å­¦è§’åº¦ï¼Œè¾“å‡ºåº”è¯¥ä¸º$1\times nm$</li>
<li>ä½†ä¸ºäº†æ–¹ä¾¿æ›´æ–°å‚æ•°ï¼Œå…¶shapeåº”ä¸º$n\times m$</li>
</ul>
<p>$\frac{\partial s}{\partial \mathbf{W}}=\delta^Tx^T$ï¼Œtransposeå¯ä»¥è§£å†³shapeçš„é—®é¢˜</p>
<p><strong><em>Lecture 4. Dependency Parsing</em></strong></p>
<hr>
<h2 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h2><h3 id="constituency-Parsing"><a href="#constituency-Parsing" class="headerlink" title="constituency Parsing"></a>constituency Parsing</h3><p>constituency = phrase structure grammer = <strong>context-free</strong> grammersï¼ˆCFGsï¼‰</p>
<ul>
<li>starting unitï¼šword</li>
<li>words ç»„åˆæˆçŸ­è¯­</li>
<li><p>çŸ­è¯­å¯ä»¥é€’å½’åœ°ç»„åˆæˆæ›´é•¿çš„çŸ­è¯­</p>
</li>
<li><p>Det Determineré™å®šè¯ </p>
</li>
<li>NP Noun Phraseåè¯çŸ­è¯­ </li>
<li>VP Verb PhraseåŠ¨è¯çŸ­è¯­ </li>
<li>P Preposition ä»‹è¯<ul>
<li>PP Prepositional Phraseä»‹è¯çŸ­è¯­</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/XybS6hjsQ3KWJc9.png" alt=""></p>
<h3 id="dependency-structure"><a href="#dependency-structure" class="headerlink" title="dependency structure"></a>dependency structure</h3><p>ç”¨å•è¯é—´çš„ä¾èµ–/ä¿®é¥°å…³ç³»è¡¨ç¤ºå¥å­çš„ç»“æ„</p>
<h4 id="why-do-we-need-sentense-structure"><a href="#why-do-we-need-sentense-structure" class="headerlink" title="why do we need sentense structure"></a>why do we need sentense structure</h4><ul>
<li>è‡ªç„¶è¯­è¨€ä¸­å°†å•è¯ç»„åˆæˆæ›´å¤§çš„çŸ­è¯­ã€å¥å­æ¥è¡¨è¾¾å¤æ‚çš„å«ä¹‰å’Œæ€æƒ³</li>
<li>äººç±»åœ¨ç†è§£è‡ªç„¶è¯­è¨€æ˜¯éœ€è¦å¼„æ¸…æ¥šå•è¯é—´çš„ç›¸å…³æ€§</li>
<li>è‡ªç„¶è¯­è¨€æ¨¡å‹éœ€è¦åŒæ ·çš„ã€ç†è§£å¥å­ç»“æ„çš„èƒ½åŠ›</li>
</ul>
<h4 id="some-ambiguity"><a href="#some-ambiguity" class="headerlink" title="some ambiguity"></a>some ambiguity</h4><h5 id="PP-attachment-ambiguity-ä»‹è¯çŸ­è¯­æ­§ä¹‰"><a href="#PP-attachment-ambiguity-ä»‹è¯çŸ­è¯­æ­§ä¹‰" class="headerlink" title="PP attachment ambiguity ä»‹è¯çŸ­è¯­æ­§ä¹‰"></a>PP attachment ambiguity ä»‹è¯çŸ­è¯­æ­§ä¹‰</h5><ul>
<li><p><em>San Jose cops kill man with knife</em></p>
<ul>
<li>San Jose cops kill <strong>man with knife</strong><ul>
<li>knifeæ˜¯mançš„modifier</li>
</ul>
</li>
<li>San Jose cops <strong>kill</strong> man <strong>with knife</strong><ul>
<li>knifeæ˜¯killçš„modifier</li>
</ul>
</li>
</ul>
</li>
<li><p>å…³é”®æ˜¯å¦‚ä½•åˆ†æä¾å­˜å…³ç³»</p>
<ul>
<li>å¯¹å¤æ‚çš„å¥å­ç»“æ„ï¼Œæœ‰æŒ‡æ•°çº§çš„ä¾å­˜å…³ç³»parsing tree </li>
<li>Catalan number $C_n=(2n)!/[(n+1)!n!]$<br>å‡ºç°åœ¨æ ‘å½¢ç»“æ„çš„ç¯å¢ƒä¸­ï¼Œå¦‚n+2å¤šè¾¹å½¢çš„ä¸‰è§’å‰–åˆ†å¯èƒ½æ•°</li>
</ul>
</li>
</ul>
<h5 id="coordination-scope-ambiguity-åè°ƒèŒƒå›´æ¨¡ç³Š"><a href="#coordination-scope-ambiguity-åè°ƒèŒƒå›´æ¨¡ç³Š" class="headerlink" title="coordination scope ambiguity åè°ƒèŒƒå›´æ¨¡ç³Š"></a>coordination scope ambiguity åè°ƒèŒƒå›´æ¨¡ç³Š</h5><ul>
<li><em>Shuttle veteran and longtime NASA executive Fred Gregory appointed to board</em><ul>
<li><strong>Shuttle veteran and longtime NASA executive</strong> Fred Gregory appointed to board<ul>
<li>shuttle veteran å’Œ longtime NASA executive éƒ½æ˜¯æŒ‡Fred</li>
</ul>
</li>
<li>Shuttle veteran and <strong>longtime NASA executive Fred Gregory</strong> appointed to board<ul>
<li>ä»…longtime NASA executive æ˜¯æŒ‡Fred</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="adj-adv-modifier-ambiguity"><a href="#adj-adv-modifier-ambiguity" class="headerlink" title="adj adv modifier ambiguity"></a>adj adv modifier ambiguity</h5><ul>
<li><em>Students get first hand job experience</em><ul>
<li>Students get <strong>first hand job</strong> experience<ul>
<li>first hand ä¿®é¥° job</li>
</ul>
</li>
<li>Students get <strong>first hand</strong> job <strong>experience</strong><ul>
<li>first hand ä¿®é¥° experience</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="VP-attachment-ambiguity"><a href="#VP-attachment-ambiguity" class="headerlink" title="VP attachment ambiguity"></a>VP attachment ambiguity</h5><ul>
<li><em>Multilated body washed up on Rio beach to be used for Olympics beach volleyball</em><ul>
<li>Multilated <strong>body</strong> washed up on Rio beach <strong>to be used for Olympics beach volleyball</strong><ul>
<li>to be used ä¿®é¥° body</li>
</ul>
</li>
<li>Multilated body washed up on Rio <strong>beach to be used for Olympics beach volleyball</strong><ul>
<li>to be used ä¿®é¥° beach</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="dependency-grammar-and-dependency-structure"><a href="#dependency-grammar-and-dependency-structure" class="headerlink" title="dependency grammar and dependency structure"></a>dependency grammar and dependency structure</h4><p>dependency syntaxä¸­å•è¯é—´çš„å…³ç³»é€šå¸¸æ˜¯äºŒå…ƒä¸å¯¹ç§°çš„ä¾èµ–å…³ç³»ï¼ˆç®­å¤´ ï¼‰</p>
<ul>
<li>ç®­å¤´è¿æ¥headå’Œdependency<ul>
<li>head-&gt;dependency</li>
</ul>
</li>
<li>ä¾èµ–å…³ç³»å½¢æˆå•å¤´æ— ç¯çš„å›¾ </li>
<li>é€šå¸¸é¢å¤–å¼•å…¥ä¸€ä¸ªROOTèŠ‚ç‚¹ï¼Œä½¿æ‰€æœ‰å•è¯æœ‰head</li>
</ul>
<h4 id="universal-dependency-treebanks"><a href="#universal-dependency-treebanks" class="headerlink" title="universal dependency treebanks"></a>universal dependency treebanks</h4><p>ç»Ÿä¸€ã€å¹¶è¡Œã€é€šç”¨çš„ä¾èµ–å…³ç³»æè¿°</p>
<ul>
<li>äººå·¥ç¼–å†™è¯­æ³•è§„åˆ™ï¼Œè®­ç»ƒå¾—åˆ°parser</li>
<li>è§„åˆ™æ„ˆå‘å¤æ‚ï¼Œä¸èƒ½é‡ç”¨ä¹‹å‰çš„å·¥ä½œ</li>
</ul>
<p><strong>treebank</strong></p>
<ul>
<li>èµ·åˆæ„å»ºtreebankæ¯”è¯­æ³•è§„åˆ™è¦æ…¢ä¸”ç”¨å¤„æœ‰é™</li>
<li>å¯é‡ç”¨<ul>
<li>åœ¨treebankåŸºç¡€ä¸Šå»ºç«‹parserã€taggerç­‰</li>
<li>è¯­è¨€å­¦çš„</li>
</ul>
</li>
<li>è¦†ç›–é¢å¹¿æ³›</li>
<li>åˆ©ç”¨äº†ç»Ÿè®¡ä¿¡æ¯</li>
<li>å¯ä»¥è¯„ä¼°NLPç³»ç»Ÿ</li>
</ul>
<h4 id="dependency-conditioning-preference"><a href="#dependency-conditioning-preference" class="headerlink" title="dependency conditioning preference"></a>dependency conditioning preference</h4><p>ä¾èµ–è§£æçš„ä¿¡æ¯ï¼š</p>
<ul>
<li>Bilexical affinitiesä¸¤ä¸ªå•è¯é—´çš„å¯†åˆ‡å…³ç³»</li>
<li>Dependency distanceï¼šå¤§å¤šæ•°ä¾èµ–éƒ½æ˜¯å’Œç›¸é‚»è¯çš„</li>
<li>intervening materialï¼šä¾èµ–å…³ç³»å¾ˆå°‘è·¨è¶Šä¸­é—´åŠ¨è¯oræ ‡ç‚¹ç¬¦å·</li>
<li>valency of headï¼šä¸åŒç±»å‹headçš„ä¾èµ–å…³ç³»ä½ç½®ç›¸å¯¹å›ºå®š</li>
</ul>
<h4 id="Dependency-Parsing-1"><a href="#Dependency-Parsing-1" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h4><ul>
<li>ä¸ºæ¯ä¸ªå•è¯é€‰æ‹©å…¶dependencyæ¥è§£æå¥å­</li>
<li>åªæœ‰ä¸€ä¸ªå•è¯æ˜¯ä¾èµ–äºrootçš„</li>
<li>ä¸èƒ½æˆç¯ï¼ˆåªèƒ½æˆtreeï¼‰</li>
<li>ç®­å¤´å¯ä»¥äº¤å‰ï¼Œä¸äº¤å‰çš„ç§°ä¸ºnon-projective</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/jDhBrbPnF7osI3z.png" alt=""></p>
<h5 id="projectivity"><a href="#projectivity" class="headerlink" title="projectivity"></a>projectivity</h5><ul>
<li>å•è¯æŒ‰çº¿æ€§é¡ºåºæ’åˆ—æ—¶ï¼Œæ²¡æœ‰äº¤å‰çš„dependencyã€‚</li>
<li>CFGæ ‘çš„ä¾èµ–å…³ç³»å¿…é¡»æ˜¯projectiveçš„</li>
<li>non-projectiveç»“æ„è§£é‡Šç§»ä½çš„word</li>
</ul>
<p>ä»‹è¯ææµ…<br><img src="https://s2.loli.net/2022/01/27/xSE4cLkqvpFNbHP.png" alt=""></p>
<h5 id="methods"><a href="#methods" class="headerlink" title="methods"></a>methods</h5><ul>
<li>Dynamic programming<ul>
<li>Eisneræå‡ºçš„å¤æ‚åº¦ä¸º$O(n^3)$çš„clever algoï¼Œé€šè¿‡åœ¨æœ«å°¾è€Œéä¸­é—´ç”Ÿæˆhead</li>
</ul>
</li>
<li>Graph algo<ul>
<li>å¯¹å¥å­å»ºç«‹æœ€å°ç”Ÿæˆæ ‘ï¼ˆMinimun Spanning Treeï¼‰</li>
</ul>
</li>
<li>constraint satisfaction<ul>
<li>å»æ‰ä¸æ»¡è¶³ç¡¬çº¦æŸçš„edge</li>
</ul>
</li>
<li>transition-based parsing or deterministic dependency parsing<ul>
<li>åŸºäºml classifierçš„è´ªå¿ƒé€‰æ‹©</li>
<li>æ¯”è¾ƒå¤æ‚ï¼Œå…·ä½“è¯¦è§<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/index.html#schedule" target="_blank" rel="noopener">æœ¬è¯¾ç¨‹Lecture4</a></li>
</ul>
</li>
</ul>
<p><em>Lecture 5. Recurrent Neural Networks and Language Models</em></p>
<hr>
<h5 id="how-do-we-gain-from-a-neural-dependency-parser"><a href="#how-do-we-gain-from-a-neural-dependency-parser" class="headerlink" title="how do we gain from a neural dependency parser"></a>how do we gain from a neural dependency parser</h5><ul>
<li><p>indicator feature çš„é—®é¢˜ï¼š</p>
<ul>
<li>è¿™æ ·çš„ç‰¹å¾å‘é‡å¾ˆç¨€ç–</li>
<li>ç‰¹å¾ä¸å®Œæ•´</li>
<li>ç‰¹å¾çš„è®¡ç®—å¼€é”€å¾ˆå¤§</li>
</ul>
</li>
<li><p>distributed representation</p>
<ul>
<li>å°†æ¯ä¸ªå•è¯è¡¨ç¤ºä¸ºdense vector</li>
<li>part-of-sppeech tagså’Œdependency labelsä¹Ÿè¢«è¡¨ç¤ºä¸ºåŒæ ·ç»´åº¦çš„dense vector</li>
</ul>
</li>
<li><p>deep learning and non-linear classifier</p>
<ul>
<li>ä¼ ç»Ÿ ML classiferåªæœ‰linear boundary</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/WdrXvBsxtLJQKTi.png" alt=""></p>
<h5 id="graph-based-dependency-parsers"><a href="#graph-based-dependency-parsers" class="headerlink" title="graph-based dependency parsers"></a>graph-based dependency parsers</h5><ul>
<li>å¯¹æ¯ä¸ªå¯èƒ½çš„dependencyè®¡ç®—score<ul>
<li>éœ€è¦ç†è§£å•è¯çš„contextual representation</li>
</ul>
</li>
</ul>
<h2 id="more-things-about-NN"><a href="#more-things-about-NN" class="headerlink" title="more things about NN"></a>more things about NN</h2><h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><ul>
<li>å®Œæ•´çš„loss funcåŒ…å«å¯¹æ‰€æœ‰å‚æ•°$\theta$çš„æ­£åˆ™åŒ–<ul>
<li>L2 regularization $\lambda\Sigma_k \theta_k^2$</li>
</ul>
</li>
<li>æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢å¤§é‡ç‰¹å¾/æ·±å±‚æ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆ</li>
<li>ä¸ºæ¨¡å‹æä¾›äº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ˆè¿‡æ‹Ÿåˆå®é™…ä¸Šä¸æ˜¯é—®é¢˜ï¼Œéœ€è¦æ³›åŒ–èƒ½åŠ›ï¼‰</li>
</ul>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><ul>
<li>åœ¨è®­ç»ƒæ—¶éšæœºå°†ç¥ç»å…ƒè¾“å‡ºç‰¹å¾çš„ä¸€éƒ¨åˆ†ç½®é›¶ï¼ˆä»è€Œä¸æ›´æ–°è¯¥éƒ¨åˆ†æƒé‡ï¼‰</li>
<li>é¿å…ç‰¹å¾co-adaptationï¼šä¸ä¼šå‡ºç°æŸç‰¹å¾ä»…åœ¨å…¶ä»–ç‰¹å¾å‡ºç°æ—¶æ‰æœ‰ç”¨çš„æƒ…å†µ</li>
<li>åœ¨å•å±‚ä¸­ï¼Œå¯ä»¥çœ‹åšnaive bayesï¼ˆæ‰€æœ‰æƒé‡è¢«ç‹¬ç«‹è®¾ç½®ï¼‰å’Œlogistic regressionï¼ˆæƒé‡åœ¨æ‰€æœ‰ä¸Šä¸‹æ–‡ä¸­è®¾ç½®ï¼‰çš„æŠ˜ä¸­</li>
<li>å¯ä»¥çœ‹åšä¸€ç§model baggingï¼ˆensemble modelï¼‰ï¼Œæˆ–æ˜¯ä¸€ç§ç‰¹å¾ç‹¬ç«‹çš„å¼ºæ­£åˆ™åŒ–</li>
</ul>
<h3 id="non-linearities"><a href="#non-linearities" class="headerlink" title="non-linearities"></a>non-linearities</h3><p><img src="https://s2.loli.net/2022/01/27/GBCEb8AKiyukLvJ.png" alt=""></p>
<ul>
<li>oldï¼šæŒ‡æ•°çº§è®¡ç®—<ul>
<li>logisticï¼ˆsigmoidï¼‰ï¼š0ï½1</li>
<li>tanhï¼š1ï½-1ï¼Œå®é™…ä¸Šæ˜¯é‡æ–°ç¼©æ”¾å’Œç§»åŠ¨çš„sigmoid<ul>
<li>$tanh(z)=2sigmoid(2z)-1$</li>
</ul>
</li>
</ul>
</li>
<li>new<ul>
<li>hard tanh</li>
<li>ReLU<ul>
<li>ç¥ç»å…ƒè¦ä¹ˆdeadè¦ä¹ˆå°±åœ¨ä¼ é€’ä¿¡æ¯</li>
<li>æœ‰å¾ˆå¥½çš„æ¢¯åº¦backflowï¼Œè®­ç»ƒå¾ˆå¿«</li>
</ul>
</li>
</ul>
</li>
<li>varient<br><img src="https://s2.loli.net/2022/01/27/Jt426ZfwesoF1yv.png" alt=""></li>
</ul>
<h3 id="prameter-initialization"><a href="#prameter-initialization" class="headerlink" title="prameter initialization"></a>prameter initialization</h3><ul>
<li>å°†æƒé‡åˆå§‹åŒ–ä¸ºå°éšæœºå€¼ï¼Œé¿å…å¯¹ç§°æ€§å½±å“å­¦ä¹ </li>
<li>Uniform(-r,r)</li>
<li>Xavieråˆå§‹åŒ–ä¸­ï¼Œæ–¹å·®å’Œå‰åå±‚çš„å°ºå¯¸æœ‰å…³<ul>
<li>$Var(W_i)=\frac{2}{n_{in}+n_{out}}$</li>
</ul>
</li>
</ul>
<h3 id="optimizers"><a href="#optimizers" class="headerlink" title="optimizers"></a>optimizers</h3><ul>
<li>ç®€å•çš„SGDå¾ˆæœ‰æ•ˆ</li>
<li>å…¶ä»–ä¼˜åŒ–å™¨<ul>
<li>Adagrad</li>
<li>RMSprop</li>
<li>Adamï¼ˆå®‰å…¨æœ‰æ•ˆï¼‰</li>
<li>SparseAdam</li>
</ul>
</li>
</ul>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><ul>
<li>lrå¤ªå¤§æ¨¡å‹ä¼šä¸æ”¶æ•›ï¼Œå¤ªå°æ¨¡å‹æ•ˆæœä¼šå˜å·®</li>
<li>åœ¨è®­ç»ƒæ—¶é€šå¸¸éœ€è¦åŠ¨æ€è°ƒæ•´lr<ul>
<li>by handï¼škä¸ªepochåå‡åŠ</li>
<li>by formulaï¼š$lr=lr_0e_{-kt}$, for epoch t</li>
<li>å¾ªç¯lr</li>
</ul>
</li>
</ul>
<h2 id="language-model-and-RNN"><a href="#language-model-and-RNN" class="headerlink" title="language model and RNN"></a>language model and RNN</h2><h3 id="language-modeling"><a href="#language-modeling" class="headerlink" title="language modeling"></a>language modeling</h3><ul>
<li>è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ˜¯é¢„æµ‹ä¸‹ä¸€ä½ç½®çš„å•è¯<ul>
<li>ç»™å®šå•è¯åºåˆ—$x^{(1)},\dots,x^{(t)}$ï¼Œè®¡ç®—ä¸‹ä¸€å•è¯$x^{(t+1)}$çš„æ¦‚ç‡åˆ†å¸ƒ$P(x^{(t+1)}|x^{(t)},\dots,x^{(1 )})$</li>
</ul>
</li>
<li>è¯­è¨€æ¨¡å‹ä¹Ÿå¯ä»¥çœ‹åšå°†æ¦‚ç‡åˆ†é…ç»™ä¸€æ®µtext<ul>
<li>$P(x^{(1)},\dots,x^{(t)})=\Pi^T_{t=1}P(x^{(t)}|x^{(t-1 )},\dots,x^{(1 )})$</li>
</ul>
</li>
</ul>
<h3 id="n-gram-language-model"><a href="#n-gram-language-model" class="headerlink" title="n-gram language model"></a>n-gram language model</h3><ul>
<li>n-gramï¼šç”±nä¸ªå•è¯ç»„æˆçš„å—</li>
<li>ç»Ÿè®¡ä¸åŒn-gramå‡ºç°çš„é¢‘ç‡æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯</li>
<li><p>Markovå‡è®¾ï¼š</p>
<ul>
<li>$x^{(t+1)}$åªå–å†³äºä¹‹å‰çš„n-1ä¸ªå•è¯<br>$\begin{aligned} P(x^{(t+1)}|x^{(t)},\dots,x^{(1 )})&amp;=P(x^{(t+1)}|x^{(t)},\dots,x^{(t-n+2)})\\<br>&amp;= \frac{P(x^{(t+1)},\dots,x^{(t-n+2)})}{P(x^{(t)},\dots,x^{(t-n+2)})} \\<br>&amp;= \frac{\text{prob of n-gram}}{\text{prob of (n-1)-gram}}\end{aligned}$</li>
</ul>
</li>
<li><p>ä½¿ç”¨countçš„ç»Ÿè®¡è¿‘ä¼¼è®¡ç®—ä»¥ä¸Šæ¦‚ç‡</p>
</li>
</ul>
<h4 id="sparsity-problem-with-n-gram"><a href="#sparsity-problem-with-n-gram" class="headerlink" title="sparsity problem with n-gram"></a>sparsity problem with n-gram</h4><ul>
<li>é—®é¢˜ï¼š(n-1)-gram + w çš„å½¢å¼ä¸å­˜åœ¨ï¼Œæ¦‚ç‡ä¸º0<ul>
<li>è§£å†³æ–¹æ¡ˆï¼šä¸ºæ¯ä¸ªwè®¾ç½®ä¸€ä¸ª$\delta$çš„å°æ¦‚ç‡ï¼ˆsmoothingï¼‰</li>
</ul>
</li>
<li>é—®é¢˜ï¼š(n-1)-gramçš„å½¢å¼ä¸å­˜åœ¨ï¼Œæ¦‚ç‡ä¸º0<ul>
<li>è§£å†³æ–¹æ¡ˆï¼šå›é€€åˆ°(n-2)-gram</li>
</ul>
</li>
<li>nè¶Šå¤§ï¼Œç¨€ç–é—®é¢˜è¶Šä¸¥é‡</li>
</ul>
<h4 id="generating-text-with-n-gram"><a href="#generating-text-with-n-gram" class="headerlink" title="generating text with n-gram"></a>generating text with n-gram</h4><ul>
<li>å¯ä»¥ç”Ÿæˆè¯­æ³•ä¸Šæ­£ç¡®çš„textï¼Œä½†è¯­ä¹‰ä¸Šä¸è¿è´¯ã€‚</li>
<li>æ¨¡æ‹Ÿè¯­è¨€çš„èƒ½åŠ›éœ€è¦æ›´å¤§çš„nï¼Œä½†ä¸sparsityç›¸æ‚–</li>
</ul>
<h3 id="neural-language-model"><a href="#neural-language-model" class="headerlink" title="neural language model"></a>neural language model</h3><h4 id="fixed-window-neural-language-model"><a href="#fixed-window-neural-language-model" class="headerlink" title="fixed-window neural language model"></a>fixed-window neural language model</h4><p><img src="/uploads/upload_f3fafeddf213bc74ed958d82ad952ba1.png" alt=""></p>
<p>ä¼˜ç‚¹ï¼š</p>
<ul>
<li>ä¸å­˜åœ¨sparsityé—®é¢˜</li>
<li>ä¸éœ€è¦ä½¿ç”¨æ‰€æœ‰n-gram<br>é—®é¢˜ï¼š</li>
<li>çª—å£å¤ªå°</li>
<li>å¢å¤§çª—å£éœ€è¦å¢å¤§Wï¼ˆå¢å¤§æ¨¡å‹ï¼‰</li>
<li>è¾“å…¥ä¹˜ä»¥Wä¸­ä¸åŒçš„æƒé‡ï¼Œå¯¹å•è¯çš„å¤„ç†ä¸å¯¹ç§°</li>
</ul>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p><img src="https://s2.loli.net/2022/01/27/H6MUQuW51aT2ROq.png" alt=""></p>
<ul>
<li>æ ¸å¿ƒæ€æƒ³ï¼šé‡å¤ä½¿ç”¨åŒæ ·çš„Weight<br>ä¼˜ç‚¹ï¼š</li>
<li>å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥</li>
<li>ç†è®ºä¸Šå¯ä»¥åˆ©ç”¨åºåˆ—ä¹‹å‰çš„ä¿¡æ¯</li>
<li>æ¨¡å‹å¤§å°ä¸ä¼šéšè¾“å…¥å¢å¤§è€Œå˜åŒ–</li>
<li>åœ¨æ¯ä¸ªtimestepä¸Šä½¿ç”¨å¯¹ç§°çš„å¤„ç†æ–¹å¼</li>
</ul>
<p>é—®é¢˜ï¼š</p>
<ul>
<li>æ—¶åºçš„é€’å½’è®¡ç®—æ…¢</li>
<li>é•¿ç¨‹ä¾èµ–</li>
</ul>
<h5 id="training-RNN-LM"><a href="#training-RNN-LM" class="headerlink" title="training RNN LM"></a>training RNN LM</h5><p>è®¡ç®—æ¯ä¸€æ­¥çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ</p>
<p>æ¯ä¸€æ­¥çš„æŸå¤±å‡½æ•°ï¼š<br>$J^{(t)}(\theta)=CE(y^{(t)}, \hat{y}^{(t)})=-\Sigma log\ y_w^{(t)}log\ \hat y_w^{(t)}$</p>
<p>æ€»ä½“æŸå¤±å‡½æ•°<br>$J(\theta)=\frac{1}{T}\Sigma\ J^{(t)}(\theta)$</p>
<ul>
<li>åœ¨æ•´ä¸ªcorpus$x^{(1)},\dots,x^{(T)}$ä¸Šè®¡ç®—losså’Œæ¢¯åº¦å¼€é”€å¾ˆå¤§ï¼Œé€šå¸¸å¯¹ä¸€ä¸ªsentenceå’Œdocumentè®¡ç®—å¹¶æ›´æ–°weight</li>
</ul>
<h5 id="backpropagation-for-RNN"><a href="#backpropagation-for-RNN" class="headerlink" title="backpropagation for RNN"></a>backpropagation for RNN</h5><p>$W_h$æ˜¯å…±äº«å‚æ•°ï¼Œæ¢¯åº¦è®¡ç®—ï¼š<br>$\frac{\partial J^{(t)}}{\partial W_h}=\Sigma^t_i\frac{\partial J^{(t)}}{\partial W_h}|_{(i)}$<br>å°†å„æ—¶åˆ»çš„æ¢¯åº¦è®¡ç®—åæ±‚å’Œã€‚</p>
<p>ä»¥ä¸Šç»“è®ºå¯ä»¥é€šè¿‡å¤šå˜é‡å¯¼æ•°çš„è®¡ç®—æ¥è¯æ˜ã€‚</p>
<p>backpropagation through time</p>
<h5 id="generating-text-with-a-RNN-LM"><a href="#generating-text-with-a-RNN-LM" class="headerlink" title="generating text with a RNN LM"></a>generating text with a RNN LM</h5><p>ä¸æ–­é‡‡æ ·ï¼Œé‡‡æ ·è¾“å‡ºæ˜¯ä¸‹ä¸€æ­¥çš„è¾“å…¥ã€‚</p>
<pre><code>- æ¯”n-gramç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ æµç•…ï¼Œä½†æ€»ä½“ä¸Šä»ä¸è¿è´¯
</code></pre><ul>
<li>è€ƒè™‘RNNå’Œæ‰‹å·¥è§„åˆ™çš„ç»“åˆ<ul>
<li>Beam Search</li>
</ul>
</li>
</ul>
<h4 id="evaluating-LM"><a href="#evaluating-LM" class="headerlink" title="evaluating LM"></a>evaluating LM</h4><ul>
<li>å›°æƒ‘åº¦perplexity<br>$\Pi^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},\dots,x^{(1)})})^{(1/T)}$</li>
<li>ç­‰äºCE-lossçš„exp</li>
</ul>
<h4 id="problems-with-vanishing-and-exploding-gradients"><a href="#problems-with-vanishing-and-exploding-gradients" class="headerlink" title="problems with vanishing and exploding gradients"></a>problems with vanishing and exploding gradients</h4><h5 id="vanishing-gradient"><a href="#vanishing-gradient" class="headerlink" title="vanishing gradient"></a>vanishing gradient</h5><ul>
<li>chain ruleä½¿åå‘ä¼ æ’­æ·±å…¥åæ¢¯åº¦ä¿¡å·è¶Šæ¥è¶Šå°</li>
<li>proof<ul>
<li>$h^{(t)} = \sigma(W_hh^{(t-1)}+W_xx^{(t)}+b_!)$</li>
<li>å‡è®¾$\sigma(x)=x$</li>
<li>$\frac{\partial h^{(t)}}{\partial h^{(t-1)}}=diag(\sigmaâ€™(W_hh^{(t-1)}+w_x^{(t)}+b_1))W_h=W_h$</li>
<li>é‚£ä¹ˆ$\frac{\partial J^{(i)}(\theta)}{\partial h^{(j)}}=\frac{\partial J^{(i)}(\theta)}{\partial h^{(i)}}W_h^{(i-j)}$</li>
<li>å¦‚æœ$W_h$çš„ç‰¹å¾å€¼å‡å°äº1(å› ä¸ºä½¿ç”¨äº†çº¿æ€§çš„$\sigma$æ‰€ä»¥ä¸º1)ï¼Œåˆ™æ¢¯åº¦ä¼šå‘ˆæŒ‡æ•°è¡°å‡ï¼›</li>
</ul>
</li>
</ul>
<p>RNN-LMæ›´å–„äºä»<strong>é¡ºåºä¸Šç›¸è¿‘</strong>çš„è€Œé<strong>è¯­æ³•ä¸Šç›¸è¿‘</strong>çš„å•è¯å­¦ä¹ </p>
<h5 id="exploding-gradient"><a href="#exploding-gradient" class="headerlink" title="exploding gradient"></a>exploding gradient</h5><p>$\theta^{new} = \theta^{old} - \alpha \bigtriangledown_\theta J(\theta)$</p>
<ul>
<li>æ¢¯åº¦çˆ†ç‚¸ä¼šè®©SGDçš„â€œæ­¥å­â€è¿‡å¤§</li>
<li>æç«¯æƒ…å†µä¸‹ä¼šå¯¼è‡´ç½‘ç»œä¸­å‡ºç°infæˆ–NaN</li>
</ul>
<h5 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h5><p><img src="https://s2.loli.net/2022/01/27/sIQich9d8Smvqf4.png" alt=""></p>
<ul>
<li>åœ¨åŒæ ·çš„æ–¹å‘ä¸Štake a smaller step</li>
</ul>
<h5 id="how-to-fix-the-vanishing-gradient-problem"><a href="#how-to-fix-the-vanishing-gradient-problem" class="headerlink" title="how to fix the vanishing gradient problem"></a>how to fix the vanishing gradient problem</h5><ul>
<li>RNNä¸­å¾ˆéš¾å­¦ä¹ è‹¥å¹²æ—¶é—´æ­¥ä¹‹å‰çš„ä¿¡æ¯</li>
<li>ä½¿ç”¨å…·æœ‰å•ç‹¬è®°å¿†çš„RNN</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul>
<li>åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæ‹¥æœ‰ä¸¤ä¸ªhidden vector<ul>
<li>hidden state $h^{(t)}$å’Œcell state $c^{(t)}$</li>
<li>äºŒè€…éƒ½æ˜¯é•¿åº¦ä¸ºnçš„å‘é‡</li>
<li>cell stateä¿å­˜é•¿æœŸä¿¡æ¯</li>
<li>LSTMä½•ä»¥ä»cellè¯»å†™åˆ ä¿¡æ¯</li>
</ul>
</li>
<li>ä¿¡æ¯è¢«è¯»/å†™/åˆ ç”±ä¸‰ä¸ªå¯¹åº”çš„gateæ§åˆ¶<ul>
<li>gatesä¹Ÿæ˜¯é•¿åº¦ä¸ºnçš„å‘é‡</li>
<li>æ¯ä¸€æ­¥ä¸Šï¼Œgateçš„æ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯open 1/close 0/ä¸¤è€…ä¹‹é—´çš„ä¸€ä¸ªæ¦‚ç‡</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/SgTA4K5b3jLlha9.png" alt=""><br><img src="https://s2.loli.net/2022/01/27/QWURPti4zaoZBET.png" alt=""></p>
<ul>
<li>LSTMç›¸æ¯”ç»å…¸RNNæ›´å®¹æ˜“ä¿å­˜æ—¶é—´æ­¥ä¸Šçš„ä¿¡æ¯</li>
<li>å¦‚æœforget gate ä¿å­˜ä¸º1ï¼Œåˆ™å¯ä»¥ä¿å­˜æ‰€æœ‰å†å²ä¿¡æ¯</li>
<li>LSTMçš„å‚æ•°ä¹Ÿæ›´éš¾å­¦ä¹ </li>
<li>ä¸ä¿è¯æ²¡æœ‰æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸</li>
</ul>
<h4 id="is-vanishing-exploding-gradient-just-a-RNN-problemï¼Ÿ"><a href="#is-vanishing-exploding-gradient-just-a-RNN-problemï¼Ÿ" class="headerlink" title="is vanishing/exploding gradient just a RNN problemï¼Ÿ"></a>is vanishing/exploding gradient just a RNN problemï¼Ÿ</h4><ul>
<li>æ‰€æœ‰çš„ç¥ç»ç½‘ç»œç»“æ„ï¼ˆå°¤å…¶æ˜¯æ·±åº¦è§£æ„ï¼‰éƒ½ä¼šæœ‰<ul>
<li>chain ruleå’Œéçº¿æ€§å‡½æ•°ä¼šå¯¼è‡´åå‘ä¼ æ’­æ—¶æ¢¯åº¦å¾ˆå°</li>
<li>ä½å±‚æ¬¡çš„å­¦ä¹ å¾ˆæ…¢</li>
<li>RNNä¸­é‡å¤ä¹˜ä»¥ç»Ÿä¸€æ¢¯åº¦ï¼Œæ›´åŠ æ˜æ˜¾</li>
</ul>
</li>
<li>solutionï¼šæ·»åŠ direct coneection<ul>
<li>residual connection</li>
</ul>
</li>
</ul>
<h4 id="bidirectional-RNN"><a href="#bidirectional-RNN" class="headerlink" title="bidirectional RNN"></a>bidirectional RNN</h4><p><img src="https://s2.loli.net/2022/01/27/bLHlzje3ZtnuNDG.png" alt=""></p>
<ul>
<li>åŒ…å«forward RNNå’Œbackward RNN</li>
<li>éœ€è¦æœ‰æ•´ä¸ªè¾“å…¥åºåˆ—æ—¶æ‰èƒ½ä½¿ç”¨ï¼ˆLMä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªwordæ—¶ï¼Œç›¸å½“äºåªæœ‰å·¦è¾¹çš„contextï¼Œæ•…ä¸èƒ½ä½¿ç”¨åŒå‘RNNï¼‰</li>
</ul>
<h4 id="multi-layer-RNN"><a href="#multi-layer-RNN" class="headerlink" title="multi-layer RNN"></a>multi-layer RNN</h4><ul>
<li>å †å å¤šå±‚RNNï¼Œåœ¨ç©ºé—´ç»´åº¦ä¸Šæ·±å…¥ï¼ˆRNNæœ¬èº«æ˜¯åœ¨æ—¶åºä¸Šæ·±å…¥ï¼‰</li>
<li>æ•è·ä¸åŒå±‚æ¬¡çš„ç‰¹å¾</li>
<li>iå±‚çš„è¾“å…¥æ˜¯i-1å±‚çš„hidden state</li>
<li>ç»éªŒæ³•åˆ™ï¼š2åˆ°4å±‚æ¯”è¾ƒåˆé€‚</li>
<li></li>
</ul>
<p><em>Lecture 7. Machine Translation, Attention, Subword Models</em></p>
<hr>
<h2 id="machine-translation"><a href="#machine-translation" class="headerlink" title="machine translation"></a>machine translation</h2><h3 id="statistical-MT"><a href="#statistical-MT" class="headerlink" title="statistical MT"></a>statistical MT</h3><ul>
<li>ç»™å®šæ³•è¯­sentence xï¼Œæƒ³è¦æ‰¾åˆ°æœ€å¥½çš„è‹±è¯­sentence y</li>
<li>$argmax_xP(y|x)$</li>
<li>ç”¨bayes ruleå¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªç»„ä»¶ï¼š$argmax_xP(x|y)P(y)$</li>
<li>$P(x|y)$<ul>
<li>ç¿»è¯‘æ¨¡å‹ï¼Œåˆ†æå•è¯å’ŒçŸ­è¯­åº”è¯¥å¦‚ä½•ç¿»è¯‘</li>
<li>ä»äººå·¥ç¿»è¯‘çš„å¹¶è¡Œæ•°æ®ä¸­å­¦ä¹ </li>
</ul>
</li>
<li>$P(y)$<ul>
<li>è¯­è¨€æ¨¡å‹ï¼Œæ¨¡å‹å¦‚ä½•ç”Ÿæˆåˆé€‚çš„è¯­è¨€</li>
<li>ä»å•è¯æ•°æ®ä¸­å­¦ä¹ </li>
</ul>
</li>
</ul>
<h4 id="learning-alignment-for-SMT"><a href="#learning-alignment-for-SMT" class="headerlink" title="learning alignment for SMT"></a>learning alignment for SMT</h4><ul>
<li>éœ€è¦å­¦ä¹ $P(x,a|y)$,å…¶ä¸­$a$æ˜¯å•è¯é—´çš„å¯¹åº”</li>
<li>$a$æ˜¯éšå˜é‡ï¼Œéœ€è¦ç”¨EMç®—æ³•ç­‰æ¥å­¦ä¹ </li>
</ul>
<h4 id="decoding-for-SMT"><a href="#decoding-for-SMT" class="headerlink" title="decoding for SMT"></a>decoding for SMT</h4><ul>
<li>å¦‚ä½•è®¡ç®—argmaxï¼Œåˆ—ä¸¾æ‰€æœ‰yå¹¶è®¡ç®—æ¦‚ç‡æ˜¾ç„¶ä¸å¯è¡Œ</li>
<li>ç”¨å¯å‘å¼ç®—æ³•ä¸¢å¼ƒæ¦‚ç‡è¿‡ä½çš„å‡è®¾</li>
</ul>
<h3 id="neural-MT"><a href="#neural-MT" class="headerlink" title="neural MT"></a>neural MT</h3><h4 id="sequence-to-sequence-model"><a href="#sequence-to-sequence-model" class="headerlink" title="sequence-to-sequence model"></a>sequence-to-sequence model</h4><ul>
<li>encoder+decoder</li>
<li>ç”¨äºå¾ˆå¤šNLPä»»åŠ¡<ul>
<li>æ–‡æœ¬æ‘˜è¦</li>
<li>å¯¹è¯ç”Ÿæˆ</li>
<li>æ–‡æœ¬è§£æ</li>
<li>ä»£ç ç”Ÿæˆ</li>
</ul>
</li>
<li>æ˜¯ä¸€ç§<strong>conditional LM</strong><ul>
<li>$P(y|x)=P(y_1|x)P(y_2|y_1,x)\dots P(y_T|y_1,\dots,y_{T-1},x)$</li>
</ul>
</li>
<li>sequence-to-sequenceæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œåå‘ä¼ æ’­åœ¨æ•´ä¸ªç³»ç»Ÿä¸­ã€‚</li>
</ul>
<h4 id="greedy-decoding"><a href="#greedy-decoding" class="headerlink" title="greedy decoding"></a>greedy decoding</h4><ul>
<li>åœ¨è§£ç å™¨çš„æ¯ä¸€æ­¥éƒ½ä½¿ç”¨argmaxï¼ˆä½†æ€»ä½“çš„æ¦‚ç‡å¹¶éæœ€å¤§ï¼‰</li>
<li>æ— æ³•æ’¤é”€æ­¥éª¤</li>
</ul>
<h4 id="exhaustive-search-decoding"><a href="#exhaustive-search-decoding" class="headerlink" title="exhaustive search decoding"></a>exhaustive search decoding</h4><ul>
<li>è®¡ç®—æ‰€æœ‰å¯èƒ½çš„åºåˆ—ï¼Œå¼€é”€å¾ˆå¤§ï¼Œæœ‰V^Tç§å¯èƒ½</li>
</ul>
<h4 id="beam-search-decoding"><a href="#beam-search-decoding" class="headerlink" title="beam search decoding"></a>beam search decoding</h4><ul>
<li>æ ¸å¿ƒæ€æƒ³ï¼šåœ¨decoderçš„æ¯ä¸€æ­¥ï¼Œè·Ÿè¸ªkä¸ªæœ€å¯èƒ½çš„è¾“å‡ºï¼ˆhypothesesï¼‰<ul>
<li>kæ˜¯beam sizeï¼Œé€šå¸¸ä¸º5-10</li>
<li>æŒ‰å¾—åˆ†é€‰æ‹©$score=log P_{LM}(y_1,\dots,y_t|x)=\Sigma^t_{i=1}log P_{LM}(y_i|y_1,\dots,y_{i-1},x)$</li>
</ul>
</li>
<li>ä¸ä¸€å®šèƒ½æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œä½†æ•ˆç‡æ¯”ç©·ä¸¾é«˜çš„å¤š</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/BfdEmkpctlAq4bC.png" alt=""></p>
<h5 id="stopping-criterion"><a href="#stopping-criterion" class="headerlink" title="stopping criterion"></a>stopping criterion</h5><ul>
<li>åœ¨greedy decodingä¸­ä¼šæŒç»­è§£ç åˆ°$<END>$ token</li>
<li>åœ¨beem search decoding ä¸­ï¼Œå¯èƒ½ä¼šæå‰ç”Ÿæˆ$<END>$ token<ul>
<li>ä¸€ä¸ªhypothesisåˆ°è¾¾$<END>$ï¼Œåˆ™å…¶å·²ç»å®Œæˆ</li>
<li>ç»§ç»­å¤„ç†å…¶ä»–hypothesis</li>
</ul>
</li>
<li>beem search decodingçš„åœæ­¢æ¡ä»¶ï¼š<ul>
<li>åˆ°è¾¾é¢„è®¾timestep T</li>
<li>æœ‰é¢„è®¾çš„nä¸ªhypothesiså·²ç»å®Œæˆ</li>
</ul>
</li>
</ul>
<h5 id="finishing-up"><a href="#finishing-up" class="headerlink" title="finishing up"></a>finishing up</h5><ul>
<li>ç»“æŸæ—¶è·å¾—äº†è‹¥å¹²ä¸ªhypothesisï¼Œéœ€è¦ä»ä¸­é€‰æ‹©æœ€åˆé€‚çš„</li>
<li>å¦‚æœç›´æ¥ä½¿ç”¨scoreï¼Œåˆ™é•¿åº¦è¶Šé•¿scoreè¶Šå°</li>
<li>éœ€è¦å¯¹scoreæŒ‰é•¿åº¦æ ‡å‡†åŒ–ï¼ˆä¹˜$\frac{1}{T}$ï¼‰</li>
</ul>
<h4 id="advantages-and-disadvantages-of-NMT"><a href="#advantages-and-disadvantages-of-NMT" class="headerlink" title="advantages and disadvantages of NMT"></a>advantages and disadvantages of NMT</h4><ul>
<li>advantage<ul>
<li>æ€§èƒ½æ›´å¥½ï¼Œç¿»è¯‘æ›´é€šé¡ºï¼Œä»ä¸Šä¸‹æ–‡ä½¿ç”¨äº†æ›´å¤šä¿¡æ¯</li>
<li>ç«¯åˆ°ç«¯ç³»ç»Ÿ</li>
<li>æ‰‹åŠ¨å·¥ä½œæ›´å°‘ï¼Œä¸éœ€è¦ç‰¹å¾å·¥ç¨‹å’Œå¯¹æ¯ç§è¯­è¨€å•ç‹¬å»ºæ¨¡</li>
</ul>
</li>
<li>disadvantage<ul>
<li>å¯è§£é‡Šæ€§</li>
<li>å¯æ§æ€§ï¼ˆæ— æ³•åˆ¶å®šè§„åˆ™ï¼‰</li>
<li>å®‰å…¨æ€§</li>
</ul>
</li>
</ul>
<h3 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h3><ul>
<li>æºåºåˆ—encodingè¿‡ç¨‹éœ€è¦æ•è·æºè¯­å¥çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå­˜åœ¨ä¿¡æ¯ç“¶é¢ˆ</li>
<li>attentionçš„æ ¸å¿ƒæ€æƒ³ï¼šåœ¨decoderçš„æ¯ä¸€æ­¥ï¼Œç”¨encoderçš„è¿æ¥æ¥ä½¿å…¶èƒ½å…³æ³¨åˆ°æºåºåˆ—çš„ç‰¹å®šéƒ¨åˆ†</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/wRmljrWgoh15OTi.png" alt=""></p>
<p><em>Lecture 8. self-attention and Transformer</em></p>
<hr>
<h4 id="attention-in-equations"><a href="#attention-in-equations" class="headerlink" title="attention in equations"></a>attention in equations</h4><ul>
<li>encoder hidden states: $h_i\in \mathbb{R}^h$</li>
<li>decoder hidden state on timestep t: $s_t\in \mathbb{R}^h$</li>
<li>attention scores: $e^t=[s^T_th_1,\dots,s_t^Th_N]\in\mathbb{R}^N$</li>
<li>attention distribution: $\alpha^t={\rm softmax}(e^t)\in\mathbb{R}^N$</li>
<li>attention output $\mathbf{a}_t=\Sigma_{i=1}^N\alpha_i^t\mathbf{h}_i\in\mathbb{R}^h$</li>
<li>$[\mathbf{a}_t,s_t]\in\mathbb{R}^{2h}$</li>
</ul>
<h4 id="advantages-of-attention"><a href="#advantages-of-attention" class="headerlink" title="advantages of attention"></a>advantages of attention</h4><ul>
<li>ç»•è¿‡äº†ä¿¡æ¯ç“¶é¢ˆçš„é—®é¢˜ï¼Œç›´æ¥å…è®¸decoderè¿æ¥æ¥è‡ªsourceçš„ä¿¡æ¯</li>
<li>æ³¨æ„åŠ›æœºåˆ¶æ›´human-like</li>
<li>å¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜â€”â€”direct connection</li>
<li>å¯è§£é‡Šæ€§<ul>
<li>æä¾›äº†soft alignmentï¼ˆç½‘ç»œè‡ªå·±å­¦ä¹ äº†alignmentï¼‰</li>
<li>é€šè¿‡æ³¨æ„åŠ›åˆ†å¸ƒå¯ä»¥çœ‹åˆ°decoderçš„å…³æ³¨ç‚¹</li>
</ul>
</li>
</ul>
<h4 id="attention-variants"><a href="#attention-variants" class="headerlink" title="attention variants"></a>attention variants</h4><p><strong>query attends to the values</strong></p>
<p>ç»™å®švalues $h_!,\dots,h_N\in\mathbb{R}^{d_1}$å’Œquery $s\in\mathbb{R}^{d_2}$ï¼Œattentionæœºåˆ¶åŒ…æ‹¬ï¼š</p>
<ul>
<li>è®¡ç®—attention scores $e\in\mathbb{R}^N$</li>
<li>è®¡ç®—softmaxè·å¾—attention distribution $\alpha={\rm softmax}(e)\in\mathbb{R}^N$</li>
<li>å¯¹attention distributionåŠ æƒæ±‚å’Œ  $\mathbf{a}=\Sigma_{i=1}^N\alpha_i\mathbf{h}_i\in\mathbb{R}^{d_1}$</li>
</ul>
<p>å‡ ç§attentionæ–¹æ³•</p>
<ul>
<li>dot-product attention: $e_i=s^Th_i$<ul>
<li>è¦æ±‚$s$å’Œ$h$ç»´åº¦ç›¸åŒ</li>
</ul>
</li>
<li>multiplicative attention: $e_i=s^TWH_i$<ul>
<li>$W\in\mathbb{R}^{d_1\times d_2}$æ˜¯ä¸€ä¸ªé¢å¤–çš„æƒé‡çŸ©é˜µ</li>
<li>æƒé‡ä¼¼ä¹è¿‡å¤šï¼Œéœ€è¦å­¦ä¹ å¯¹æ‰€æœ‰éƒ¨åˆ†çš„attention</li>
</ul>
</li>
<li>reduced rank multiplicative attention: $e_i=s^T(U^TV)H_i=(Us)^T(Vh_i)$<ul>
<li>$I\in\mathbb{R}^{k\times d_2}, V\in\mathbb{R}^{k\times d_1},k\ll d_1,d_2$</li>
</ul>
</li>
<li>additive attention: $e_i=v^T{\rm tanh}(W_1h_i+W_2s)$<ul>
<li>$W_1\in\mathbb{R}^{d_3\times d_1},W_2\in\mathbb{R}^{d_3\times d_2}$æ˜¯æƒé‡çŸ©é˜µï¼Œ$v\in\mathbb{R}^{d_3}$æ˜¯æƒé‡å‘é‡</li>
<li>å®é™…ä¸Šç”¨äº†ä¸€ä¸ªç½‘ç»œæ¥è®¡ç®—attention</li>
</ul>
</li>
</ul>
<p><em>Lecture 9. Transformers</em></p>
<hr>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Issues-with-RNN"><a href="#Issues-with-RNN" class="headerlink" title="Issues with RNN"></a>Issues with RNN</h3><ul>
<li>Linear interaction distanceï¼šæ•è·ä¸¤ä¸ªè¯çš„äº¤äº’éœ€è¦$O(sequence length)$ steps<ul>
<li>é•¿ç¨‹ä¾èµ–é—®é¢˜ï¼ˆæ¢¯åº¦éš¾ä»¥ä¼ æ’­ï¼‰</li>
<li>çº¿æ€§é¡ºåºçš„è¾“å…¥å¤„ç†sentenceä¸åˆç†</li>
</ul>
</li>
<li>Lack of parallelizability<ul>
<li>forward å’Œ backwardéƒ½éœ€è¦$O(sequence length)$</li>
</ul>
</li>
</ul>
<h3 id="if-not-recurrence-the-what"><a href="#if-not-recurrence-the-what" class="headerlink" title="if not recurrence, the what?"></a>if not recurrence, the what?</h3><ul>
<li>word window(CNN)<ul>
<li>1D-Conv</li>
<li>maximum interaction distance = sequence length/window sizeâ€”â€”é€šè¿‡å¤šå±‚å·ç§¯</li>
</ul>
</li>
<li>attention</li>
</ul>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h3><h4 id="self-attention-equation"><a href="#self-attention-equation" class="headerlink" title="self-attention equation"></a>self-attention equation</h4><ul>
<li>query $q_i\in\mathbb{R}^d$</li>
<li>key $k_i\in\mathbb{R}^d$</li>
<li>value $v_i\in\mathbb{R}^d$</li>
<li>å¯¹self-attentionæ¥è¯´ï¼Œq,k,væ¥è‡ªåŒä¸€ä¸ªä¸œè¥¿<ul>
<li>æœ€ç®€å•çš„æ–¹å¼æ˜¯$q_i=k_i=v_i=x_i$</li>
<li>æ­¤æ—¶$e_{ij}=q_i^Tk_j$</li>
<li>$\alpha_{ij}=\frac{exp(e_{ij})}{\Sigma_{jâ€™}exp(e_{ijâ€™})}$</li>
<li>$output=\Sigma_j{\alpha_{ij}v_j}$</li>
</ul>
</li>
</ul>
<h4 id="Problem1-sequence-order-position-embedding"><a href="#Problem1-sequence-order-position-embedding" class="headerlink" title="Problem1. sequence order(position embedding)"></a>Problem1. sequence order(position embedding)</h4><ul>
<li>self-attentionå¯¹è¯­åºå®Œå…¨ä¸æ•æ„Ÿï¼Œæ‰€ä»¥éœ€è¦ç¼–ç order</li>
<li>è€ƒè™‘å°†sequence indexç¼–ç ä¸ºvector $p_i$ï¼ŒåŠ åˆ°qkvä¸Š</li>
</ul>
<h5 id="sinusoidal-position-representations"><a href="#sinusoidal-position-representations" class="headerlink" title="sinusoidal position representations"></a>sinusoidal position representations</h5><p>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$<br>$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$</p>
<ul>
<li>ç»å¯¹ä½ç½®å¹¶ä¸é‚£ä¹ˆé‡è¦</li>
<li>å¯ä»¥å¤–æ¨åˆ°æ›´é•¿çš„é•¿åº¦ï¼ˆå‘¨æœŸæ€§ï¼‰</li>
<li>æ²¡æœ‰å¯å­¦ä¹ çš„å‚æ•°</li>
</ul>
<h5 id="learned-absolute-position-representation"><a href="#learned-absolute-position-representation" class="headerlink" title="learned absolute position representation"></a>learned absolute position representation</h5><p>å­¦ä¹ position embedding matrix $p \in \mathbb{R}^{d\times T}$</p>
<ul>
<li>æ¯”è¾ƒçµæ´»ï¼Œæ¯ä¸ªä½ç½®éƒ½å¯ä»¥æ ¹æ®dataå»å­¦ä¹ </li>
<li>é•¿åº¦å›ºå®šï¼Œæ— æ³•å¤–æ¨</li>
</ul>
<h4 id="Problem2-lack-of-nonlinearities"><a href="#Problem2-lack-of-nonlinearities" class="headerlink" title="Problem2. lack of nonlinearities"></a>Problem2. lack of nonlinearities</h4><ul>
<li>self-attentionåªæ˜¯weighted averageï¼Œä¸åŒ…å«éçº¿æ€§</li>
<li>è§£å†³æ–¹æ¡ˆ<ul>
<li>æ·»åŠ ä¸€ä¸ªfeed-forward networkå¯¹è¾“å‡ºåšåå¤„ç†</li>
<li>$m_i=MLP(o_i)=W_2ReLu(W_1o_i+b_1)+b_2$</li>
</ul>
</li>
</ul>
<h4 id="Problem3-â€œdonâ€™t-look-at-the-featureâ€"><a href="#Problem3-â€œdonâ€™t-look-at-the-featureâ€" class="headerlink" title="Problem3. â€œdonâ€™t look at the featureâ€"></a>Problem3. â€œdonâ€™t look at the featureâ€</h4><ul>
<li>åœ¨decoderä¸­éœ€è¦mask</li>
<li>mask out attention to future wordsâ€”â€”å°†attention scoresè®¾ç½®ä¸º$-\infty$</li>
</ul>
<h3 id="Transformer-1"><a href="#Transformer-1" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="key-query-value-attention"><a href="#key-query-value-attention" class="headerlink" title="key-query-value attention"></a>key-query-value attention</h4><ul>
<li>$k_i=Kx_i, K \in\mathbb{R}^{d\times d}\text{ is the key matrix}$</li>
<li>$q_i=Qx_i, Q \in\mathbb{R}^{d\times d}\text{ is the query matrix}$</li>
<li>$v_i=Vx_i, V \in\mathbb{R}^{d\times d}\text{ is the value matrix}$</li>
<li>$output=softmax(XQK^TX^T)XV$</li>
</ul>
<h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><ul>
<li>$Q_l,K_l,V_l\in \mathbb{R}^{d\times\frac{d}{h}}$</li>
<li>è®¡ç®—attentionç„¶åconcat</li>
</ul>
<h4 id="Residual-connection"><a href="#Residual-connection" class="headerlink" title="Residual connection"></a>Residual connection</h4><ul>
<li>instead of $X^{(i)}=Layer(X^{(i-1)})$</li>
<li>let $X^{(i)}=X^{(i-1)+Layer(X^{(i-1)})$</li>
</ul>
<h4 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h4><ul>
<li>layernormçš„æˆåŠŸå¯èƒ½å› ä¸ºå…¶å¯¹æ¢¯åº¦çš„norm</li>
<li>$x\in\mathbb{R}^d$æ˜¯ä¸€ä¸ªå•ç‹¬çš„word vector</li>
<li>$\mu=\Sigma_{j=1}^dx_j\in \mathbb{R}$æ˜¯å‡å€¼</li>
<li>$\sigma=\sqrt{\frac{1}{d}\Sigma_{j=1}^d(x_j-\mu)^2}\in\mathbb{R}$æ˜¯æ ‡å‡†å·®</li>
<li>$\gamma\in\mathbb{R}^d,\beta\in\mathbb{R}^d$æ˜¯å¯å­¦ä¹ çš„gainå’Œbiaså‚æ•°(å·¥ä¸šä¸Šçš„ä¹ æƒ¯)</li>
<li>$output=\frac{x-\mu}{\sigma+\epsilon}*\gamma+\beta$</li>
</ul>
<h4 id="scaled-dot-product"><a href="#scaled-dot-product" class="headerlink" title="scaled dot-product"></a>scaled dot-product</h4><ul>
<li>å‘é‡ç»´åº¦å¾ˆå¤§æ—¶ï¼Œdot-productä¹Ÿä¼šå¾ˆå¤§ï¼Œå¯¼è‡´softmaxå¾ˆé™¡å³­ï¼Œæ¢¯åº¦å¾ˆå°</li>
<li>${\rm Attention}(Q,K,V)={\rm softmax}(\frac{QK_T}{\sqrt{d_k}})V$</li>
</ul>
<h4 id="why-fix-about-Transformer"><a href="#why-fix-about-Transformer" class="headerlink" title="why fix about Transformer"></a>why fix about Transformer</h4><ul>
<li>quadratic compute in self attention<ul>
<li>linear in RNN</li>
</ul>
</li>
<li>position representation<ul>
<li>relative linear position attention</li>
<li>dependency syntax-based position</li>
</ul>
</li>
</ul>
<h4 id="varient-of-Transformer"><a href="#varient-of-Transformer" class="headerlink" title="varient of Transformer"></a>varient of Transformer</h4><ul>
<li>for quadratic compute<ul>
<li>æŠ•å½±åˆ°æ›´ä½ç»´åº¦è®¡ç®—attention ï¼ˆlinformerï¼‰</li>
<li>ä¸è®¡ç®—æ‰€æœ‰çš„pairsï¼Œåªè®¡ç®—ä¸€éƒ¨åˆ†ï¼ˆBigBirdï¼‰</li>
</ul>
</li>
</ul>
<h4 id="for-pretrain"><a href="#for-pretrain" class="headerlink" title="for pretrain"></a>for pretrain</h4><ul>
<li>å¦‚æœåªç”¨decoderï¼Œåˆ™ä¸éœ€è¦cross attentionå’Œlayernorm</li>
</ul>
<p><em>Lecture 10. Transformers and pretrain</em></p>
<hr>
<h2 id="word-structure-and-subword-models"><a href="#word-structure-and-subword-models" class="headerlink" title="word structure and subword models"></a>word structure and subword models</h2><ul>
<li>æµ‹è¯•æ—¶é‡åˆ°çš„OOV wordå°†ä¼šè¢«æ˜ å°„ä¸ºUNK</li>
<li>æœ‰é™çš„è¯æ±‡æ˜¾ç„¶ä¸èƒ½æ»¡è¶³å¾ˆå¤šè¯­å¢ƒ</li>
</ul>
<h3 id="the-byte-pair-encoding-algorithm"><a href="#the-byte-pair-encoding-algorithm" class="headerlink" title="the byte-pair encoding algorithm"></a>the byte-pair encoding algorithm</h3><ul>
<li>ä»åªåŒ…å«å•ä¸ªå­—æ¯çš„vocabularyå¼€å§‹</li>
<li>å¯»æ‰¾æœ€å¸¸ç›¸é‚»å‡ºç°çš„å­—æ¯ï¼Œå°†å…¶æ·»åŠ ä¸ºsubword</li>
<li>ä¸æ–­æ·»åŠ ç›´åˆ°ç‰¹å®šçš„å¤§å°</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/HGZQu7p3WcgEf9L.png" alt=""></p>
<h2 id="Motivating-model-pretraining-from-word-embedding"><a href="#Motivating-model-pretraining-from-word-embedding" class="headerlink" title="Motivating model pretraining from word embedding"></a>Motivating model pretraining from word embedding</h2><h3 id="two-types-of-pretrain"><a href="#two-types-of-pretrain" class="headerlink" title="two types of pretrain"></a>two types of pretrain</h3><h4 id="pretrained-word-embedding"><a href="#pretrained-word-embedding" class="headerlink" title="pretrained word embedding"></a>pretrained word embedding</h4><ul>
<li>ä»…é¢„è®­ç»ƒè¯å‘é‡</li>
<li>é¢„è®­ç»ƒçš„è®­ç»ƒæ•°æ®å¿…é¡»èƒ½å¤Ÿå……åˆ†å­¦ä¹ ä¸Šä¸‹æ–‡<ul>
<li>ä¸€ä¸ªwordçš„é¢„è®­ç»ƒè¯å‘é‡æ˜¯å›ºå®šçš„ï¼Œä¸ä¸Šä¸‹æ–‡æ— å…³</li>
</ul>
</li>
<li>æ•´ä¸ªç½‘ç»œçš„å¤§éƒ¨åˆ†ä»æ˜¯éšæœºåˆå§‹åŒ–çš„</li>
</ul>
<h4 id="pretraining-whole-models"><a href="#pretraining-whole-models" class="headerlink" title="pretraining whole models"></a>pretraining whole models</h4><ul>
<li>æ‰€æœ‰å‚æ•°éƒ½è¢«pretrain</li>
<li>å¯ä»¥å­¦ä¹ è¯­è¨€çš„æ¦‚ç‡åˆ†å¸ƒ</li>
</ul>
<h3 id="three-ways-of-model-pretrain"><a href="#three-ways-of-model-pretrain" class="headerlink" title="three ways of model pretrain"></a>three ways of model pretrain</h3><ul>
<li>decoder<ul>
<li>LM</li>
<li>ç”Ÿæˆä»»åŠ¡ï¼Œçœ‹ä¸åˆ°future word</li>
</ul>
</li>
<li>encoder<ul>
<li>æœ‰åŒå‘çš„context</li>
<li>å¦‚ä½•è®­ç»ƒï¼Ÿï¼Ÿ</li>
</ul>
</li>
<li>encoder-decoder </li>
</ul>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul>
<li>finetuneå¯ä»¥å¿½ç•¥pretrained modelåŸæœ¬çš„ä»»åŠ¡<br><img src="https://s2.loli.net/2022/01/27/xKmtgsUA3S4Y61V.png" alt=""></li>
<li><p>å¯ä»¥ç»§ç»­ç”¨generatorçš„æ–¹æ³•(Prompt)<br><img src="https://s2.loli.net/2022/01/27/PrljySvV9sBF7RW.png" alt=""></p>
</li>
<li><p>GPT</p>
<ul>
<li>12å±‚ Transformer decoder </li>
</ul>
</li>
<li>GPT-2<ul>
<li>æ›´å¤§çš„GPT</li>
</ul>
</li>
</ul>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul>
<li>encoderæ–¹æ³•ä¸­å¯ä»¥çœ‹åˆ°åŒå‘çš„ä¸Šä¸‹æ–‡ï¼Œä¸èƒ½ç›´æ¥ç”¨LMæ–¹æ³•è®­ç»ƒ</li>
<li><p>Ideaï¼šç”¨[MASK] tokenç›–ä½éƒ¨åˆ†wordï¼Œè®©æ¨¡å‹é¢„æµ‹è¢«ç›–ä½çš„word</p>
<ul>
<li>åªæ·»åŠ masked word çš„lossï¼Œç§°ä½œMasked LM </li>
</ul>
</li>
<li><p>BERT</p>
<ul>
<li>é¢„æµ‹15%çš„(sub)word<ul>
<li>80% ç”¨[mask]æ›¿æ¢</li>
<li>10% ç”¨éšæœºçš„tokenæ›¿æ¢</li>
<li>10% ä¸å˜ï¼ˆä½†ä»ç„¶predictï¼‰</li>
</ul>
</li>
<li>bertä¹Ÿé¢„æµ‹ä¸‹ä¸€ä¸ªchunkæ˜¯çœŸå®followçš„è¿˜æ˜¯éšæœºé‡‡æ ·çš„ï¼Œå³next sentence predictionï¼ˆNSPï¼‰</li>
</ul>
</li>
</ul>
<h5 id="limitation-of-pretrained-encoders"><a href="#limitation-of-pretrained-encoders" class="headerlink" title="limitation of pretrained encoders"></a>limitation of pretrained encoders</h5><ul>
<li>åœ¨ç”Ÿæˆä»»åŠ¡ä¸Šæœ€å¥½ä½¿ç”¨decoder</li>
<li>bertåœ¨è‡ªå›å½’ä»»åŠ¡ä¸Šå¹¶æ²¡æœ‰é‚£ä¹ˆå¥½</li>
</ul>
<h5 id="extension-of-bert"><a href="#extension-of-bert" class="headerlink" title="extension of bert"></a>extension of bert</h5><ul>
<li>RoBERTaï¼štrain BERT for longer and remove next sentence prediction</li>
<li>SpanBERTï¼šè¿ç»­maskâ€”â€”æ›´éš¾ä¸”æœ‰ç”¨çš„ä»»åŠ¡</li>
</ul>
<h4 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h4><p><img src="https://s2.loli.net/2022/01/27/ezkWJgcwqCl6hjA.png" alt=""></p>
<ul>
<li>Encoderç¼–ç prefixä¸”ä¸è¿›è¡Œé¢„æµ‹ï¼ŒDecoderé¢„æµ‹åé¢çš„éƒ¨åˆ†</li>
<li><p>encoderä»contextå—ç›Šï¼Œdecoderåˆ™åƒLMä¸€æ ·é¢„æµ‹</p>
</li>
<li><p>T5<br><img src="https://s2.loli.net/2022/01/27/HTqr6XfiCj8IEZS.png" alt=""></p>
<ul>
<li>span corruption<ul>
<li>ä»è¾“å…¥ä¸­ç”¨ç‰¹å®šçš„å ä½ç¬¦å»æ›¿ä»£ä¸åŒé•¿åº¦çš„æ–‡æœ¬ï¼ˆå’ŒBERTä¸åŒçš„æ˜¯ï¼ŒT5åªçŸ¥é“æ­¤å¤„ç¼ºå¤±ï¼Œä¸çŸ¥é“ç¼ºå¤±çš„é•¿åº¦ï¼‰</li>
<li>decoderè§£ç å‡ºç¼ºå¤±éƒ¨åˆ†</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="very-large-model-and-in-context-learning"><a href="#very-large-model-and-in-context-learning" class="headerlink" title="very large model and in-context learning"></a>very large model and in-context learning</h4><ul>
<li>ä¸¤ç§ä½¿ç”¨pretrained modelçš„æ–¹å¼ï¼š<ul>
<li>prompt</li>
<li>finetune</li>
</ul>
</li>
<li>GPT-3<ul>
<li>seems to perform some kind of learning <strong>withou gradient steps</strong> simply from examples you provide within their contexts</li>
<li>in-context learningï¼šç”šè‡³æ²¡æœ‰finetune</li>
</ul>
</li>
</ul>
<p><em>Lecture 11. QA (Chen Danqi)</em></p>
<hr>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><h3 id="Read-Comprehension"><a href="#Read-Comprehension" class="headerlink" title="Read Comprehension"></a>Read Comprehension</h3><ul>
<li><p>ç†è§£ä¸€æ®µtextï¼Œç„¶åå›ç­”ä¸€æ®µé—®é¢˜ $(Passage, Question)-&gt;Answer$</p>
</li>
<li><p>Problem formulation for SQuAD(Stanfordçš„QAæ•°æ®é›†ï¼Œansweråœ¨passageä¸­å‡ºç°)</p>
<ul>
<li>Input: $C=(c_1,c_2,\dots,c_N),Q=(q_1,q_2,\dots,q_M),c_i,q_i\in V$</li>
<li>Output: $1\le start \le end le N$</li>
</ul>
</li>
<li><p>neural models for reading comprehension</p>
<ul>
<li>LSTM-based model</li>
<li>BERT finetune model </li>
</ul>
</li>
</ul>
<h4 id="LSTM-based-model"><a href="#LSTM-based-model" class="headerlink" title="LSTM-based model"></a>LSTM-based model</h4><ul>
<li>MTä»»åŠ¡ä¸­çš„sourceå’Œtargetå˜æˆpassgeå’Œquestion</li>
<li>éœ€è¦é€šè¿‡attentionå»ºæ¨¡å“ªäº›wordå’Œquestionæœ€ç›¸å…³ï¼Œä»¥åŠå’Œquestionä¸­å“ªéƒ¨åˆ†wordç›¸å…³</li>
<li>ä¸éœ€è¦auto regressive decoderï¼Œè€Œæ˜¯è®­ç»ƒé¢„æµ‹startå’Œendä½ç½®çš„åˆ†ç±»å™¨</li>
</ul>
<h5 id="BiDAF-Bidirectional-Attention-Flow"><a href="#BiDAF-Bidirectional-Attention-Flow" class="headerlink" title="BiDAF(Bidirectional Attention Flow)"></a>BiDAF(Bidirectional Attention Flow)</h5><p><img src="https://s2.loli.net/2022/01/27/wax2cASZhqPltGo.png" alt=""></p>
<ul>
<li>Encodingï¼šcharacter embed layer+word embed layer+phrase embed layer<ul>
<li>$e(c_i)=f([Glove(c_i);charEmb(c_i)])$<br>$e(q_i)=f([Glove(q_i);charEmb(q_i)])$<br>å°†word embeddingï¼ˆGloVeï¼‰å’Œcharacter embeddingï¼ˆCNN+maxpooling over character embeddingï¼‰æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡high-way networks</li>
<li>ç”¨Bi-LSTMç”Ÿæˆcontextå’Œqueryçš„ä¸Šä¸‹æ–‡embedding</li>
</ul>
</li>
<li>attentionï¼šattention flow layer<ul>
<li>context-to-query attentionï¼šå¯¹contextä¸­çš„wordï¼Œå¯»æ‰¾queryä¸­ä¸ä¹‹æœ€ç›¸å…³çš„word</li>
<li>query-to-context attention</li>
<li>è®¡ç®—æ¯ä¸ªè¯å¯¹çš„similarity score<br>$S_{i,j}=w_{sim}^T[c_i;q_j;c_i\odot q_j]\in\mathbb{R}, w_{sim}\in\mathbb{R}^{6H}$<br>c2q: æ‰¾å‡ºå’Œc_iç›¸å…³çš„question words $\alpha_{i,j}=softmax_j(S_{i,j})\in\mathbb{R}, a_i=\Sigma_j\alpha_{i,j}q_j\in\mathbb{R}^{2H}$<br>q2c: æ‰¾å‡ºcontextä¸­é‡è¦çš„è¯ $\beta_i=softmax_i(max_j(S_{i,j}))\in\mathbb{R}^{N}, b_i=\Sigma_i\beta_ic_i\in\mathbb{R}^{2H}$<br>output: $g_i=[c_i;a_i;c_i\odot a_i;ci\odot b_i]\in\mathbb{R}^{8H}$</li>
</ul>
</li>
<li>modeling and output layer<ul>
<li>modeling layerï¼š å°†$g_i$é€è¿›ä¸¤å±‚Bi-LSTM<br>$m_i=BiLSTM(G_i)\in\mathbb{R}^{2H}$<ul>
<li>attention layerç”¨äºå»ºæ¨¡queryå’Œcontextçš„å…³ç³»</li>
<li>modeling layerç”¨äºå»ºæ¨¡ contextå†…éƒ¨çš„å…³ç³»</li>
</ul>
</li>
<li>output layerï¼š ä¸¤ä¸ªç”¨äºé¢„æµ‹startå’Œendä½ç½®çš„classifer<br>$p_{start}=softmax(w_s^T[g_i;m_i])$<br>$p_{end}=softmax(w_e^T[g_i;m_iâ€™]), m_iâ€™=BiLSTM(m_i)\in\mathbb{R}^{2H}$</li>
</ul>
</li>
</ul>
<h4 id="Bert-for-reading-comprehension"><a href="#Bert-for-reading-comprehension" class="headerlink" title="Bert for reading comprehension"></a>Bert for reading comprehension</h4><p><img src="https://s2.loli.net/2022/01/27/VqIZBAheSKRJbUg.png" alt=""></p>
<h4 id="comparisons"><a href="#comparisons" class="headerlink" title="comparisons"></a>comparisons</h4><ul>
<li>BiDAFç­‰æ¨¡å‹æ•è·questionå’Œpassageçš„å…³ç³»</li>
<li>BERTåœ¨questionå’Œpassageçš„concatä¸Šåšself-attentionï¼ˆattention(p,q),attention(q,p),attention(p,p),attention(q,q)ï¼‰ï¼ŒåŒ…å«äº†qå’Œpçš„å…³ç³»</li>
<li>Clark and Gardner, 2018è¯æ˜åœ¨BiDAFä¸­æ·»åŠ attention(p,p)å¯ä»¥è·å¾—æå‡</li>
</ul>
<p><em>Lecture 12.natural language generationï¼ˆAntoine Bosselutï¼‰</em></p>
<hr>
<h2 id="natural-language-generationï¼ˆAntoine-Bosselutï¼‰"><a href="#natural-language-generationï¼ˆAntoine-Bosselutï¼‰" class="headerlink" title="natural language generationï¼ˆAntoine Bosselutï¼‰"></a>natural language generationï¼ˆAntoine Bosselutï¼‰</h2><h3 id="what-is-NLG"><a href="#what-is-NLG" class="headerlink" title="what is NLG"></a>what is NLG</h3><ul>
<li>NLG task<ul>
<li>Machine Translation</li>
<li>dialogue systems</li>
<li>Summarization</li>
<li>Data-to-Text Generation</li>
<li>visual description</li>
<li>creative generation</li>
</ul>
</li>
</ul>
<h3 id="formulating-for-NLG"><a href="#formulating-for-NLG" class="headerlink" title="formulating for NLG"></a>formulating for NLG</h3><ul>
<li>auto-regressive model</li>
<li>Stepï¼š $S=f({y_{&lt;t},\theta})$<br>  $P(y_t|{y_{&lt;t}})=\frac{exp(S_w)}{\Sigma_{wâ€™\in V}exp(S_{wâ€™})}$</li>
<li>Loss: $L=-\Sigma_t logP(y_t|{y_{&lt;t}})$ </li>
</ul>
<h3 id="decoding-from-NLG-models"><a href="#decoding-from-NLG-models" class="headerlink" title="decoding from NLG models"></a>decoding from NLG models</h3><h4 id="Greedy-methods"><a href="#Greedy-methods" class="headerlink" title="Greedy methods"></a>Greedy methods</h4><ul>
<li>argmax decoding</li>
<li>beam search</li>
</ul>
<h4 id="get-random-sampling"><a href="#get-random-sampling" class="headerlink" title="get random: sampling"></a>get random: sampling</h4><h5 id="top-k"><a href="#top-k" class="headerlink" title="top-k"></a>top-k</h5><ul>
<li>æ¯æ¬¡åªé€‰top kä¸ªtoken</li>
<li>å¢å¤§kä¼šè®©æ¨¡å‹æ›´å¤šæ ·ä¹Ÿå®¹æ˜“å‡ºé”™</li>
<li>å‡å°kä¼šè®©æ¨¡å‹æ›´greedy</li>
</ul>
<h5 id="top-p"><a href="#top-p" class="headerlink" title="top-p"></a>top-p</h5><ul>
<li>top-kçš„é—®é¢˜<ul>
<li>å¯¹å¹³æ»‘çš„åˆ†å¸ƒï¼Œtop-kä¼šå‰Šå‡è®¸å¤šå¯èƒ½çš„é€‰é¡¹</li>
<li>å¯¹é™¡å³­çš„åˆ†å¸ƒï¼Œtop-kä¼šæ·»åŠ è®¸å¤šä¸å¤ªå¯èƒ½çš„é€‰é¡¹</li>
</ul>
</li>
<li>solution<ul>
<li>æ ¹æ®åˆ†å¸ƒçš„ç‰¹ç‚¹ï¼ŒåŠ¨æ€é€‰å–k</li>
<li>è°ƒæ•´ï¼ˆrebalanceï¼‰æ¦‚ç‡åˆ†å¸ƒï¼šé™¤ä»¥ä¸€ä¸ªè¶…å‚æ•° $\tau$åå†è®¡ç®—softmax</li>
</ul>
</li>
</ul>
<h4 id="re-balancing-distribution"><a href="#re-balancing-distribution" class="headerlink" title="re-balancing distribution"></a>re-balancing distribution</h4><p>ä»…ä¾èµ–modelè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå¯èƒ½å¹¶ä¸å¯é </p>
<ul>
<li>æ ¹æ®n-gramçš„ç»Ÿè®¡æ•°æ®æ¥è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ</li>
<li>æ ¹æ®æ¢¯åº¦åå‘ä¼ æ’­è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ<br><img src="https://s2.loli.net/2022/01/27/YcwPTaDo8xAeZSn.png" alt=""></li>
</ul>
<h3 id="traning-NLG-models"><a href="#traning-NLG-models" class="headerlink" title="traning NLG models"></a>traning NLG models</h3><h4 id="Maximum-Likelihood-Training"><a href="#Maximum-Likelihood-Training" class="headerlink" title="Maximum Likelihood Training"></a>Maximum Likelihood Training</h4><p>$L=-\Sigma_t logP(y_t|{y_{&lt;t}})$ </p>
<p>ä¸åˆ©äºå¤šæ ·åŒ–çš„text generation</p>
<h4 id="Unlikelihood-Training"><a href="#Unlikelihood-Training" class="headerlink" title="Unlikelihood Training"></a>Unlikelihood Training</h4><p>ç»™å®šä¸€ä¸ªä¸æœŸæœ›å¾—åˆ°çš„token é›†åˆ$\mathcal{C}$ï¼Œé™ä½å…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„likelihood</p>
<p>$\mathcal{L}_{UL}^t=-\Sigma_{y_{neg}\in\mathcal{C}}log(1-P(y_{neg}|\{y\}_{&lt;t}))$</p>
<p>å°†unlikelihoodå’Œlikelihoodç»“åˆ<br>$\mathcal{L}_{ULE}=\mathcal{L}_{MLE}+\alpha\mathcal{L}_{UL}$</p>
<h4 id="exposure-biasé—®é¢˜"><a href="#exposure-biasé—®é¢˜" class="headerlink" title="exposure biasé—®é¢˜"></a>exposure biasé—®é¢˜</h4><ul>
<li>è®­ç»ƒæ—¶ï¼Œæ¨¡å‹è¾“å…¥çš„æ˜¯gold contextï¼›ç”Ÿæˆæ—¶ï¼Œæ¨¡å‹è¾“å…¥æ˜¯decoderä¹‹å‰çš„è¾“å‡º</li>
<li>Solution<ul>
<li>scheduled sampling<ul>
<li>ä»¥æ¦‚ç‡pè§£ç å‡ºä¸€ä¸ªtokenå¹¶é€å…¥æ¨¡å‹ä½œä¸ºä¸‹ä¸€ä¸ªinputï¼ˆè€Œégold tokenï¼‰</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¢å¤§p</li>
<li>ä½†è®­ç»ƒä¸­åŒæ—¶ä¹Ÿä¼šç”Ÿæˆä¸€äº›å¥‡æ€ªçš„è¾“å‡º</li>
</ul>
</li>
</ul>
</li>
<li>dataset aggregation<ul>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆä¸€äº›æ–°åºåˆ—åŠ å…¥è®­ç»ƒé›†</li>
</ul>
</li>
<li>sequence re-writing<ul>
<li>åœ¨corpusä¸­æœç´¢ä¸€ä¸ªhuman-writtençš„prototype textï¼Œ</li>
</ul>
</li>
<li>reinforcement learning</li>
</ul>
<h3 id="evaluating-NLG"><a href="#evaluating-NLG" class="headerlink" title="evaluating NLG"></a>evaluating NLG</h3><ul>
<li>N-gram overlap metricsâ€”â€”BLEUã€ROUGEã€METEORï¼ˆæ•ˆæœä¸å¥½ï¼‰</li>
<li>semantic overlap metricsâ€”â€”PYRAMIDã€SPICEã€SPIDEr</li>
<li>model-based metrics<ul>
<li>word distance functions<ul>
<li>vector similarity</li>
<li>word moverâ€™s distance</li>
<li>bert score</li>
</ul>
</li>
<li>beyond word matching<ul>
<li>sentnce movers similarity</li>
<li>BLEURT</li>
</ul>
</li>
</ul>
</li>
<li>human evalutions</li>
</ul>
<p><em>Lecture 13. Coreference resolution</em></p>
<hr>
<h2 id="Co-reference-resolution"><a href="#Co-reference-resolution" class="headerlink" title="Co-reference resolution"></a>Co-reference resolution</h2><h3 id="what-is-co-reference-resolution"><a href="#what-is-co-reference-resolution" class="headerlink" title="what is co-reference resolution"></a>what is co-reference resolution</h3><ul>
<li><p>è¯†åˆ«å‡ºæ–‡æœ¬ä¸­å…±æŒ‡åŒä¸€ä¸ªå®ä½“çš„å•è¯</p>
</li>
<li><p>application</p>
<ul>
<li>æ–‡æœ¬ç†è§£<ul>
<li>ä¿¡æ¯æŠ½å–ã€QAã€summarization</li>
</ul>
</li>
<li>æœºå™¨ç¿»è¯‘</li>
<li>å¯¹è¯ç³»ç»Ÿ</li>
</ul>
</li>
</ul>
<h3 id="co-reference-resolution-in-two-steps"><a href="#co-reference-resolution-in-two-steps" class="headerlink" title="co-reference resolution in two steps"></a>co-reference resolution in two steps</h3><ul>
<li>detect the mentionsâ€”â€”easy</li>
<li>cluster the mentionsâ€”â€”hard</li>
</ul>
<h4 id="mention-detection"><a href="#mention-detection" class="headerlink" title="mention detection"></a>mention detection</h4><ul>
<li>detect a span of text referring to some entity<ul>
<li>pronouns: I, your, it, she, him, etc.</li>
<li>named entitied: people, place, Biden, etc.</li>
<li>noun phrase: a dog, etc.</li>
</ul>
</li>
<li>ç®€å•ä½†æ²¡é‚£ä¹ˆç®€å•<ul>
<li>å¾ˆå¤šç»“æœå¹¶émentions</li>
<li><strong>it</strong> is sunnyã€<strong>every student</strong> </li>
</ul>
</li>
</ul>
<h5 id="how-to-deal-with-bad-mentions"><a href="#how-to-deal-with-bad-mentions" class="headerlink" title="how to deal with bad mentions"></a>how to deal with bad mentions</h5><ul>
<li>å¯ä»¥è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨æ¥è¿‡æ»¤æ‰spurious mentions<ul>
<li>POS taggerã€NERã€parser</li>
</ul>
</li>
<li>æ›´å¸¸ç”¨çš„æ–¹æ³•ï¼šè®©æ‰€æœ‰çš„mentionséƒ½ä½œä¸ºâ€candidate mentionsâ€<ul>
<li>åœ¨co-reference ç³»ç»Ÿè¿è¡Œå®Œåï¼Œä¸¢å¼ƒæ‰è¿™äº›mention</li>
</ul>
</li>
</ul>
<h3 id="some-linguistics"><a href="#some-linguistics" class="headerlink" title="some linguistics"></a>some linguistics</h3><ul>
<li>co-referenceï¼šä¸¤ä¸ªmentionæŒ‡ä»£åŒä¸€ä¸ªå®ä½“</li>
<li><p>anaphoraï¼šä¸€ä¸ªç…§åº”è¯anaphoræŒ‡ä»£å¦ä¸€ä¸ªå…ˆè¡Œè¯antecedent</p>
</li>
<li><p>å¹¶éæ‰€æœ‰çš„anaphoricå…³ç³»éƒ½æ˜¯co-referential</p>
<ul>
<li>å¹¶éæ‰€æœ‰çš„è¯éƒ½æœ‰reference<ul>
<li><em>every dancer twisted her knee</em><br>äºŒè€…éƒ½æœ‰</li>
<li><em>no dancer twisted her knee</em><br>å­˜åœ¨anaphoricä½†æ²¡æœ‰co-referential</li>
</ul>
</li>
<li>bridging anaphora<ul>
<li><em>We went to see a concert last night. The tickets were really expensive.</em></li>
<li>ticketså¯¹concertæ²¡æœ‰co-referenceï¼Œæ˜¯bridging anaphoraçš„å…³ç³»</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/aeiMFBNr3C41jO5.png" alt=""></p>
<h3 id="4-kinds-of-co-reference-models"><a href="#4-kinds-of-co-reference-models" class="headerlink" title="4 kinds of co-reference models"></a>4 kinds of co-reference models</h3><ul>
<li>Rule-basedï¼ˆpronominal anaphora resolutionï¼‰</li>
<li>mention pair</li>
<li>mention ranking</li>
<li>clustering</li>
</ul>
<h4 id="Hobbsâ€™-naive-algo"><a href="#Hobbsâ€™-naive-algo" class="headerlink" title="Hobbsâ€™ naive algo"></a>Hobbsâ€™ naive algo</h4><ul>
<li>Rule-based</li>
<li><p>å¹¶æœªçœŸæ­£ç†è§£è¯­è¨€</p>
</li>
<li><p>Knowledge-based Pronominal Coreference</p>
<ul>
<li>She poured water from the pitcher into the cup until it was full.</li>
<li>She poured water from the pitcher into the cup until it was empty.</li>
<li>The city council refused the women a permit because they feared violence.</li>
<li>The city council refused the women a permit because they advocated violence</li>
</ul>
</li>
</ul>
<h4 id="mention-pair"><a href="#mention-pair" class="headerlink" title="mention pair"></a>mention pair</h4><ul>
<li>è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼Œå¯¹æ¯ä¸ªmention pairåˆ†ç±»</li>
<li>Lossï¼šregular CE loss<ul>
<li>$J=-\Sigma^N_{i=2}\Sigma^i_{j=1}y_{ij}log\ p(m_j,m_i)$</li>
</ul>
</li>
<li><p>é€‰æ‹©ä¸€å®šçš„é˜ˆå€¼ï¼Œç½®ä¿¡åº¦å¤§äºè¯¥é˜ˆå€¼å³æ·»åŠ co-reference è¿æ¥</p>
</li>
<li><p>disadvantage</p>
<ul>
<li>è®¸å¤šmentionåªæœ‰ä¸€ä¸ªco-reference è¿æ¥</li>
</ul>
</li>
<li>solution<ul>
<li>å¯¹æ¯ä¸ªmentionåªè®¡ç®—ä¸€ä¸ªantecedent</li>
</ul>
</li>
</ul>
<h4 id="mention-ranking"><a href="#mention-ranking" class="headerlink" title="mention ranking"></a>mention ranking</h4><ul>
<li>ä¸ºå¾—åˆ†/æ¦‚ç‡æœ€é«˜çš„candidateæ·»åŠ è¿æ¥</li>
<li>Lossï¼š<ul>
<li>$J=\Sigma^{i-1}_{j=1}\mathbb{1}(y_{ij}=1) p(m_j,m_i)$$</li>
</ul>
</li>
</ul>
<h4 id="how-to-calculate-score-probability"><a href="#how-to-calculate-score-probability" class="headerlink" title="how to calculate score/probability"></a>how to calculate score/probability</h4><ul>
<li>Non-neural statistical classifier</li>
<li>Simple neural network</li>
<li>More advanced model using LSTMs, attention, transformers</li>
</ul>
<h4 id="Non-Neural-model-Features"><a href="#Non-Neural-model-Features" class="headerlink" title="Non-Neural model: Features"></a>Non-Neural model: Features</h4><p><img src="https://s2.loli.net/2022/01/27/Y9pQXbrMAOJL723.png" alt=""></p>
<h4 id="neural-modelï¼šæ ‡å‡†å‰é¦ˆç¥ç»ç½‘ç»œ"><a href="#neural-modelï¼šæ ‡å‡†å‰é¦ˆç¥ç»ç½‘ç»œ" class="headerlink" title="neural modelï¼šæ ‡å‡†å‰é¦ˆç¥ç»ç½‘ç»œ"></a>neural modelï¼šæ ‡å‡†å‰é¦ˆç¥ç»ç½‘ç»œ</h4><p><img src="https://s2.loli.net/2022/01/27/NYhIscpa1WJRMeT.png" alt=""></p>
<ul>
<li>è¾“å…¥word embeddingå’Œdistanceã€document genreä½“è£ã€speaker infomationç­‰å…¶ä»–ç‰¹å¾</li>
</ul>
<h4 id="neural-modelï¼šCNN"><a href="#neural-modelï¼šCNN" class="headerlink" title="neural modelï¼šCNN"></a>neural modelï¼šCNN</h4><ul>
<li>å¿½ç•¥äº†è¯­æ³•ä¿¡æ¯ï¼Œå¹¶ä¸é‚£ä¹ˆlinguistically</li>
</ul>
<h4 id="neural-modelï¼šend-to-end-neural-coref-model"><a href="#neural-modelï¼šend-to-end-neural-coref-model" class="headerlink" title="neural modelï¼šend-to-end neural coref model"></a>neural modelï¼šend-to-end neural coref model</h4><ul>
<li>mention ranking model</li>
<li>åœ¨FFNåŸºç¡€ä¸Šä½¿ç”¨LSTMã€attention</li>
<li>æ²¡æœ‰mention detectionçš„è¿‡ç¨‹</li>
<li>ä¸å†å°†æ¯æ®µæ–‡æœ¬éƒ½ä½œä¸ºcandidateï¼Œåˆ©ç”¨attentionå»ä¿®å‰ª</li>
</ul>
<p>æ¨¡å‹æ¶æ„ï¼š</p>
<ul>
<li>ç”¨<strong>character-level CNN</strong> embed the words</li>
<li>å°†documenté€å…¥Bi-LSTM</li>
<li>å°†æ¯ä¸ªspanè¡¨ç¤ºä¸ºå‘é‡</li>
<li>è®¡ç®—æ¯å¯¹spançš„å¾—åˆ†</li>
</ul>
<p><img src="https://s2.loli.net/2022/01/27/X53FQpeHSOVnqB8.png" alt=""><br>$\hat x_i$æ˜¯spanä¸­word embeddingçš„attentionåŠ æƒæ±‚å’Œã€‚<br><img src="https://s2.loli.net/2022/01/27/mLipzkHxgRZdISs.png" alt=""></p>
<ul>
<li>$score(i,j)=s_m(i)+s_m(j)+s_a(i,j)$</li>
<li>$s_m(i)=w_m\cdot FFNN_m(g_i)$ è¡¨å¾iæ˜¯å¦ä¸ºmention</li>
<li>$s_a(i,j)=w_a\cdot FFNN_a([g_i,g_j,g_i\circ g_i,\phi(i,j)])$è¡¨å¾iå’Œjæ˜¯å¦coreferent</li>
</ul>
<h4 id="neural-modelï¼šBERT-based-model"><a href="#neural-modelï¼šBERT-based-model" class="headerlink" title="neural modelï¼šBERT-based model"></a>neural modelï¼šBERT-based model</h4><ul>
<li>idea<ul>
<li>é¢„è®­ç»ƒåœ¨span-basedé¢„æµ‹ä»»åŠ¡ä¸Šï¼ˆcorefå’ŒQAï¼‰è¡¨ç°æ›´å¥½çš„BERTæ¨¡å‹</li>
<li>BERT-QAï¼šå°†corefçœ‹ä½œQA task<ul>
<li>Qï¼šmention</li>
<li>Aï¼šmentionçš„antecedent</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="co-reference-evaluation"><a href="#co-reference-evaluation" class="headerlink" title="co-reference evaluation"></a>co-reference evaluation</h3><ul>
<li>B-cubed<ul>
<li>å¯¹æ¯ä¸ªmentionè®¡ç®—på’Œr</li>
<li>å¯¹På’ŒRå–å¹³å‡</li>
</ul>
</li>
</ul>
<p><em>Lecture14. T5 and large LM:the good, the bad and the ugly(Colin Raffel)</em></p>
<hr>
<h2 id="T5-and-large-LM"><a href="#T5-and-large-LM" class="headerlink" title="T5 and large LM"></a>T5 and large LM</h2><h3 id="larger-LM"><a href="#larger-LM" class="headerlink" title="larger LM"></a>larger LM</h3><ul>
<li>larger corpus</li>
<li>more parameters</li>
<li>more tokens</li>
<li>different optimizer</li>
</ul>
<h3 id="T5-Text-to-text-Transfer-Transformer"><a href="#T5-Text-to-text-Transfer-Transformer" class="headerlink" title="T5(Text-to-text Transfer Transformer)"></a>T5(Text-to-text Transfer Transformer)</h3><ul>
<li>text-to-text task<ul>
<li>MT</li>
<li>CoLA(Corpus of Linguistic Acceptability)</li>
<li>STS-B(Semantic Textual Similarity Benchmark)</li>
<li>summarize</li>
</ul>
</li>
<li>pre-train dataset<ul>
<li>C4 dataset</li>
</ul>
</li>
</ul>
<h4 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h4><ul>
<li>original text<ul>
<li>thank you for inviting me to your party last week</li>
</ul>
</li>
<li>inputs<ul>
<li>thank you <X> me to your party <Y> week</li>
</ul>
</li>
<li>targets<ul>
<li><X> for inviting <Y> last <Z></li>
</ul>
</li>
</ul>
<h4 id="pretrain"><a href="#pretrain" class="headerlink" title="pretrain"></a>pretrain</h4><p><img src="https://s2.loli.net/2022/01/26/7UCwHlkAMSqvYDe.png" alt=""></p>
<p><em>Lecture 14. Integrating knowledge in language models</em></p>
<h2 id="Integrating-knowledge-in-language-models"><a href="#Integrating-knowledge-in-language-models" class="headerlink" title="Integrating knowledge in language models"></a>Integrating knowledge in language models</h2><p>can a LM be used as a KBï¼Ÿ</p>
<h3 id="what-does-a-LM-know"><a href="#what-does-a-LM-know" class="headerlink" title="what does a LM know?"></a>what does a LM know?</h3><ul>
<li>é¢„æµ‹ç»“æœé€šå¸¸æ˜¯æœ‰æ„ä¹‰çš„ï¼ˆè¯­æ³•/ç±»å‹åŒ¹é…ï¼‰ï¼Œä½†å¹¶ä¸æ­£ç¡®</li>
<li>åŸå› ï¼š<ul>
<li>unseen factï¼šåœ¨è®­ç»ƒé›†ä¸­ä¸€äº›factè¿˜æ²¡æœ‰å‘ç”Ÿ</li>
<li>rare factï¼šæ ·æœ¬ä¸å¤Ÿï¼Œæ— æ³•è®°ä½ç›¸å…³fact</li>
<li>model sensitivityï¼šå¯¹ä¸Šä¸‹æ–‡éå¸¸æ•æ„Ÿ<ul>
<li>x was made in y å’Œ x was created in y</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="advantages-of-LM-over-traditional-KB"><a href="#advantages-of-LM-over-traditional-KB" class="headerlink" title="advantages of LM over traditional KB"></a>advantages of LM over traditional KB</h3><ul>
<li>LMä»å¤§è§„æ¨¡éç»“æ„åŒ–å’Œæ— æ ‡æ³¨æ–‡æœ¬ä¸Šé¢„è®­ç»ƒ<ul>
<li>KBéœ€è¦äººå·¥æ ‡æ³¨å’Œå¤æ‚çš„NLPè§„åˆ™</li>
</ul>
</li>
<li><p>LMæ”¯æŒæ›´çµæ´»çš„query</p>
</li>
<li><p>challenges</p>
<ul>
<li>éš¾ä»¥è§£é‡ŠæŸäº›ç»“æœ</li>
<li>æœ‰æ—¶éš¾ä»¥ç›¸ä¿¡</li>
<li>éš¾ä»¥ä¿®æ”¹æ¨¡å‹</li>
</ul>
</li>
</ul>
<h3 id="techniques-to-add-knowledge-to-LMs"><a href="#techniques-to-add-knowledge-to-LMs" class="headerlink" title="techniques to add knowledge to LMs"></a>techniques to add knowledge to LMs</h3><ul>
<li>add pretrained entity embeddings<ul>
<li>ERNIE</li>
<li>KnowBERT</li>
</ul>
</li>
<li>use an external memory<ul>
<li>KGLM</li>
<li>KNN-LM</li>
</ul>
</li>
<li>modify the training data<ul>
<li>WKLM</li>
<li>ERNIE, salient span masking</li>
</ul>
</li>
</ul>
<h4 id="add-pretrained-entity-embedding"><a href="#add-pretrained-entity-embedding" class="headerlink" title="add pretrained entity embedding"></a>add pretrained entity embedding</h4><ul>
<li>çœŸå®ä¸–ç•Œçš„äº‹ç‰©é€šå¸¸æ˜¯å®ä½“</li>
<li>pretrained word embeddingæ²¡æœ‰å®ä½“çš„æ¦‚å¿µ<ul>
<li>æ¯”å¦‚USAå’ŒAmericaæ˜¯æŒ‡å‘åŒä¸€å®ä½“çš„</li>
</ul>
</li>
<li>è§£å†³æ–¹æ¡ˆ<ul>
<li>å¯¹æŒ‡å‘åŒä¸€å®ä½“çš„wordsè¿æ¥åŒæ ·çš„entity embedding<br>entity linkingï¼šlink mentions in text to entities in a KB</li>
</ul>
</li>
</ul>
<h5 id="techniques-for-training-entity-embeddings"><a href="#techniques-for-training-entity-embeddings" class="headerlink" title="techniques for training entity embeddings"></a>techniques for training entity embeddings</h5><ul>
<li>KG embedding(TransE)</li>
<li>word-entity co-occurrence(Wikipedia2Vec)</li>
<li>Transformer encodings of entity description(BLINK)</li>
</ul>
<h5 id="how-to-incorporate-entity-embeddings-from-a-different-embedding-space"><a href="#how-to-incorporate-entity-embeddings-from-a-different-embedding-space" class="headerlink" title="how to incorporate entity embeddings from a different embedding space"></a>how to incorporate entity embeddings from a different embedding space</h5><ul>
<li>å­¦ä¹ ä¸€ä¸ªfusion layerèåˆcontextå’Œentity information<br>$h_j=F(W_tw_j+W_Ee_k+b)$</li>
</ul>
<h3 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h3><h4 id="architecture"><a href="#architecture" class="headerlink" title="architecture"></a>architecture</h4><p><img src="https://s2.loli.net/2022/01/27/PHzyKTZMmVJdFna.png" alt=""></p>
<ul>
<li>text-encoder<ul>
<li>multi-layer bidirectional Transformer</li>
</ul>
</li>
<li>knowledge encoder<ul>
<li>2 multi-head attention over entity embedding and token embedding</li>
<li>1 fusion layer to combine<br><img src="https://s2.loli.net/2022/01/26/MpydcXslbDKvqOI.png" alt=""></li>
</ul>
</li>
</ul>
<h4 id="pretrain-tasks"><a href="#pretrain-tasks" class="headerlink" title="pretrain tasks"></a>pretrain tasks</h4><ul>
<li>BERT tasks<ul>
<li>MLM</li>
<li>NSP</li>
</ul>
</li>
<li>knowledge pretraining task(dEA, denoising entity autoencoder)<ul>
<li>éšæœºmaskæ‰ä¸€ä¸ªtoken-entity alignmentï¼Œç„¶åæ ¹æ®sequenceä¸­çš„å®ä½“é¢„æµ‹å‡ºè¯¥tokençš„ç›¸å…³entity<br>$p(e_j|w_i)=\frac{exp(Ww_i\cdot e_j)}{\Sigma_{k=1}^{m}exp(Ww_i\cdot e_k)}$</li>
</ul>
</li>
</ul>
<p>$L_{ERNIE}=L_{MLM}+L_{NSP}+L_{dEA}$</p>
<h4 id="strengths"><a href="#strengths" class="headerlink" title="strengths"></a>strengths</h4><ul>
<li>é€šè¿‡fusion layerå’ŒdEAèåˆäº†entityå’Œcontext</li>
<li><p>å¯¹çŸ¥è¯†é©±åŠ¨çš„ä¸‹æ¸¸ä»»åŠ¡æå‡å¾ˆå¤§</p>
</li>
<li><p>remaining challenges</p>
<ul>
<li>éœ€è¦å¸¦entityæ ‡æ³¨çš„æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œéœ€è¦ä¸€ä¸ªentity linker</li>
<li>éœ€è¦further pretrain</li>
</ul>
</li>
</ul>
<h3 id="KnowBERT"><a href="#KnowBERT" class="headerlink" title="KnowBERT"></a>KnowBERT</h3><ul>
<li>æ ¸å¿ƒæ€æƒ³ï¼šåœ¨BERTä¹‹å¤–é¢„è®­ç»ƒä¸€ä¸ªentity linker<br>$L_{KnowBERT}=L_{MLM}+L_{NSP}+L_{EL}$</li>
<li>åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¸éœ€è¦é¢å¤–çš„entityæ ‡æ³¨</li>
</ul>
<h3 id="KGLM"><a href="#KGLM" class="headerlink" title="KGLM"></a>KGLM</h3><ul>
<li>ä¹‹å‰çš„æ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒçš„entity embeddingæ¥ä»KBç¼–ç çŸ¥è¯†</li>
<li>éœ€è¦æ›´ç›´æ¥çš„æ–¹æ³•æ¥ä¸ºLMæä¾›é¢å¤–çŸ¥è¯†</li>
<li><p>solutionï¼šè®©æ¨¡å‹å¯ä»¥ç›´æ¥è®¿é—®ä¸‰å…ƒç»„ï¼ˆkey-value storeï¼‰</p>
</li>
<li><p>advantages</p>
<ul>
<li>å¯ä»¥æ›´å¥½åœ°æ›´æ–°çŸ¥è¯†</li>
<li>å¯è§£é‡Šæ€§æ›´å¼º</li>
</ul>
</li>
<li><p>æ ¸å¿ƒæ€æƒ³ï¼šåœ¨KGä¸Šè°ƒæ•´LM</p>
</li>
<li>åˆ©ç”¨entityçš„ä¿¡æ¯æ¥é¢„æµ‹ä¸‹ä¸€word $P(x^{(t+1)},\epsilon^{(t+1)}|x^{(t)},\dots,x^{(1)},\epsilon^{(t)},\dots,\epsilon^{(1)})$<br>$\epsilon^{(t)}$æ˜¯tæ—¶åˆ»æåŠçš„KG entities</li>
</ul>
<h4 id="architecture-1"><a href="#architecture-1" class="headerlink" title="architecture"></a>architecture</h4><ul>
<li>åœ¨è¿­ä»£æ—¶æ„å»ºä¸€ä¸ªâ€localâ€ KG<ul>
<li>local KGâ€”â€”full KGçš„å­é›†ï¼šåªåŒ…å«å’Œsequenceç›¸å…³çš„entities</li>
</ul>
</li>
<li>next wordæœ‰ä¸‰ç±»ï¼š<ul>
<li>å·²ç»åœ¨local KGä¸­çš„related entity<ul>
<li>éœ€è¦æ‰¾åˆ°æœ€é«˜å¾—åˆ†çš„headå’Œrelation</li>
<li>$P(e_h)=softmax(v_h\cdot h_t)$</li>
<li>æ‰¾åˆ°tail entity</li>
<li>ä»entityæ‰©å±•token</li>
</ul>
</li>
<li>ä¸åœ¨local KGä¸­çš„new entity<ul>
<li>åœ¨full KGä¸­æ‰¾åˆ°tail entity</li>
</ul>
</li>
<li>éentity<ul>
<li>é€€åŒ–</li>
</ul>
</li>
</ul>
</li>
<li>ç”¨LSTMçš„hidden stateé¢„æµ‹net word çš„ç±»å‹</li>
</ul>
<h3 id="KNN-LM"><a href="#KNN-LM" class="headerlink" title="KNN-LM"></a>KNN-LM</h3><ul>
<li>æ ¸å¿ƒæ€æƒ³ï¼šå­¦ä¹ å¥å­é—´çš„similarityè¦æ¯”é¢„æµ‹next wordæ›´å®¹æ˜“ï¼Œå°¤å…¶å¯¹long-tail patternæ›´æœ‰æ•ˆ<ul>
<li>å¯»æ‰¾kä¸ªç›¸ä¼¼å¥å­</li>
<li>ä»ä¸­æ£€ç´¢ç›¸åº”çš„valueï¼ˆi.e. next wordï¼‰</li>
<li>å°†knnæ¦‚ç‡å’ŒLMæ¦‚ç‡ç»“åˆ</li>
</ul>
</li>
</ul>
<h3 id="WKLM"><a href="#WKLM" class="headerlink" title="WKLM"></a>WKLM</h3><ul>
<li>ä¹‹å‰çš„æ–¹æ³•æ˜¯ä»å¤–éƒ¨å¼•å…¥çŸ¥è¯†æˆ–ä½¿ç”¨é¢å¤–çš„memory</li>
<li><p>å°†knowledgeå¼•å…¥éç»“æ„æ–‡æœ¬</p>
</li>
<li><p>æ ¸å¿ƒæ€æƒ³ï¼šè®­ç»ƒæ¨¡å‹å»è¾¨åˆ«çŸ¥è¯†çš„çœŸä¼ª</p>
</li>
<li>ç”¨åŒç±»å‹çš„å…¶ä»–entityæ›¿æ¢ï¼Œç”Ÿæˆnegative knowledge statement</li>
<li>Lossï¼š$L=L_{MLM}+L_{entRep}$ï¼Œå‰è€…æ˜¯token-levelçš„ï¼Œåè€…æ˜¯entity-levelçš„<br><img src="https://s2.loli.net/2022/01/27/x9rEy6kcVaiFlO8.png" alt=""></li>
</ul>
<h3 id="Learn-inductive-biases-through-masking"><a href="#Learn-inductive-biases-through-masking" class="headerlink" title="Learn inductive biases through masking"></a>Learn inductive biases through masking</h3><h4 id="ERNIE-1"><a href="#ERNIE-1" class="headerlink" title="ERNIE"></a>ERNIE</h4><p><img src="https://s2.loli.net/2022/01/27/VuRlc84eyKxYXfA.png" alt=""></p>
<ul>
<li>ä½¿ç”¨æ›´æœ‰æ•ˆçš„mask<ul>
<li>phrase-level</li>
<li>entity-level</li>
</ul>
</li>
</ul>
<h4 id="Salient-span-masking"><a href="#Salient-span-masking" class="headerlink" title="Salient span masking"></a>Salient span masking</h4><ul>
<li>maskæ‰salient spans<ul>
<li>named entities </li>
<li>dates</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/" class="post-title-link" itemprop="url">(AAAI2022) When Shift Operation Meets Vision Transformer:An Extremely Simple Alternative to Attention Mechanism</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-24 14:58:16" itemprop="dateCreated datePublished" datetime="2022-01-24T14:58:16+08:00">2022-01-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 23:35:51" itemprop="dateModified" datetime="2022-02-14T23:35:51+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>2 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>MicroSoft åœ¨AAAI2022çš„ä¸€ç¯‡æ–‡ç« ï¼Œå°†ViTä¸­çš„attentionæœºåˆ¶æ›¿æ¢ä¸ºæ— å‚æ•°çš„shiftæ“ä½œï¼Œåœ¨å‡ ä¸ªä¸»æµä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†ç›¸å½“å¥½çš„ç»“æœâ€”â€”attentionæœºåˆ¶æˆ–è®¸å¹¶éViTæˆåŠŸçš„å…³é”®å› ç´ ã€‚<br>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/pdf/2201.10801.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2201.10801.pdf</a><br>å¼€æºä»£ç ï¼š<a href="https://github.com/microsoft/SPACH" target="_blank" rel="noopener">https://github.com/microsoft/SPACH</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/24/(AAAI2022)%20When%20Shift%20Operation%20Meets%20Vision%20Transformer-An%20Extremely%20Simple%20Alternative%20to%20Attention%20Mechanism/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/20/attention%20is%20all%20you%20need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/attention%20is%20all%20you%20need/" class="post-title-link" itemprop="url">Reread Attention Is All You Need</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-20 18:53:16" itemprop="dateCreated datePublished" datetime="2022-01-20T18:53:16+08:00">2022-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-01-26 00:09:44" itemprop="dateModified" datetime="2022-01-26T00:09:44+08:00">2022-01-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
                  ï¼Œ
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
                  ï¼Œ
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Transformer/Attention/" itemprop="url" rel="index"><span itemprop="name">Attention</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>2 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>è·Ÿææ²å†è¯»<em>Attention Is All You Need</em>ã€‚<br>è§†é¢‘é“¾æ¥ï¼š<a href="https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/20/attention%20is%20all%20you%20need/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/" class="post-title-link" itemprop="url">(EMNLP2021) A Semantic Filter Based on Relations for Knowledge Graph Completion</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2022-01-20 14:20:16" itemprop="dateCreated datePublished" datetime="2022-01-20T14:20:16+08:00">2022-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 20:17:03" itemprop="dateModified" datetime="2022-02-14T20:17:03+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/KGC/" itemprop="url" rel="index"><span itemprop="name">KGC</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>1 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>æ¥è‡ªå›½é˜²ç§‘æŠ€å¤§å­¦ï¼Œé’ˆå¯¹å…³ç³»å’Œå¤´å°¾å®ä½“çš„è”ç³»ï¼Œæå‡ºåº”è¯¥ä¸ºæ¯ä¸ªå…³ç³»rå®šä¹‰ä¸€ä¸ªå¯¹å¤´å°¾å®ä½“çš„æ»¤æ³¢å™¨ã€‚<br>è®ºæ–‡åœ°å€ï¼š<a href="https://aclanthology.org/2021.emnlp-main.625.pdf" target="_blank" rel="noopener">https://aclanthology.org/2021.emnlp-main.625.pdf</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2022/01/20/(EMNLP2021)%20A%20Semantic%20Filter%20Based%20on%20Relations%20for%20Knowledge%20Graph%20Completion/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/" class="post-title-link" itemprop="url">(EMNLP2021) Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2021-12-10 14:20:16" itemprop="dateCreated datePublished" datetime="2021-12-10T14:20:16+08:00">2021-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 23:46:11" itemprop="dateModified" datetime="2022-02-14T23:46:11+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
                  ï¼Œ
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GNN/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">çŸ­æ–‡æœ¬åˆ†ç±»</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>1.4k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>1 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>æ¥è‡ªç™¾åº¦å’Œæ¸…åï¼Œæå‡ºä¸ºæ–‡æœ¬æ„å»ºå¤šä¸ªå¼‚è´¨å›¾çš„æ–¹æ³•ã€‚<br>è®ºæ–‡åœ°å€ï¼š<a href="https://arxiv.org/abs/2111.00180v1" target="_blank" rel="noopener">https://arxiv.org/abs/2111.00180v1</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/12/10/(EMNLP2021)%20Hierarchical%20Heterogeneous%20Graph%20Representation%20Learning%20for%20Short%20Text%20Classification/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/" class="post-title-link" itemprop="url">(EMNLP2019) Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2021-11-27 18:53:16" itemprop="dateCreated datePublished" datetime="2021-11-27T18:53:16+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 20:10:02" itemprop="dateModified" datetime="2022-02-14T20:10:02+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">çŸ­æ–‡æœ¬åˆ†ç±»</span></a>
                </span>
                  ï¼Œ
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%AD%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/GNN/" itemprop="url" rel="index"><span itemprop="name">GNN</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>1.4k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>1 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>æ¥è‡ªåŒ—é‚®å›¢é˜Ÿï¼Œæå‡ºäº†å¯ä»¥é›†æˆå¤šç§é¢å¤–ä¿¡æ¯çš„HINå’Œdual-levelçš„GATï¼Œåˆ©ç”¨é™„åŠ ä¿¡æ¯å¸®åŠ©åŠç›‘ç£STCã€‚åœ¨AGNewsä¸Šç”±TextGCNçš„67.67æ¶¨åˆ°72.10ã€‚</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/11/27/(EMNLP2019)%20Heterogeneous%20Graph%20Attention%20Networks%20for%20Semi-supervised%20Short%20Text%20Classification/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/" class="post-title-link" itemprop="url">(EMNLP2021) SimCSE: Simple Contrastive Learning of Sentence Embeddings</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2021-10-29 14:20:16" itemprop="dateCreated datePublished" datetime="2021-10-29T14:20:16+08:00">2021-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2022-02-14 23:50:21" itemprop="dateModified" datetime="2022-02-14T23:50:21+08:00">2022-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sentence-embedding/" itemprop="url" rel="index"><span itemprop="name">Sentence embedding</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>1 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>From Princeton Chen Danqiï¼Œæå‡ºå¯¹æ¯”å­¦ä¹ çš„sentence embeddingå­¦ä¹ æ¡†æ¶ ï¼Œå‘è¡¨åœ¨EMNLP2021ã€‚</p>
<p>è®ºæ–‡åœ°å€: <a href="https://arxiv.org/pdf/2104.08821.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2104.08821.pdf</a><br>å¼€æºä»£ç : <a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener">https://github.com/princeton-nlp/SimCSE</a></p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/10/29/(EMNLP2021)%20SimCSE-Simple%20Contrastive%20Learning%20of%20Sentence%20Embeddings/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/" class="post-title-link" itemprop="url">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼ˆäº”ï¼‰--å†³ç­–æ ‘</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2021-01-27 14:06:28" itemprop="dateCreated datePublished" datetime="2021-01-27T14:06:28+08:00">2021-01-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2021-02-17 14:53:00" itemprop="dateModified" datetime="2021-02-17T14:53:00+08:00">2021-02-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>2 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>å†³ç­–æ ‘ï¼ˆdesign treeï¼‰æ˜¯ä¸€ç§åŸºæœ¬çš„åˆ†ç±»ä¸å›å½’æ–¹æ³•ã€‚ æ˜¯if-thenè§„åˆ™çš„é›†åˆæˆ–è€…å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸ç±»ç©ºé—´ä¸Šçš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%86%B3%E7%AD%96%E6%A0%91/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rooki3ray.github.io/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Rooki3Ray">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rooki3Ray | Cyber Security">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" class="post-title-link" itemprop="url">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼ˆå››ï¼‰--æœ´ç´ è´å¶æ–¯</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>
              

              <time title="åˆ›å»ºæ—¶é—´ï¼š2021-01-27 00:25:15 / ä¿®æ”¹æ—¶é—´ï¼š14:05:26" itemprop="dateCreated datePublished" datetime="2021-01-27T00:25:15+08:00">2021-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">ç»Ÿè®¡å­¦ä¹ æ–¹æ³•</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">æœ¬æ–‡å­—æ•°ï¼š</span>
              <span>420</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">é˜…è¯»æ—¶é•¿ &asymp;</span>
              <span>1 åˆ†é’Ÿ</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>æœ´ç´ è´å¶æ–¯ï¼ˆnaive Bayesï¼‰æ˜¯åŸºäºBayeså®šç†ä¸ç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„åˆ†ç±»æ–¹æ³•ã€‚åŸºäºç‰¹å¾æ¡ä»¶ç‹¬ç«‹å‡è®¾å­¦ä¹ è¾“å…¥è¾“å‡ºçš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶ååŸºäºæ­¤æ¨¡å‹å¯¹ç»™å®šè¾“å…¥xåˆ©ç”¨Bayeså®šç†æ±‚å‡ºåéªŒæ¦‚ç‡æœ€å¤§çš„è¾“å‡ºyã€‚</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/01/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/#more" rel="contents">
                é˜…è¯»å…¨æ–‡ &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="ä¸‹ä¸€é¡µ"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Rooki3Ray"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Rooki3Ray</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/rooki3ray" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;rooki3ray" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2922859032@qq.com" title="E-Mail â†’ mailto:2922859032@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5355766445" title="Weibo â†’ https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5355766445" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="/2922859032" title="QQ â†’ 2922859032"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/rooki3ray" title="https:&#x2F;&#x2F;github.com&#x2F;rooki3ray" rel="noopener" target="_blank">GitHub</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rooki3Ray</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">ç«™ç‚¹æ€»å­—æ•°ï¼š</span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">154k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">ç«™ç‚¹é˜…è¯»æ—¶é•¿ &asymp;</span>
    <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">2:20</span>
</div>



        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
